{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Predict the Spatial Sea Ice Concentration with BayesConvLSTM at weekly time scale** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2020.03.02 ** <br>\n",
    "** Last Update  : 2020.03.06 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, dlacs, matplotlib **<br>\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. The Bayesian Convolutional Long Short Time Memory neural network is used to deal with this spatial-temporal sequence problem. We use Pytorch as the deep learning framework. <br>\n",
    "<br>\n",
    "** Here we predict sea ice concentration with one extra relevant field from either ocean or atmosphere to test the predictor.** <br>\n",
    "\n",
    "Return Values   : Time series and figures <br>\n",
    "\n",
    "The regionalization adopted here follows that of the MASIE (Multisensor Analyzed Sea Ice Extent) product available from the National Snow and Ice Data Center:<br>\n",
    "https://nsidc.org/data/masie/browse_regions<br>\n",
    "It is given by paper J.Walsh et. al., 2019. Benchmark seasonal prediction skill estimates based on regional indices.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlacs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86dc5b6861cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#sys.path.append(\"C:\\\\Users\\\\nosta\\\\ML4Climate\\\\Scripts\\\\DLACs\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../DLACs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlacs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdlacs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBayesConvLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdlacs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlacs'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import numbers\n",
    "\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "#sys.path.append(os.path.join('C:','Users','nosta','ML4Climate','Scripts','DLACs'))\n",
    "#sys.path.append(\"C:\\\\Users\\\\nosta\\\\ML4Climate\\\\Scripts\\\\DLACs\")\n",
    "sys.path.append(\"../../../DLACs\")\n",
    "import dlacs\n",
    "import dlacs.BayesConvLSTM\n",
    "import dlacs.preprocess\n",
    "import dlacs.function\n",
    "\n",
    "# for visualization\n",
    "import dlacs.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import iris # also helps with regriding\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing device is Dell Inspirion 5680 with Intel Core i7-8700 x64 CPU and Nvidia GTX 1060 6GB GPU.<br>\n",
    "Here is a benchmark about cpu v.s. gtx 1060 <br>\n",
    "https://www.analyticsindiamag.com/deep-learning-tensorflow-benchmark-intel-i5-4210u-vs-geforce-nvidia-1060-6gb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2500000,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data ** <br>\n",
    "Time span of each product included: <br>\n",
    "** Reanalysis ** <br>\n",
    "- **ERA-Interim** 1979 - 2016 (ECMWF)\n",
    "- **ORAS4**       1958 - 2014 (ECMWF)\n",
    "\n",
    "** Index ** <br>\n",
    "- **NINO3.4**     1950 - 2017 (NOAA)\n",
    "- **AO**          1950 - 2017 (NOAA)\n",
    "- **NAO**         1950 - 2017 (NOAA)\n",
    "- **AMO**         1950 - 2017 (NOAA)\n",
    "- **PDO**         1950 - 2017 (University of Washington)\n",
    "\n",
    "!! These index are given by NCEP/NCAR Reanalysis (CDAS) <br>\n",
    "\n",
    "\n",
    "Alternative (not in use yet) <br>\n",
    "** Reanalysis ** <br>\n",
    "- **MERRA2**      1980 - 2016 (NASA)\n",
    "- **JRA55**       1979 - 2015 (JMA)\n",
    "- **GLORYS2V3**   1993 - 2014 (Mercartor Ocean)\n",
    "- **SODA3**       1980 - 2015\n",
    "- **PIOMASS**     1980 - 2015\n",
    "\n",
    "** Observations ** <br>\n",
    "- **NSIDC**       1958 - 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# \n",
    "#########                           datapath                             ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ERA-Interim'\n",
    "#datapath_ERAI = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ERA-Interim'\n",
    "datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4 = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4_mask = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "#datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "#datapath_PIOMASS = 'H:\\\\Creator_Zone\\\\Core_Database_AMET_OMET_reanalysis\\\\PIOMASS'\n",
    "#datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "#datapath_clim_index = 'F:\\\\PhD_essential\\\\Core_Database_AMET_OMET_reanalysis\\\\Climate_index'\n",
    "output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/BayesMaps'\n",
    "#output_path = 'C:\\\\Users\\\\nosta\\\\ML4Climate\\\\PredictArctic\\\\BayesMaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** get the key to the datasets *************************\n",
      "*********************** extract variables *************************\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    # weekly variables on ERAI grid\n",
    "    dataset_ERAI_fields_sic = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'sic_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_slp = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       'slp_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_t2m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       't2m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z500 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z500_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z850 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z850_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_uv10m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'uv10m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_rad = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                         'rad_flux_weekly_erai_1979_2017.nc'))\n",
    "    #dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "    #                             'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    # OHC interpolated on ERA-Interim grid\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                'ohc_monthly_oras2erai_1978_2017.nc'))\n",
    "#     dataset_index = Dataset(os.path.join(datapath_clim_index,\n",
    "#                             'index_climate_monthly_regress_1950_2017.nc'))\n",
    "    #dataset_ERAI_fields_flux = Dataset(os.path.join(datapath_ERAI_fields,\n",
    "    #                                  'surface_erai_monthly_regress_1979_2017_radiation.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    #################################################################################\n",
    "    #########                        data gallery                           #########\n",
    "    #################################################################################\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2013\n",
    "    # validation: 2014 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim) - benckmark\n",
    "    SIC_ERAI = dataset_ERAI_fields_sic.variables['sic'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI = dataset_ERAI_fields_sic.variables['year'][:-1]\n",
    "    week_ERAI = dataset_ERAI_fields_sic.variables['week'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_sic.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_sic.variables['longitude'][:]\n",
    "    # T2M (ERA-Interim)\n",
    "#     T2M_ERAI = dataset_ERAI_fields_t2m.variables['t2m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_t2m = dataset_ERAI_fields_t2m.variables['year'][:-1]\n",
    "#     week_ERAI_t2m = dataset_ERAI_fields_t2m.variables['week'][:]\n",
    "#     latitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['latitude'][:]\n",
    "#     longitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['longitude'][:]\n",
    "    # SLP (ERA-Interim)\n",
    "#     SLP_ERAI = dataset_ERAI_fields_slp.variables['slp'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_slp = dataset_ERAI_fields_slp.variables['year'][:-1]\n",
    "#     week_ERAI_slp = dataset_ERAI_fields_slp.variables['week'][:]\n",
    "#     latitude_ERAI_slp = dataset_ERAI_fields_slp.variables['latitude'][:]\n",
    "#     longitude_ERAI_slp = dataset_ERAI_fields_slp.variables['longitude'][:]\n",
    "    # Z500 (ERA-Interim)\n",
    "#     Z500_ERAI = dataset_ERAI_fields_z500.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z500 = dataset_ERAI_fields_z500.variables['year'][:-1]\n",
    "#     week_ERAI_z500 = dataset_ERAI_fields_z500.variables['week'][:]\n",
    "#     latitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['latitude'][:]\n",
    "#     longitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['longitude'][:]\n",
    "    # Z850 (ERA-Interim)\n",
    "#     Z850_ERAI = dataset_ERAI_fields_z850.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z850 = dataset_ERAI_fields_z850.variables['year'][:-1]\n",
    "#     week_ERAI_z850 = dataset_ERAI_fields_z850.variables['week'][:]\n",
    "#     latitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['latitude'][:]\n",
    "#     longitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['longitude'][:]\n",
    "    # UV10M (ERA-Interim)\n",
    "#     U10M_ERAI = dataset_ERAI_fields_uv10m.variables['u10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     V10M_ERAI = dataset_ERAI_fields_uv10m.variables['v10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['year'][:-1]\n",
    "#     week_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['week'][:]\n",
    "#     latitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['latitude'][:]\n",
    "#     longitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['longitude'][:]\n",
    "    # SFlux (ERA-Interim)\n",
    "#     SFlux_ERAI = dataset_ERAI_fields_rad.variables['SFlux'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_SFlux = dataset_ERAI_fields_rad.variables['year'][:-1]\n",
    "#     week_ERAI_SFlux = dataset_ERAI_fields_rad.variables['week'][:]\n",
    "#     latitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['latitude'][:]\n",
    "#     longitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    #SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    #year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # OHC (ORAS4)\n",
    "    # from 1978 - 2017 (for interpolation) / from 90 N upto 40 N\n",
    "    OHC_300_ORAS4 = dataset_ORAS4_OHC.variables['OHC'][:-1,:,:67,:]/1000 # unit Peta Joule\n",
    "    latitude_ORAS4 = dataset_ORAS4_OHC.variables['latitude'][:]\n",
    "    longitude_ORAS4 = dataset_ORAS4_OHC.variables['longitude'][:]\n",
    "    mask_OHC = np.ma.getmask(OHC_300_ORAS4[0,0,:,:])\n",
    "    # AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "#     AO = dataset_index.variables['AO'][348:-1] # from 1979 - 2017\n",
    "#     NAO = dataset_index.variables['NAO'][348:-1]\n",
    "#     NINO = dataset_index.variables['NINO'][348:-1]\n",
    "#     AMO = dataset_index.variables['AMO'][348:-1]\n",
    "#     PDO = dataset_index.variables['PDO'][348:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.5  88.75 88.   87.25 86.5  85.75 85.   84.25 83.5  82.75 82.   81.25\n",
      " 80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25\n",
      " 62.5  61.75 61.   60.25 59.5  58.75 58.   57.25 56.5  55.75 55.   54.25\n",
      " 53.5  52.75 52.   51.25 50.5  49.75 49.   48.25 47.5  46.75 46.   45.25\n",
      " 44.5  43.75 43.   42.25 41.5  40.75 40.  ]\n",
      "[-180.   -179.25 -178.5  -177.75 -177.   -176.25 -175.5  -174.75 -174.\n",
      " -173.25 -172.5  -171.75 -171.   -170.25 -169.5  -168.75 -168.   -167.25\n",
      " -166.5  -165.75 -165.   -164.25 -163.5  -162.75 -162.   -161.25 -160.5\n",
      " -159.75 -159.   -158.25 -157.5  -156.75 -156.   -155.25 -154.5  -153.75\n",
      " -153.   -152.25 -151.5  -150.75 -150.   -149.25 -148.5  -147.75 -147.\n",
      " -146.25 -145.5  -144.75 -144.   -143.25 -142.5  -141.75 -141.   -140.25\n",
      " -139.5  -138.75 -138.   -137.25 -136.5  -135.75 -135.   -134.25 -133.5\n",
      " -132.75 -132.   -131.25 -130.5  -129.75 -129.   -128.25 -127.5  -126.75\n",
      " -126.   -125.25 -124.5  -123.75 -123.   -122.25 -121.5  -120.75 -120.\n",
      " -119.25 -118.5  -117.75 -117.   -116.25 -115.5  -114.75 -114.   -113.25\n",
      " -112.5  -111.75 -111.   -110.25 -109.5  -108.75 -108.   -107.25 -106.5\n",
      " -105.75 -105.   -104.25 -103.5  -102.75 -102.   -101.25 -100.5   -99.75\n",
      "  -99.    -98.25  -97.5   -96.75  -96.    -95.25  -94.5   -93.75  -93.\n",
      "  -92.25  -91.5   -90.75  -90.    -89.25  -88.5   -87.75  -87.    -86.25\n",
      "  -85.5   -84.75  -84.    -83.25  -82.5   -81.75  -81.    -80.25  -79.5\n",
      "  -78.75  -78.    -77.25  -76.5   -75.75  -75.    -74.25  -73.5   -72.75\n",
      "  -72.    -71.25  -70.5   -69.75  -69.    -68.25  -67.5   -66.75  -66.\n",
      "  -65.25  -64.5   -63.75  -63.    -62.25  -61.5   -60.75  -60.    -59.25\n",
      "  -58.5   -57.75  -57.    -56.25  -55.5   -54.75  -54.    -53.25  -52.5\n",
      "  -51.75  -51.    -50.25  -49.5   -48.75  -48.    -47.25  -46.5   -45.75\n",
      "  -45.    -44.25  -43.5   -42.75  -42.    -41.25  -40.5   -39.75  -39.\n",
      "  -38.25  -37.5   -36.75  -36.    -35.25  -34.5   -33.75  -33.    -32.25\n",
      "  -31.5   -30.75  -30.    -29.25  -28.5   -27.75  -27.    -26.25  -25.5\n",
      "  -24.75  -24.    -23.25  -22.5   -21.75  -21.    -20.25  -19.5   -18.75\n",
      "  -18.    -17.25  -16.5   -15.75  -15.    -14.25  -13.5   -12.75  -12.\n",
      "  -11.25  -10.5    -9.75   -9.     -8.25   -7.5    -6.75   -6.     -5.25\n",
      "   -4.5    -3.75   -3.     -2.25   -1.5    -0.75    0.      0.75    1.5\n",
      "    2.25    3.      3.75    4.5     5.25    6.      6.75    7.5     8.25\n",
      "    9.      9.75   10.5    11.25   12.     12.75   13.5    14.25   15.\n",
      "   15.75   16.5    17.25   18.     18.75   19.5    20.25   21.     21.75\n",
      "   22.5    23.25   24.     24.75   25.5    26.25   27.     27.75   28.5\n",
      "   29.25   30.     30.75   31.5    32.25   33.     33.75   34.5    35.25\n",
      "   36.     36.75   37.5    38.25   39.     39.75   40.5    41.25   42.\n",
      "   42.75   43.5    44.25   45.     45.75   46.5    47.25   48.     48.75\n",
      "   49.5    50.25   51.     51.75   52.5    53.25   54.     54.75   55.5\n",
      "   56.25   57.     57.75   58.5    59.25   60.     60.75   61.5    62.25\n",
      "   63.     63.75   64.5    65.25   66.     66.75   67.5    68.25   69.\n",
      "   69.75   70.5    71.25   72.     72.75   73.5    74.25   75.     75.75\n",
      "   76.5    77.25   78.     78.75   79.5    80.25   81.     81.75   82.5\n",
      "   83.25   84.     84.75   85.5    86.25   87.     87.75   88.5    89.25\n",
      "   90.     90.75   91.5    92.25   93.     93.75   94.5    95.25   96.\n",
      "   96.75   97.5    98.25   99.     99.75  100.5   101.25  102.    102.75\n",
      "  103.5   104.25  105.    105.75  106.5   107.25  108.    108.75  109.5\n",
      "  110.25  111.    111.75  112.5   113.25  114.    114.75  115.5   116.25\n",
      "  117.    117.75  118.5   119.25  120.    120.75  121.5   122.25  123.\n",
      "  123.75  124.5   125.25  126.    126.75  127.5   128.25  129.    129.75\n",
      "  130.5   131.25  132.    132.75  133.5   134.25  135.    135.75  136.5\n",
      "  137.25  138.    138.75  139.5   140.25  141.    141.75  142.5   143.25\n",
      "  144.    144.75  145.5   146.25  147.    147.75  148.5   149.25  150.\n",
      "  150.75  151.5   152.25  153.    153.75  154.5   155.25  156.    156.75\n",
      "  157.5   158.25  159.    159.75  160.5   161.25  162.    162.75  163.5\n",
      "  164.25  165.    165.75  166.5   167.25  168.    168.75  169.5   170.25\n",
      "  171.    171.75  172.5   173.25  174.    174.75  175.5   176.25  177.\n",
      "  177.75  178.5   179.25]\n",
      "[-180.   -179.25 -178.5  -177.75 -177.   -176.25 -175.5  -174.75 -174.\n",
      " -173.25 -172.5  -171.75 -171.   -170.25 -169.5  -168.75 -168.   -167.25\n",
      " -166.5  -165.75 -165.   -164.25 -163.5  -162.75 -162.   -161.25 -160.5\n",
      " -159.75 -159.   -158.25 -157.5  -156.75 -156.   -155.25 -154.5  -153.75\n",
      " -153.   -152.25 -151.5  -150.75 -150.   -149.25 -148.5  -147.75 -147.\n",
      " -146.25 -145.5  -144.75 -144.   -143.25 -142.5  -141.75 -141.   -140.25\n",
      " -139.5  -138.75 -138.   -137.25 -136.5  -135.75 -135.   -134.25 -133.5\n",
      " -132.75 -132.   -131.25 -130.5  -129.75 -129.   -128.25 -127.5  -126.75\n",
      " -126.   -125.25 -124.5  -123.75 -123.   -122.25 -121.5  -120.75 -120.\n",
      " -119.25 -118.5  -117.75 -117.   -116.25 -115.5  -114.75 -114.   -113.25\n",
      " -112.5  -111.75 -111.   -110.25 -109.5  -108.75 -108.   -107.25 -106.5\n",
      " -105.75 -105.   -104.25 -103.5  -102.75 -102.   -101.25 -100.5   -99.75\n",
      "  -99.    -98.25  -97.5   -96.75  -96.    -95.25  -94.5   -93.75  -93.\n",
      "  -92.25  -91.5   -90.75  -90.    -89.25  -88.5   -87.75  -87.    -86.25\n",
      "  -85.5   -84.75  -84.    -83.25  -82.5   -81.75  -81.    -80.25  -79.5\n",
      "  -78.75  -78.    -77.25  -76.5   -75.75  -75.    -74.25  -73.5   -72.75\n",
      "  -72.    -71.25  -70.5   -69.75  -69.    -68.25  -67.5   -66.75  -66.\n",
      "  -65.25  -64.5   -63.75  -63.    -62.25  -61.5   -60.75  -60.    -59.25\n",
      "  -58.5   -57.75  -57.    -56.25  -55.5   -54.75  -54.    -53.25  -52.5\n",
      "  -51.75  -51.    -50.25  -49.5   -48.75  -48.    -47.25  -46.5   -45.75\n",
      "  -45.    -44.25  -43.5   -42.75  -42.    -41.25  -40.5   -39.75  -39.\n",
      "  -38.25  -37.5   -36.75  -36.    -35.25  -34.5   -33.75  -33.    -32.25\n",
      "  -31.5   -30.75  -30.    -29.25  -28.5   -27.75  -27.    -26.25  -25.5\n",
      "  -24.75  -24.    -23.25  -22.5   -21.75  -21.    -20.25  -19.5   -18.75\n",
      "  -18.    -17.25  -16.5   -15.75  -15.    -14.25  -13.5   -12.75  -12.\n",
      "  -11.25  -10.5    -9.75   -9.     -8.25   -7.5    -6.75   -6.     -5.25\n",
      "   -4.5    -3.75   -3.     -2.25   -1.5    -0.75    0.      0.75    1.5\n",
      "    2.25    3.      3.75    4.5     5.25    6.      6.75    7.5     8.25\n",
      "    9.      9.75   10.5    11.25   12.     12.75   13.5    14.25   15.\n",
      "   15.75   16.5    17.25   18.     18.75   19.5    20.25   21.     21.75\n",
      "   22.5    23.25   24.     24.75   25.5    26.25   27.     27.75   28.5\n",
      "   29.25   30.     30.75   31.5    32.25   33.     33.75   34.5    35.25\n",
      "   36.     36.75   37.5    38.25   39.     39.75   40.5    41.25   42.\n",
      "   42.75   43.5    44.25   45.     45.75   46.5    47.25   48.     48.75\n",
      "   49.5    50.25   51.     51.75   52.5    53.25   54.     54.75   55.5\n",
      "   56.25   57.     57.75   58.5    59.25   60.     60.75   61.5    62.25\n",
      "   63.     63.75   64.5    65.25   66.     66.75   67.5    68.25   69.\n",
      "   69.75   70.5    71.25   72.     72.75   73.5    74.25   75.     75.75\n",
      "   76.5    77.25   78.     78.75   79.5    80.25   81.     81.75   82.5\n",
      "   83.25   84.     84.75   85.5    86.25   87.     87.75   88.5    89.25\n",
      "   90.     90.75   91.5    92.25   93.     93.75   94.5    95.25   96.\n",
      "   96.75   97.5    98.25   99.     99.75  100.5   101.25  102.    102.75\n",
      "  103.5   104.25  105.    105.75  106.5   107.25  108.    108.75  109.5\n",
      "  110.25  111.    111.75  112.5   113.25  114.    114.75  115.5   116.25\n",
      "  117.    117.75  118.5   119.25  120.    120.75  121.5   122.25  123.\n",
      "  123.75  124.5   125.25  126.    126.75  127.5   128.25  129.    129.75\n",
      "  130.5   131.25  132.    132.75  133.5   134.25  135.    135.75  136.5\n",
      "  137.25  138.    138.75  139.5   140.25  141.    141.75  142.5   143.25\n",
      "  144.    144.75  145.5   146.25  147.    147.75  148.5   149.25  150.\n",
      "  150.75  151.5   152.25  153.    153.75  154.5   155.25  156.    156.75\n",
      "  157.5   158.25  159.    159.75  160.5   161.25  162.    162.75  163.5\n",
      "  164.25  165.    165.75  166.5   167.25  168.    168.75  169.5   170.25\n",
      "  171.    171.75  172.5   173.25  174.    174.75  175.5   176.25  177.\n",
      "  177.75  178.5   179.25]\n"
     ]
    }
   ],
   "source": [
    "    # first check of grid\n",
    "    print(latitude_ERAI)\n",
    "    print(longitude_ERAI)\n",
    "    print(longitude_ORAS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** create mask *************************\n",
      "*********************** calc mask *************************\n",
      "*********************** packing *************************\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ###########                 global land-sea mask                      ###########\n",
    "    #################################################################################\n",
    "    sea_ice_mask_global = np.ones((len(latitude_ERAI),len(longitude_ERAI)),dtype=float)\n",
    "    sea_ice_mask_global[SIC_ERAI[0,0,:,:]==-1] = 0\n",
    "    #################################################################################\n",
    "    ###########                regionalization sea mask                   ###########\n",
    "    #################################################################################\n",
    "    print ('*********************** create mask *************************')\n",
    "    # W:-156 E:-124 N:80 S:67\n",
    "    mask_Beaufort = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:-156 N:80 S:66\n",
    "    mask_Chukchi = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:146 E:180 N:80 S:67\n",
    "    mask_EastSiberian = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:100 E:146 N:80 S:67\n",
    "    mask_Laptev = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:60 E:100 N:80 S:67\n",
    "    mask_Kara = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:18 E:60 N:80 S:64\n",
    "    mask_Barents = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-44 E:18 N:80 S:55\n",
    "    mask_Greenland = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:180 N:90 S:80\n",
    "    mask_CenArctic = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    print ('*********************** calc mask *************************')\n",
    "    mask_Beaufort[13:31,32:76] = 1\n",
    "\n",
    "    mask_Chukchi[13:32,0:32] = 1\n",
    "    mask_Chukchi[13:32,-1] = 1\n",
    "\n",
    "    mask_EastSiberian[13:31,434:479] = 1\n",
    "\n",
    "    mask_Laptev[13:31,374:434] = 1\n",
    "\n",
    "    mask_Kara[13:31,320:374] = 1\n",
    "\n",
    "    mask_Barents[13:36,264:320] = 1\n",
    "\n",
    "    mask_Greenland[13:47,179:264] = 1\n",
    "    mask_Greenland[26:47,240:264] = 0\n",
    "\n",
    "    mask_CenArctic[:13,:] = 1\n",
    "    print ('*********************** packing *************************')\n",
    "    mask_dict = {'Beaufort': mask_Beaufort[:,:],\n",
    "                 'Chukchi': mask_Chukchi[:,:],\n",
    "                 'EastSiberian': mask_EastSiberian[:,:],\n",
    "                 'Laptev': mask_Laptev[:,:],\n",
    "                 'Kara': mask_Kara[:,:],\n",
    "                 'Barents': mask_Barents[:,:],\n",
    "                 'Greenland': mask_Greenland[:,:],\n",
    "                 'CenArctic': mask_CenArctic[:,:]}\n",
    "    seas_namelist = ['Beaufort','Chukchi','EastSiberian','Laptev',\n",
    "                     'Kara', 'Barents', 'Greenland','CenArctic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########                  temporal interpolation matrix                  ########\n",
    "    #################################################################################\n",
    "    # interpolate from monthly to weekly\n",
    "    # original monthly data will be taken as the last week of the month\n",
    "    OHC_300_ORAS4_weekly_series = np.zeros(SIC_ERAI.reshape(len(year_ERAI)*48,len(latitude_ERAI),len(longitude_ERAI)).shape,\n",
    "                                           dtype=float)\n",
    "    OHC_300_ORAS4_series= dlacs.preprocess.operator.unfold(OHC_300_ORAS4)\n",
    "    # calculate the difference between two months\n",
    "    OHC_300_ORAS4_deviation_series = (OHC_300_ORAS4_series[1:,:,:] - OHC_300_ORAS4_series[:-1,:,:]) / 4\n",
    "    for i in np.arange(4):\n",
    "        OHC_300_ORAS4_weekly_series[3-i::4,:,:] = OHC_300_ORAS4_series[12:,:,:] - i * OHC_300_ORAS4_deviation_series[11:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  calculate extent from spatial fields  *******************\n",
      "================  reshape input data into time series  =================\n"
     ]
    }
   ],
   "source": [
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / 480\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "#     SFlux_ERAI_area = np.zeros(SFlux_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,:,i,:] = SIC_ERAI[:,:,i,:]* dx[i] * dy / 1E+6 # unit km2\n",
    "#         SFlux_ERAI_area[:,:,i,:] = SFlux_ERAI[:,:,i,:]* dx[i] * dy / 1E+12 # unit TeraWatt\n",
    "    SIC_ERAI_area[SIC_ERAI_area<0] = 0 # switch the mask from -1 to 0\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    SIC_ERAI_area_series = dlacs.preprocess.operator.unfold(SIC_ERAI_area)\n",
    "#     T2M_ERAI_series = dlacs.preprocess.operator.unfold(T2M_ERAI)\n",
    "#     SLP_ERAI_series = dlacs.preprocess.operator.unfold(SLP_ERAI)\n",
    "#     Z500_ERAI_series = dlacs.preprocess.operator.unfold(Z500_ERAI)\n",
    "#     Z850_ERAI_series = dlacs.preprocess.operator.unfold(Z850_ERAI)\n",
    "#     U10M_ERAI_series = dlacs.preprocess.operator.unfold(U10M_ERAI)\n",
    "#     V10M_ERAI_series = dlacs.preprocess.operator.unfold(V10M_ERAI)\n",
    "#     SFlux_ERAI_area_series = dlacs.preprocess.operator.unfold(SFlux_ERAI_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  choose the fields from target region  *******************\n",
      "******************  choose the fields from target region  *******************\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n"
     ]
    }
   ],
   "source": [
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select land-sea mask\n",
    "    sea_ice_mask_barents = sea_ice_mask_global[12:36,264:320]\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select the area between greenland and ice land for instance 60-70 N / 44-18 W\n",
    "    sic_exp = SIC_ERAI_area_series[:,12:36,264:320]\n",
    "#     t2m_exp = T2M_ERAI_series[:,12:36,264:320]\n",
    "#     slp_exp = SLP_ERAI_series[:,12:36,264:320]\n",
    "#     z500_exp = Z500_ERAI_series[:,12:36,264:320]\n",
    "#     z850_exp = Z850_ERAI_series[:,12:36,264:320]\n",
    "#     u10m_exp = U10M_ERAI_series[:,12:36,264:320]\n",
    "#     v10m_exp = V10M_ERAI_series[:,12:36,264:320]\n",
    "#     sflux_exp = SFlux_ERAI_area_series[:,12:36,264:320]\n",
    "    ohc_exp = OHC_300_ORAS4_weekly_series[:,12:36,264:320]\n",
    "    print(sic_exp.shape)\n",
    "#     print(t2m_exp.shape)\n",
    "#     print(slp_exp.shape)\n",
    "#     print(z500_exp.shape)\n",
    "#     print(u10m_exp.shape)\n",
    "#     print(v10m_exp.shape)\n",
    "#     print(sflux_exp.shape)\n",
    "    print(ohc_exp.shape)\n",
    "    print(latitude_ERAI[12:36])\n",
    "    print(longitude_ERAI[264:320])\n",
    "    print(latitude_ORAS4[12:36])\n",
    "    print(longitude_ORAS4[264:320])\n",
    "    #print(latitude_ERAI[26:40])\n",
    "    #print(longitude_ERAI[180:216])\n",
    "    #print(sic_exp[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  pre-processing  *********************\n",
      "=========================   normalize data   ===========================\n",
      "================  save the normalizing factor  =================\n",
      "1565.2049481856002 km2\n",
      "0.0 km2\n",
      "====================    A series of time (index)    ====================\n",
      "===================  artificial data for evaluation ====================\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('=========================   normalize data   ===========================')\n",
    "    sic_exp_norm = dlacs.preprocess.operator.normalize(sic_exp)\n",
    "#     t2m_exp_norm = deepclim.preprocess.operator.normalize(t2m_exp)\n",
    "#     slp_exp_norm = deepclim.preprocess.operator.normalize(slp_exp)\n",
    "#     z500_exp_norm = deepclim.preprocess.operator.normalize(z500_exp)\n",
    "#     z850_exp_norm = deepclim.preprocess.operator.normalize(z850_exp)\n",
    "#     u10m_exp_norm = deepclim.preprocess.operator.normalize(u10m_exp)\n",
    "#     v10m_exp_norm = deepclim.preprocess.operator.normalize(v10m_exp)\n",
    "#     sflux_exp_norm = deepclim.preprocess.operator.normalize(sflux_exp)\n",
    "    ohc_exp_norm = dlacs.preprocess.operator.normalize(ohc_exp)\n",
    "    print('================  save the normalizing factor  =================')\n",
    "    sic_max = np.amax(sic_exp)\n",
    "    sic_min = np.amin(sic_exp)\n",
    "    print(sic_max,\"km2\")\n",
    "    print(sic_min,\"km2\")\n",
    "    print ('====================    A series of time (index)    ====================')\n",
    "    _, yy, xx = sic_exp_norm.shape # get the lat lon dimension\n",
    "    year = np.arange(1979,2017,1)\n",
    "    year_cycle = np.repeat(year,48)\n",
    "    month_cycle = np.repeat(np.arange(1,13,1),4)\n",
    "    month_cycle = np.tile(month_cycle,len(year)+1) # one extra repeat for lead time dependent prediction\n",
    "    month_cycle.astype(float)\n",
    "    month_2D = np.repeat(month_cycle[:,np.newaxis],yy,1)\n",
    "    month_exp = np.repeat(month_2D[:,:,np.newaxis],xx,2)\n",
    "    print ('===================  artificial data for evaluation ====================')\n",
    "    # calculate climatology of SIC\n",
    "#     seansonal_cycle_SIC = np.zeros(48,dtype=float)\n",
    "#     for i in np.arange(48):\n",
    "#         seansonal_cycle_SIC[i] = np.mean(SIC_ERAI_sum_norm[i::48],axis=0)\n",
    "    # weight for loss\n",
    "#     weight_month = np.array([0,1,1,\n",
    "#                              1,0,0,\n",
    "#                              1,1,1,\n",
    "#                              0,0,0])\n",
    "    #weight_loss = np.repeat(weight_month,4)\n",
    "    #weight_loss = np.tile(weight_loss,len(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565.2049481856002\n",
      "0.0\n",
      "(1824, 24, 56)\n",
      "[[251.4346045  251.05600413 250.50871172 ... 244.2139345  244.13173904\n",
      "  244.01692955]\n",
      " [246.64799721 246.04600942 245.45885542 ... 245.36829478 245.24715214\n",
      "  245.12353721]\n",
      " [246.7402512  247.31849816 248.16998491 ... 246.47754407 246.37387686\n",
      "  246.26773736]\n",
      " ...\n",
      " [271.39089775 271.56084288 271.75937185 ... 272.40565781 271.30805881\n",
      "  270.25455477]\n",
      " [271.93968032 272.07067828 272.09157428 ... 273.35488503 272.4173081\n",
      "  271.39831464]\n",
      " [272.82825155 272.92514534 272.69329114 ... 274.15672261 273.34753587\n",
      "  272.70551716]]\n",
      "(24, 56)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print(t2m_exp.shape)\n",
    "    ax = plt.contourf(t2m_exp[443,:,:])\n",
    "    print(t2m_exp[443,:,:])\n",
    "    print(month_exp[0,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26267704 0.26046318 0.25661401 ... 0.22950646 0.22708498 0.22442972]\n",
      " [0.20302922 0.20179229 0.20073181 ... 0.23818486 0.23636252 0.23451899]\n",
      " [0.19524795 0.20810658 0.22415041 ... 0.25322432 0.25219251 0.25112267]\n",
      " ...\n",
      " [0.48474634 0.48954066 0.49081374 ... 0.36213768 0.336037   0.3028926 ]\n",
      " [0.49387301 0.5059012  0.52108218 ... 0.37193518 0.34423099 0.31239209]\n",
      " [0.52729484 0.54777388 0.5776589  ... 0.38700331 0.35481465 0.32946398]]\n",
      "(4, 24, 56)\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  pretest input shape  *********************')\n",
    "    x_input = np.stack((sic_exp_norm[1,:,:],\n",
    "                        t2m_exp_norm[1,:,:],\n",
    "                        ohc_exp_norm[1,:,:],\n",
    "                        month_exp[1,:,:]))\n",
    "    print(x_input[1,:,:])\n",
    "    print(x_input[:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  parameter for check  *********************\n",
      "*******************  create basic dimensions for tensor and network  *********************\n",
      "*******************  cross validation and testing data  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  parameter for check  *********************')\n",
    "    choice_exp_norm = ohc_exp_norm\n",
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    # specifications of neural network\n",
    "    input_channels = 3\n",
    "    hidden_channels = [3, 2, 1] # number of channels & hidden layers, the channels of last layer is the channels of output, too\n",
    "    #hidden_channels = [3, 3, 3, 3, 2]\n",
    "    #hidden_channels = [2]\n",
    "    kernel_size = 3\n",
    "    # here we input a sequence and predict the next step only\n",
    "    #step = 1 # how many steps to predict ahead\n",
    "    #effective_step = [0] # step to output\n",
    "    batch_size = 1\n",
    "    #num_layers = 1\n",
    "    learning_rate = 0.005\n",
    "    num_epochs = 1500\n",
    "    print ('*******************  cross validation and testing data  *********************')\n",
    "    # take 10% data as cross-validation data\n",
    "    cross_valid_year = 4\n",
    "    # take 10% years as testing data\n",
    "    test_year = 4\n",
    "    # minibatch\n",
    "    #iterations = 3 # training data divided into 3 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  check the environment  *********************\n",
      "Pytorch version 1.1.0\n",
      "Is CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  check the environment  *********************')\n",
    "    print (\"Pytorch version {}\".format(torch.__version__))\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    # CUDA settings torch.__version__ must > 0.4\n",
    "    # !!! This is important for the model!!! The first option is gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  preview of input tensor  *********************\n",
      "1824\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  preview of input tensor  *********************')\n",
    "    #plt.plot(SIC_ERAI_sum)\n",
    "    #print(SIC_ERAI_sum_norm[:-test_year*12])\n",
    "    #print(x_input.shape)\n",
    "    #print(x_input[:,:,:])\n",
    "    sequence_len, _, _ = sic_exp_norm.shape\n",
    "    print(sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print ('*******************  module for calculating accuracy  *********************')\n",
    "#     def accuracy(out, labels):\n",
    "#         outputs = np.argmax(out, axis=1)\n",
    "#     return np.sum(outputs==labels)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  load exsited LSTM model  *********************\n",
      "ConvLSTM(\n",
      "  (cell0): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell1): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell2): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print ('*******************  load exsited LSTM model  *********************')\n",
    "    # load model parameters\n",
    "    model = dlacs.BayesConvLSTM.BayesConvLSTM(input_channels, hidden_channels, kernel_size).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(output_path, 'Barents','bayesconvlstm_era_sic_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl')))\n",
    "    # load entire model\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_z850_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_t2m_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_sflux_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_slp_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_z850_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    print(model)\n",
    "    # check the sequence length (dimension in need for post-processing)\n",
    "    sequence_len, height, width = sic_exp_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  run LSTM  *********************\n",
      "The model is designed to make many to one prediction.\n",
      "A series of multi-chanel variables will be input to the model.\n",
      "The model learns by verifying the output at each timestep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\torch\\nn\\_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM(\n",
      "  (cell0): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell1): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell2): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.005\n",
      "    weight_decay: 0\n",
      ")\n",
      "##############################################################\n",
      "##################  start training loop  #####################\n",
      "##############################################################\n",
      "Epoch  0 MSE:  47.379188537597656\n",
      "Epoch  50 MSE:  12.66511058807373\n",
      "Epoch  100 MSE:  5.799999237060547\n",
      "Epoch  150 MSE:  4.382219314575195\n",
      "Epoch  200 MSE:  3.6787402629852295\n",
      "Epoch  250 MSE:  3.3828399181365967\n",
      "Epoch  300 MSE:  3.1627047061920166\n",
      "Epoch  350 MSE:  2.9865870475769043\n",
      "Epoch  400 MSE:  2.848874807357788\n",
      "Epoch  450 MSE:  2.7484591007232666\n",
      "Epoch  500 MSE:  2.663966655731201\n",
      "Epoch  550 MSE:  2.5547988414764404\n",
      "Epoch  600 MSE:  2.498825788497925\n",
      "Epoch  650 MSE:  2.5437891483306885\n",
      "Epoch  700 MSE:  2.4073853492736816\n",
      "Epoch  750 MSE:  2.3751778602600098\n",
      "Epoch  800 MSE:  2.3555850982666016\n",
      "Epoch  850 MSE:  2.3165905475616455\n",
      "Epoch  900 MSE:  2.301396369934082\n",
      "Epoch  950 MSE:  2.2788639068603516\n",
      "Epoch  1000 MSE:  2.252392053604126\n",
      "Epoch  1050 MSE:  2.3186490535736084\n",
      "Epoch  1100 MSE:  2.23225998878479\n",
      "Epoch  1150 MSE:  2.192002773284912\n",
      "Epoch  1200 MSE:  2.195979118347168\n",
      "Epoch  1250 MSE:  2.159921884536743\n",
      "Epoch  1300 MSE:  2.22209095954895\n",
      "Epoch  1350 MSE:  2.1580252647399902\n",
      "Epoch  1400 MSE:  2.132978916168213\n",
      "Epoch  1450 MSE:  2.1170670986175537\n",
      "Wall time: 9h 35min 7s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print ('*******************  run BayesConvLSTM  *********************')\n",
    "    print ('The model is designed to make many to one prediction.')\n",
    "    print ('A series of multi-chanel variables will be input to the model.')\n",
    "    print ('The model learns by verifying the output at each timestep.')\n",
    "    # check the sequence length\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "\n",
    "    # initialize our model\n",
    "    model = dlacs.BayesConvLSTM.BayesConvLSTM(input_channels, hidden_channels, kernel_size).to(device)\n",
    "    loss_fn = torch.nn.NllLoss(size_average=True)\n",
    "    # stochastic gradient descent\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # Adam optimizer\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(model)\n",
    "    print(loss_fn)\n",
    "    print(optimiser)\n",
    "    print('##############################################################')\n",
    "    print('#############  preview model parameters matrix  ###############')\n",
    "    print('##############################################################')\n",
    "    print('Number of parameter matrices: ', len(list(model.parameters())))\n",
    "    for i in range(len(list(model.parameters()))):\n",
    "        print(list(model.parameters())[i].size())\n",
    "    print('##############################################################')\n",
    "    print('##################  start training loop  #####################')\n",
    "    print('##############################################################')\n",
    "    hist = np.zeros(num_epochs)\n",
    "    # loop of epoch\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # loop of timestep\n",
    "        for timestep in range(sequence_len - cross_valid_year*12*4 - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #################################################################################\n",
    "            ########          create input tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                choice_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).cuda()\n",
    "            #################################################################################\n",
    "            ########       create training tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            y_train_stack = sic_exp_norm[timestep+1,:,:] #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).cuda()\n",
    "            #################################################################################   \n",
    "            # Forward pass\n",
    "            y_pred, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_train = y_var        \n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            if timestep == 0:\n",
    "                loss = loss_fn(y_pred, y_train)\n",
    "            else:\n",
    "                loss += loss_fn(y_pred, y_train)\n",
    "            #print (timestep)\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 50 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            #print(y_pred)\n",
    "            # gradient check\n",
    "            # Gradcheck requires double precision numbers to run\n",
    "            #res = torch.autograd.gradcheck(loss_fn, (y_pred.double(), y_train.double()), eps=1e-6, raise_exception=True)\n",
    "            #print(res)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    # save the model\n",
    "    # (recommended) save the model parameters only\n",
    "    torch.save(model.state_dict(), os.path.join(output_path,'bayesconvlstm.pkl'))\n",
    "    # save the entire model\n",
    "    # torch.save(model, os.path.join(output_path,'bayesconvlstm.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  Loss with time  **********************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X1wHPWd5/H3t+dRz7JlyRbIxDYQ8EOw41VYWNiQAGHzsHm4DbkCloTNccflLpdiL5faOA+1u6FSW5DsJVlS2SVUAqESloQN2YOlkriyhCTLkrIRDs/GK/NkZIQt2ZZkPc1MT//uj24JyZZtWZY00zOfV5VLMz09M1/95Pn0b379625zziEiIvHnlboAERGZHwp0EZEKoUAXEakQCnQRkQqhQBcRqRAKdBGRCqFAFxGpEAp0EZEKoUAXEakQycV8s2XLlrlVq1Yt5luKiMTe448/3u+caz3Reosa6KtWraKrq2sx31JEJPbM7JXZrKchFxGRCqFAFxGpEAp0EZEKsahj6CJSfgqFAj09PYyPj5e6lKqXzWbp6OgglUrN6fkKdJEq19PTQ0NDA6tWrcLMSl1O1XLOceDAAXp6eli9evWcXkNDLiJVbnx8nJaWFoV5iZkZLS0tp/RNSYEuIgrzMnGqf4dYBPo//66Hu7fNahqmiEjVikWgP/hkL/ds31PqMkRkARw4cIBNmzaxadMmVqxYwemnnz55P5/Pz+o1Pv7xj7Nr167jrvOtb32Lu+++ez5K5uKLL+aJJ56Yl9eaT7HYKZpJeYwXglKXISILoKWlZTIc//qv/5r6+no+85nPTFvHOYdzDs+buQ965513nvB9PvnJT556sWUuFj30bDJBzi+WugwRWUS7d+9mw4YNfOITn2Dz5s309vZyww030NnZyfr167npppsm153oMfu+T3NzM1u2bGHjxo1ceOGF7N+/H4AvfvGLfOMb35hcf8uWLZx//vmcc845PProowCMjIzw4Q9/mI0bN3L11VfT2dl5wp74D37wA97ylrewYcMGPv/5zwPg+z4f/ehHJ5ffeuutAHz9619n3bp1bNy4kWuvvXbe20w9dBGZ9KV/eZbnXhua19dcd1ojf/X+9XN67nPPPcedd97JbbfdBsDNN9/M0qVL8X2fd77znVx55ZWsW7du2nMGBwe55JJLuPnmm/n0pz/NHXfcwZYtW456becc27dv54EHHuCmm27i5z//Od/85jdZsWIF9913H08++SSbN28+bn09PT188YtfpKuri6amJi6//HIefPBBWltb6e/v5+mnnwZgYGAAgK985Su88sorpNPpyWXzKRY99EwywXhBPXSRanPmmWfytre9bfL+Pffcw+bNm9m8eTM7d+7kueeeO+o5NTU1vOc97wHg937v93j55ZdnfO0/+ZM/OWqdRx55hKuuugqAjRs3sn798TdE27Zt49JLL2XZsmWkUimuueYafvOb33DWWWexa9cubrzxRrZu3UpTUxMA69ev59prr+Xuu++e88FDxxOLHno2lSDnq4custDm2pNeKHV1dZO3u7u7+bu/+zu2b99Oc3Mz11577YxzttPp9OTtRCKB7/szvnYmkzlqHefcSdV3rPVbWlp46qmn+NnPfsatt97Kfffdx+23387WrVv59a9/zf3338+Xv/xlnnnmGRKJxEm95/HEpIfukfcDguDkGltEKsfQ0BANDQ00NjbS29vL1q1b5/09Lr74Yu69914Ann766Rm/AUx1wQUX8PDDD3PgwAF83+eHP/whl1xyCX19fTjn+MhHPsKXvvQlduzYQbFYpKenh0svvZSvfvWr9PX1MTo6Oq/1x6aHDpDzA2rS87c1E5H42Lx5M+vWrWPDhg2sWbOGiy66aN7f41Of+hQf+9jHOO+889i8eTMbNmyYHC6ZSUdHBzfddBPveMc7cM7x/ve/n/e9733s2LGD66+/HuccZsYtt9yC7/tcc801HD58mCAI+OxnP0tDQ8O81m8n+xXjVHR2drq5XODijkde4qYHn+OJv3wXzbXpEz9BRGZt586drF27ttRllAXf9/F9n2w2S3d3N1dccQXd3d0kk4vX953p72FmjzvnOk/03Fj10DXTRUQW0vDwMJdddhm+7+Oc49vf/vaihvmpikWl2VQ41K+ZLiKykJqbm3n88cdLXcacxWSnaNRD18FFIgtiMYde5dhO9e8Qi0Cvy4SBPjw+8/QjEZm7bDbLgQMHFOolNnE+9Gw2O+fXiMWQS1tD+Av2Hc6VuBKRytPR0UFPTw99fX2lLqXqTVyxaK5iEejLG8MDAPYN6RJZIvMtlUrN+Qo5Ul5iMeSypDZN0jP2qYcuInJMsQh0zzPam7PsOTi/R1WJiFSSWAQ6wPr2Jp7dO1jqMkREylZsAn3D6Y28fGCUofFCqUsRESlLMQr08HwKz+6d33M1i4hUivgF+msadhERmUlsAn1ZfYbWhgw7ew+XuhQRkbIUm0AHWNveyK59GnIREZlJrAL9TUtr6Tk0VuoyRETKUqwC/bTmGgZGC4zkdE4XEZEjxSzQw3O6vDagXrqIyJFiFejLG8NA369TAIiIHCVWgb4kuvzcwKgOLhIROVKsAr25NgXAwFi+xJWIiJSfWQe6mSXM7Hdm9mB0f7WZbTOzbjP7kZkt+NWbm2qiQFcPXUTkKCfTQ78R2Dnl/i3A151zZwOHgOvns7CZZFMJsimPwTEFuojIkWYV6GbWAbwP+E5034BLgR9Hq9wFfGghCjzSkto0h0Y05CIicqTZ9tC/AfwFEET3W4AB59zEhPAe4PSZnmhmN5hZl5l1zcclrhqzKfXQRURmcMJAN7M/BvY75x6funiGVWe8wqxz7nbnXKdzrrO1tXWOZb6hqSalU+iKiMxgNtcUvQj4gJm9F8gCjYQ99mYzS0a99A7gtYUr8w2NNUn2DujaoiIiRzphD9059znnXIdzbhVwFfBL59yfAg8DV0arXQfcv2BVTtFYk2JIQy4iIkc5lXnonwU+bWa7CcfUvzs/JR1fY1aBLiIyk9kMuUxyzv0K+FV0+0Xg/Pkv6fiaalIczvkUA0fCm2koX0SkOsXqSFEIh1wAhsd1xkURkaliF+gTR4tq6qKIyHSxC/TGbDhKpKmLIiLTxS7Q1UMXEZlZ7AJ9YgxdM11ERKaLXaCrhy4iMrPYBXqjAl1EZEaxC/S6dIKEZwp0EZEjxC7QzYymGp1xUUTkSLELdIBmBbqIyFFiGeiNCnQRkaPEMtCbdMZFEZGjxDbQBxToIiLTxDbQNeQiIjJdbAN9aKxAEMx41TsRkaoUy0Bvrk0ROBjO6xS6IiITYhnok0eLjmrYRURkQiwDXedzERE5WqwDXVMXRUTeEOtA19RFEZE3xDrQNeQiIvKGWAZ6c60CXUTkSLEM9JpUglRCp9AVEZkqloGuU+iKiBwtloEOOuOiiMiRYhvoOuOiiMh0sQ70AR0pKiIyKdaBPjSuQBcRmRDbQM8mE+QKQanLEBEpG7EN9HTSI19UoIuITIh3oPsKdBGRCQp0EZEKEd9AT4RDLs7pqkUiIhDnQE+GpWscXUQkdMJAN7OsmW03syfN7Fkz+1K0fLWZbTOzbjP7kZmlF77cN2QmAl3DLiIiwOx66DngUufcRmAT8G4zuwC4Bfi6c+5s4BBw/cKVebS0Al1EZJoTBroLDUd3U9E/B1wK/DhafhfwoQWp8BjSCQ25iIhMNasxdDNLmNkTwH7gF8ALwIBzzo9W6QFOX5gSZ6YeuojIdLMKdOdc0Tm3CegAzgfWzrTaTM81sxvMrMvMuvr6+uZe6RFSCQW6iMhUJzXLxTk3APwKuABoNrNk9FAH8NoxnnO7c67TOdfZ2tp6KrVOM9FDzynQRUSA2c1yaTWz5uh2DXA5sBN4GLgyWu064P6FKnImmrYoIjJd8sSr0A7cZWYJwg3Avc65B83sOeCHZvZl4HfAdxewzqNkNOQiIjLNCQPdOfcU8NYZlr9IOJ5eEtopKiIyXeyPFC1oyEVEBKiAQFcPXUQkFN9A14FFIiLTxDfQNW1RRGSa2Ae6hlxEREKxDfRMIgEo0EVEJsQ20HVgkYjIdPEPdPXQRUSAGAd6wjM8A189dBERIMaBDpBMeOSLuqaoiAjEPNDTCU9HioqIRGId6KmEKdBFRCIxD3SPgoZcRESAigh09dBFRCDmgZ5OKtBFRCbEOtBf6h/h/ide01x0ERFiHugTeg6NlroEEZGSq4hAT3hW6hJEREquIgJdQy4iIjEP9C9/aAOgc6KLiEDMA/2MpbUA5PxiiSsRESm9WAd6ZuKqRQX10EVE4h3oqfAiFxpyERGJe6BPXldUQy4iIhUS6Oqhi4jEO9Anhlw0hi4iEvNAn+ih63wuIiIVEugFjaGLiMQ60NMaQxcRmRTvQE94pBMeh8f9UpciIlJysQ50M6OlPs2B4VypSxERKblYBzpAS32afgW6iEgFBHpdhgMj+VKXISJScvEP9Po0/YfVQxcRiX2gtzVk6RvO4ZwrdSkiIiV1wkA3s5Vm9rCZ7TSzZ83sxmj5UjP7hZl1Rz+XLHy5R2tryFAoOg6NFkrx9iIiZWM2PXQf+D/OubXABcAnzWwdsAV4yDl3NvBQdH/RNdWkADg8rkAXkep2wkB3zvU653ZEtw8DO4HTgQ8Cd0Wr3QV8aKGKPJ66THg+l5GcjhYVkep2UmPoZrYKeCuwDVjunOuFMPSBtvkubjbqMkkARvM6uEhEqtusA93M6oH7gD93zg2dxPNuMLMuM+vq6+ubS43HVZsOA30krx66iFS3WQW6maUIw/xu59xPosX7zKw9erwd2D/Tc51ztzvnOp1zna2trfNR8zQTQy6jOfXQRaS6zWaWiwHfBXY657425aEHgOui29cB989/eSdWpx66iAgAyVmscxHwUeBpM3siWvZ54GbgXjO7HtgDfGRhSjy+2nTUQ9cYuohUuRMGunPuEcCO8fBl81vOyZvYKapZLiJS7WJ/pGgm6eGZeugiIrEPdDOjLp1kWDtFRaTKxT7QAWozCUY15CIiVa4iAr0unWREQy4iUuUqI9AzSYZ0GToRqXIVEehrWut4Yf9wqcsQESmpigj0Ny2tpXdwDL8YlLoUEZGSqYhAX96UJXDQp2uLikgVq4hAX1qbBmBAF7kQkSpWEYE+cZELBbqIVLPKCPTaMNAHxxToIlK9KiLQm6Mhl8GxfIkrEREpnYoIdA25iIhUSKDXpRMkPdOQi4hUtYoIdDOjqSbFgAJdRKpYRQQ6hDtG1UMXkWpWMYHeXJNiUGPoIlLFKibQwyEXzXIRkepVMYHeXJvm9UEd+i8i1atiAr2pJkX/cI7XBsZKXYqISElUTKBfsX45ALteP1ziSkRESqNiAn3NsnoA9qqHLiJVqmICfVl9Gs9g/9B4qUsRESmJign0ZMJjWX2GfUPaMSoi1aliAh1geWOW19VDF5EqVWGBnmGfAl1EqlSFBXqW3sFxnHOlLkVEZNFVVKCf297I4FiBPQdHS12KiMiiq6hAP3/VUgAee/lQiSsREVl8FRXoZ7fV01ST4rGXDpa6FBGRRVdRge55xqaVzTy1d7DUpYiILLqKCnQIe+kv9Q8TBNoxKiLVpeIC/cy2esYLAa8e0o5REakuFRfob1u1BIDfvnCgxJWIiCyuigv0M1vracwmebJH4+giUl1OGOhmdoeZ7TezZ6YsW2pmvzCz7ujnkoUtc/bMjPM6mnl670CpSxERWVSz6aF/D3j3Ecu2AA85584GHorul43zOpp4vvcw44ViqUsREVk0Jwx059xvgCMndn8QuCu6fRfwoXmu65Sc19GEHzh29g6VuhQRkUUz1zH05c65XoDoZ9uxVjSzG8ysy8y6+vr65vh2J+e8jmYAntZ8dBGpIgu+U9Q5d7tzrtM519na2rrQbwdAe1OWZfUZnnxVgS4i1WOugb7PzNoBop/756+kUxfuGG3iqR7tGBWR6jHXQH8AuC66fR1w//yUM382n9HM7r5hXZJORKrGbKYt3gP8FjjHzHrM7HrgZuBdZtYNvCu6X1becU4bzsFvX9QBRiJSHZInWsE5d/UxHrpsnmuZV2vbG2nIJnmku58Pbjq91OWIiCy4ijtSdELCMy47t41f7NxHoRiUuhwRkQVXsYEO8N63tDMwWuBRnddFRKpARQf629/cSn0myU+f6i11KSIiC66iAz2bSnD52ja2Pve6hl1EpOJVdKCDhl1EpHpUfKBPDLv8846eUpciIrKgKj7Qs6kEH+ns4F+e6uXVg7qKkYhUrooPdIAb3r6GhBn/8OsXSl2KiMiCqYpAb2+q4SOdHdz72Ku82Ddc6nJERBZEVQQ6wI2Xn00m6fFXDzxLELhSlyMiMu+qJtDbGrJ8/n1r+bfufu589OVSlyMiMu+qJtABrjn/DN61bjl/89OdbH/pyIswiYjEW1UFupnxtf+8kfamLNd/7zEefaG/1CWJiMybqgp0gIZsinv/+4WsaMryZ3c8xr88+VqpSxIRmRdVF+gApzXX8E+fuJCNK5v41D2/49aHuvF1agARibmqDHSA5to037/+9/nAxtP42i/+gw/f9lueeFWXrBOR+KraQIfwKNJbr34rt179VvYeGuVD3/p3PveTp9m9/3CpSxMROWknvGJRNfjAxtO45M2tfPOhbr777y9xz/Y9vGfDCj75zrPYcHpTqcsTEZkVc27xDrLp7Ox0XV1di/Z+c7F3YIy/f3g3d2/bA8D60xq54e1ryPkBHUtq+IMzl5W4QhGpNmb2uHOu84TrKdBnNjha4B+37+EnO3ro3v/G6QK+8N61XPP7Z1CX0ZcbEVkcCvR5Ugwc/9bdx71dr7L9pYP0D+cBOHdFA401Kf7gzBauWLeCNa11ZFOJElcrIpVIgb4AnHP86879/FPXq/QP59ixZ/qsmAvXtPDOc1s5q62epXUZzl3RoJAXkVOmQF8Eh0by/OvOfXzv0Zd59rWhGddJeMYfrV/OWW0NnLG0lsA56jNJNq1spqU+TSapwBeR41Ogl8j+w+Psev0wv33hAI+/coieQ2OM5n0OjRaOWtcMVrfUkfMDljdmaKpJcW57I/WZJGaQ9wPWn9ZEx5IaltVncDiee22I81cvpTatMXyRajHbQFcqzLO2hixtDVn+8OzWactHcj79wzlG80Ve7h/hxf4RhsYLvHpwlKTnsefgKN37h/m37n78WZzed1l9muWNWVY0ZmmuTZNNeWRTifBnMkHPoTEaa5Kc1VbPkto0Z7XVU59N4plRn0mS8IxUoqoPQxCpOAr0RVKXSU7OjFnb3njcdccLRcYLRfqH8wyOFegdHKN3YByHY99Qjt7BMQAGRgu8emiU518/PPmcnB/MaoMAcHpzDalEGOxNNSmaa9OYhQdctdSlyaYSNEQbgaaaFDVpj7aGLIViwJLaNANjBVrq0iypS9NSl+aZvYPkiwGbz1gy4wbj1YOjpBIeK5qyc2hBETkRBXoZCnvaCZpr09GSJSf1fL8YUHSO3fuHGckVyfsBrw+NM5b3AegbzlMMAnoHxik6R94PGBwLvy2YweBYgcGxAoViQKE4tyG5hBduBLJJj7pMkqJzvNg3AsDZbfVkUh7LG7Iczvm01KVpzKZwOOoySfJ+QKEYkE56ZJIJ8n7Aac01NNemSCc8duw5xMBYgYvOXEZN2qMunSSbSpBMGC/2jfCb/+hj48pmVi+r483LG/AMHDA87vPQzn2c297IqpY6atIJltalqU0neH1wnK5XDnLRWctoyKSoSb+xb2O8UOTASJ72xix7B8b45fP7ecc5rZMbwePZ/tJBPvmPOzDCcwh957pOltVnjvucYuDY9uIBzlvZTL2mx87o+deHWLmkVtOHj6AxdDmmIHDk/HDjMDhW4NBInpGcT8Iz+ofztNSnOTCco284z+HxAumEx9BYgVwxwC86xgtFBscKFAOH5xmHx30GxwqkE4ZzMJIvUp9JcGA4z3Au3NgMjReoSSUIXLhRGC8UcQ7GCsVF/d09g6Tn4XCTG7V00iPvTz+JW1NNinTSI+UZyYRHMmEkPSPpeaQSxpM9gzO+/rkrGqhNJyY33tmURyrhUQwc+4dybH85PF//yqU1rGqpoz6TZGldmqRnmBmphOF5Rt9QjqFxn1UttWRSHn4xbOvhcZ9CMaAhm6QxmyKT8jgwnKc+k6QQOPxiwN//6gXOXdHAutMa2bSyGefA84xX+kfwPGPl0tqoHYx80dE7MEZTTYpM0iNfDNg3lKM2nWBFU5b6TJJM0qM+k+Kl/mH+5qfPM1YocvX5ZxAEjvWnN3JWWz04yKQSpBMeXa8cZM/BUdqbsqxeVk9bQ4ZsKkEx+oaZj06YN5LzWdGUJeV5+EGAAy77v78G4JzlDVy6to3TmmvIJD3OX7WUZCJso/p0Es+DIACHYyLq6jJJ8sWAobECS2rTJBMGhL/neCGIvl2GywIX/l8oBg4HFIrBtP1XQeDYd3ictoYswzmfVMIYyxdJJjwas0nM7NT/M6KdolJBioFjrFAMNxZ+QEt9mpGcT8F3DOd8xgpFnHPko6GgvB/QdzgHwEj0rQTC8+EbcHjcxzMoBI7RnE/eD8ikPAzDDMbyRfLFcOgqCBzJhNGYTdF3OIcfOA6M5KlNJTCDZMIoBmHo+9Fz/KLDD6Lnu3C663/7wzV0vXKIR7r7SHjh6+X8IBwq84uMF8JvJZ4ZeT9g70A4rHb+qqWM++HvPjTuUwwcgQvfoxjNmCoG4cbTDxwJz3DO4ZkRODftG5ZnYUDJiaWTHkHg8ANHOunhFwOSnkchCMgkPZwL18kVgskNz0yyqfAbZE06wfev/31WL6ubUz3aKSoVI+GFO3KnDj80ZlMlrGhu3v7mVj79rjcv2Os7547qEfrRsFnOL1KfSRK4sCd6cDQfzpxyjmLg6B/OU3SOZDRU1nc4RzJhjOaLFANHYzbFsvo0fcM5coVwOKy9Kcu+oRyDYwVG8z6phMdovkgyYaxqqSOVMPYOjDGS88kVAgIHRRduJPPFgMZsikOjeQrFgLp0krFCETPwot8hmwqDM+8H5PyAnF/EMHJ+kTWt9axtb+TgSJ6OJTXsGxpn+0sHSXhhzamEMZwrUgzCIIZwVhmE+55q0wlq0gmGcz7FaKM3Ed4jOX9y45gwC799Jj08CzfeZkzZ2HtkUx69g+O82DdCNuUxMFogm0qwojHLkrr05O+7GMNn6qGLiJS52fbQNW9NRKRCKNBFRCqEAl1EpEKcUqCb2bvNbJeZ7TazLfNVlIiInLw5B7qZJYBvAe8B1gFXm9m6+SpMREROzqn00M8HdjvnXnTO5YEfAh+cn7JERORknUqgnw68OuV+T7RMRERK4FQCfaZjWo+a1G5mN5hZl5l19fX1ncLbiYjI8ZzKoUs9wMop9zuA145cyTl3O3A7gJn1mdkrc3y/ZUD/HJ+7WMq9xnKvD8q/xnKvD1TjfCi3+t40m5XmfKSomSWB/wAuA/YCjwHXOOeendMLnvj9umZzpFQplXuN5V4flH+N5V4fqMb5UO71Hcuce+jOOd/M/hewFUgAdyxUmIuIyImd0tlinHM/BX46T7WIiMgpiNORoreXuoBZKPcay70+KP8ay70+UI3zodzrm9Ginm1RREQWTpx66CIichyxCPRyOGeMma00s4fNbKeZPWtmN0bLl5rZL8ysO/q5JFpuZnZrVPNTZrZ5kepMmNnvzOzB6P5qM9sW1fcjM0tHyzPR/d3R46sWqb5mM/uxmT0fteWFZdiG/zv6Gz9jZveYWbbU7Whmd5jZfjN7Zsqyk243M7suWr/bzK5b4Pq+Gv2dnzKzfzaz5imPfS6qb5eZ/dGU5Qv2WZ+pximPfcbMnJkti+4vehvOC+dcWf8jnEHzArAGSANPAutKUEc7sDm63UA4ZXMd8BVgS7R8C3BLdPu9wM8ID8C6ANi2SHV+GvhH4MHo/r3AVdHt24D/Ed3+n8Bt0e2rgB8tUn13Af81up0GmsupDQmPdn4JqJnSfn9W6nYE3g5sBp6Zsuyk2g1YCrwY/VwS3V6ygPVdASSj27dMqW9d9DnOAKujz3dioT/rM9UYLV9JOFvvFWBZqdpwXn7HUhcwiz/ChcDWKfc/B3yuDOq6H3gXsAtoj5a1A7ui298Grp6y/uR6C1hTB/AQcCnwYPSfsX/Kh2qyLaP/wBdGt5PRerbA9TVGYWlHLC+nNpw4pcXSqF0eBP6oHNoRWHVEYJ5UuwFXA9+esnzaevNd3xGP/Sfg7uj2tM/wRBsuxmd9phqBHwMbgZd5I9BL0oan+i8OQy5ld86Y6Gv1W4FtwHLnXC9A9LMtWq0UdX8D+Atg4qq1LcCAc27iSslTa5isL3p8MFp/Ia0B+oA7o2Gh75hZHWXUhs65vcDfAnuAXsJ2eZzyascJJ9tupfws/RfCHi/HqWPR6zOzDwB7nXNPHvFQ2dR4MuIQ6LM6Z8xiMbN64D7gz51zQ8dbdYZlC1a3mf0xsN859/gsayhFuyYJv/L+g3PurcAI4VDBsSx6jdE49AcJhwJOA+oITxF9rDrK6v9n5Fg1laRWM/sC4AN3Tyw6Rh2L/ZmpBb4A/OVMDx+jlnL8e0+KQ6DP6pwxi8HMUoRhfrdz7ifR4n1m1h493g7sj5Yvdt0XAR8ws5cJT2V8KWGPvdnC0zQcWcNkfdHjTcDBBaxv4j17nHPbovs/Jgz4cmlDgMuBl5xzfc65AvAT4A8or3accLLttujtGe00/GPgT100RlFG9Z1JuOF+MvrcdAA7zGxFGdV4UuIQ6I8BZ0ezDNKEO54eWOwizMyA7wI7nXNfm/LQA8DEnu7rCMfWJ5Z/LNpbfgEwOPH1eCE45z7nnOtwzq0ibKNfOuf+FHgYuPIY9U3UfWW0/oL2NJxzrwOvmtk50aLLgOcokzaM7AEuMLPa6G8+UWPZtOMUJ9tuW4ErzGxJ9E3kimjZgjCzdwOfBT7gnBs9ou6rohlCq4Gzge0s8mfdOfe0c67NObcq+tz0EE58eJ0yacOTVupB/FnuyHgv4aySF4AvlKiGiwm/Wj0FPBH9ey/heOlDQHf0c2m0vhFe0ekF4GmgcxFrfQdvzHJZQ/iAMjtsAAAApElEQVRh2Q38E5CJlmej+7ujx9csUm2bgK6oHf8f4UyBsmpD4EvA88AzwPcJZ2OUtB2BewjH9AuEwXP9XNqNcCx7d/Tv4wtc327C8eaJz8ttU9b/QlTfLuA9U5Yv2Gd9phqPePxl3tgpuuhtOB//dKSoiEiFiMOQi4iIzIICXUSkQijQRUQqhAJdRKRCKNBFRCqEAl1EpEIo0EVEKoQCXUSkQvx/E4ccakt1tcYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print (\"*******************  Loss with time  **********************\")\n",
    "    fig00 = plt.figure()\n",
    "    try:\n",
    "        plt.plot(hist, label=\"Training loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig00.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred_error.png'),dpi=200)\n",
    "    except:\n",
    "        print('Model is reloaded instead of trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  evaluation matrix  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  evaluation matrix  *********************')\n",
    "    # The prediction will be evaluated through RMSE against climatology\n",
    "    \n",
    "    # error score for temporal-spatial fields, without keeping spatial pattern\n",
    "    def RMSE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the RMSE. x is input series and y is reference series.\n",
    "        It calculates RMSE over the domain, not over time. The spatial structure\n",
    "        will not be kept.\n",
    "        Parameter\n",
    "        ----------------------\n",
    "        x: input time series with the shape [time, lat, lon]\n",
    "        \"\"\"\n",
    "        x_series = x.reshape(x.shape[0],-1)\n",
    "        y_series = y.reshape(y.shape[0],-1)\n",
    "        rmse = np.sqrt(np.mean((x_series - y_series)**2,1))\n",
    "        rmse_std = np.sqrt(np.std((x_series - y_series)**2,1))\n",
    "    \n",
    "        return rmse, rmse_std\n",
    "    \n",
    "    # error score for temporal-spatial fields, keeping spatial pattern\n",
    "    def MAE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the MAE. x is input series and y is reference series.\n",
    "        It calculate MAE over time and keeps the spatial structure.\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(x-y),0)\n",
    "        \n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "###################  start prediction loop ###################\n",
      "##############################################################\n",
      "*******************************  one step ahead forecast  *********************************\n",
      "************  the last 4 years of total time series are treated as test data  ************\n",
      "Wall time: 34min 18s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    #################################################################################\n",
    "    ########  operational lead time dependent prediction with testing data   ########\n",
    "    #################################################################################\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(test_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 16 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    choice_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1, training=False)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week2month(series, m):\n",
    "    \"\"\"\n",
    "    Select certain month from yearly data at weekly resolution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        Three-dimensional numeric arrays with time as the first dimenison [time, lat, lon]\n",
    "    m: int\n",
    "        Month (from 1 to 12).\n",
    "    \"\"\"\n",
    "    time_year, lat, lon = series.shape\n",
    "    time_month = time_year // 12\n",
    "    series_month = np.zeros((time_month, lat, lon), dtype=float)\n",
    "    series_month[::4,:,:] = series[(m-1)*4::48,:,:]\n",
    "    series_month[1::4,:,:] = series[(m-1)*4+1::48,:,:]\n",
    "    series_month[2::4,:,:] = series[(m-1)*4+2::48,:,:]\n",
    "    series_month[3::4,:,:] = series[(m-1)*4+3::48,:,:]\n",
    "    \n",
    "    return series_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   land-sea correction for sic prediction    #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "50.706676528696136 +- 101.52656804735905\n",
      "Mean RMSE with testing data - Climatology\n",
      "137.91125915081744 +- 212.30182952186146\n",
      "Mean RMSE with testing data - Persistence\n",
      "50.170794466755474 +- 104.00027668956385\n",
      "*******************     Lead time 1     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "73.80067980850241 +- 143.49283429508566\n",
      "Mean RMSE with testing data - Persistence\n",
      "75.9458837476023 +- 153.19988772374379\n",
      "*******************     Lead time 2     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "87.97012328792476 +- 167.7852734894779\n",
      "Mean RMSE with testing data - Persistence\n",
      "93.91398900099325 +- 186.0275468269465\n",
      "*******************     Lead time 3     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "98.5188860402766 +- 185.6837423753026\n",
      "Mean RMSE with testing data - Persistence\n",
      "110.1283583502817 +- 213.094235734779\n",
      "*******************     Lead time 4     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "107.31380808774915 +- 198.38616487734134\n",
      "Mean RMSE with testing data - Persistence\n",
      "124.67600441157826 +- 234.89027045606795\n",
      "*******************     Lead time 5     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "114.11048065520377 +- 207.20630612685207\n",
      "Mean RMSE with testing data - Persistence\n",
      "138.19688180429785 +- 253.81677913415027\n",
      "*******************     Lead time 6     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "120.01947589345633 +- 214.58713240940858\n",
      "Mean RMSE with testing data - Persistence\n",
      "149.77746834844268 +- 270.6962810970926\n",
      "*******************     Lead time 7     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "124.84219094751715 +- 218.869082473999\n",
      "Mean RMSE with testing data - Persistence\n",
      "159.60545845774672 +- 284.95967429502997\n",
      "*******************     Lead time 8     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "128.7138520548639 +- 221.84704752394927\n",
      "Mean RMSE with testing data - Persistence\n",
      "169.0739008031674 +- 298.8606259213634\n",
      "*******************     Lead time 9     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "131.58849734108065 +- 222.60721890135395\n",
      "Mean RMSE with testing data - Persistence\n",
      "178.18772775645525 +- 312.3710063626479\n",
      "*******************     Lead time 10     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "134.20592838035523 +- 222.89458852948098\n",
      "Mean RMSE with testing data - Persistence\n",
      "186.4387677456621 +- 324.1197528679833\n",
      "*******************     Lead time 11     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "136.4485921157947 +- 222.887362112187\n",
      "Mean RMSE with testing data - Persistence\n",
      "193.67677658258384 +- 334.85034457777834\n",
      "*******************     Lead time 12     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "138.1939411275557 +- 221.5000900636044\n",
      "Mean RMSE with testing data - Persistence\n",
      "200.32998646154147 +- 343.7878046378051\n",
      "*******************     Lead time 13     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "139.60593380026867 +- 219.41595016377573\n",
      "Mean RMSE with testing data - Persistence\n",
      "206.51543766766818 +- 351.4327196584504\n",
      "*******************     Lead time 14     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "140.67190120656565 +- 217.0785990279794\n",
      "Mean RMSE with testing data - Persistence\n",
      "212.35298561146928 +- 358.3893891527145\n",
      "*******************     Lead time 15     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "142.5877510277676 +- 215.99804451496826\n",
      "Mean RMSE with testing data - Persistence\n",
      "218.3251307542433 +- 365.56173747786585\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "RMSE - ConvLSTM       90.03199503565503 + - 176.79836619229437\n",
      "RMSE - Climatology    183.7509720006571 + - 280.49158911076336\n",
      "RMSE - Persistence    91.5997479178463 + - 180.9916370668999\n",
      "*******************    2     *******************\n",
      "RMSE - ConvLSTM       85.42853607325918 + - 170.12721098131493\n",
      "RMSE - Climatology    183.30330023934812 + - 273.347176573773\n",
      "RMSE - Persistence    82.42039524259681 + - 160.9626777381962\n",
      "*******************    3     *******************\n",
      "RMSE - ConvLSTM       80.57466662639172 + - 148.43041863129613\n",
      "RMSE - Climatology    199.89157834353045 + - 304.6694702911925\n",
      "RMSE - Persistence    83.91081918286545 + - 157.80740953153088\n",
      "*******************    4     *******************\n",
      "RMSE - ConvLSTM       73.26888508374091 + - 139.66337783029985\n",
      "RMSE - Climatology    207.80614115696767 + - 313.8306190145979\n",
      "RMSE - Persistence    76.4423650624924 + - 151.35030443127886\n",
      "*******************    5     *******************\n",
      "RMSE - ConvLSTM       60.01419233058145 + - 114.76646554868893\n",
      "RMSE - Climatology    182.62746359795213 + - 261.6904610096677\n",
      "RMSE - Persistence    69.48336388255558 + - 138.76485236989456\n",
      "*******************    6     *******************\n",
      "RMSE - ConvLSTM       36.12961583718595 + - 83.99245611134056\n",
      "RMSE - Climatology    149.31103629366999 + - 224.27726748630144\n",
      "RMSE - Persistence    46.7494984784352 + - 108.87355954907032\n",
      "*******************    7     *******************\n",
      "RMSE - ConvLSTM       18.469664978555283 + - 44.83353200868966\n",
      "RMSE - Climatology    73.29991659481595 + - 129.15866781119783\n",
      "RMSE - Persistence    23.531599159186072 + - 60.06396625220598\n",
      "*******************    8     *******************\n",
      "RMSE - ConvLSTM       17.788191297756605 + - 42.45268822018319\n",
      "RMSE - Climatology    46.699229356606295 + - 95.94623801740428\n",
      "RMSE - Persistence    10.98501001991176 + - 29.099367694308427\n",
      "*******************    9     *******************\n",
      "RMSE - ConvLSTM       17.088487755091954 + - 35.9739502981632\n",
      "RMSE - Climatology    42.771801967839494 + - 84.80483251772198\n",
      "RMSE - Persistence    5.635869541447041 + - 14.67266230343604\n",
      "*******************    10     *******************\n",
      "RMSE - ConvLSTM       28.363484686519065 + - 53.370271119785706\n",
      "RMSE - Climatology    72.14880256647942 + - 122.6752667121632\n",
      "RMSE - Persistence    14.224962737838037 + - 31.34599103819741\n",
      "*******************    11     *******************\n",
      "RMSE - ConvLSTM       36.86205923514393 + - 78.88086771393324\n",
      "RMSE - Climatology    147.2286115015819 + - 210.92864385138375\n",
      "RMSE - Persistence    30.528658826634373 + - 76.59773465361295\n",
      "*******************    12     *******************\n",
      "RMSE - ConvLSTM       64.46033940447249 + - 129.02921191231889\n",
      "RMSE - Climatology    166.09625619036072 + - 245.80172186617085\n",
      "RMSE - Persistence    66.53724354925671 + - 137.47315764613458\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with RMSE                 ########\n",
    "    ########              RMSE over time, and sum over domain                ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    RMSE_climatology, RMSE_climatology_std  = RMSE(climatology_seq * sic_max,sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_climatology = np.mean(RMSE_climatology)\n",
    "    RMSE_climatology_std = np.mean(RMSE_climatology_std)\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    RMSE_persist_0, RMSE_persist_0_std = RMSE(sic_exp_norm[-test_year*12*4-1:-1,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_persist_1, RMSE_persist_1_std = RMSE(sic_exp_norm[-test_year*12*4-1:-2,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_persist_2, RMSE_persist_2_std = RMSE(sic_exp_norm[-test_year*12*4-1:-3,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_persist_3, RMSE_persist_3_std = RMSE(sic_exp_norm[-test_year*12*4-1:-4,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_persist_4, RMSE_persist_4_std = RMSE(sic_exp_norm[-test_year*12*4-1:-5,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_persist_5, RMSE_persist_5_std = RMSE(sic_exp_norm[-test_year*12*4-1:-6,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_persist_6, RMSE_persist_6_std = RMSE(sic_exp_norm[-test_year*12*4-1:-7,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_persist_7, RMSE_persist_7_std = RMSE(sic_exp_norm[-test_year*12*4-1:-8,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_persist_8, RMSE_persist_8_std = RMSE(sic_exp_norm[-test_year*12*4-1:-9,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_persist_9, RMSE_persist_9_std = RMSE(sic_exp_norm[-test_year*12*4-1:-10,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_persist_10, RMSE_persist_10_std = RMSE(sic_exp_norm[-test_year*12*4-1:-11,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_persist_11, RMSE_persist_11_std = RMSE(sic_exp_norm[-test_year*12*4-1:-12,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_persist_12, RMSE_persist_12_std = RMSE(sic_exp_norm[-test_year*12*4-1:-13,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_persist_13, RMSE_persist_13_std = RMSE(sic_exp_norm[-test_year*12*4-1:-14,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_persist_14, RMSE_persist_14_std = RMSE(sic_exp_norm[-test_year*12*4-1:-15,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_persist_15, RMSE_persist_15_std = RMSE(sic_exp_norm[-test_year*12*4-1:-16,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print('##############################################################')\n",
    "    print('########   land-sea correction for sic prediction    #########')\n",
    "    print('##############################################################')\n",
    "    # correction for float point at 0\n",
    "    lead_pred_sic[lead_pred_sic<0] = 0\n",
    "    # extend the dimension of sea ice mask\n",
    "    sea_ice_mask_test = np.repeat(sea_ice_mask_barents[np.newaxis,:,:],test_year*48,0)\n",
    "    # correct the land cells in the prediction\n",
    "    for i in range(step_lead):\n",
    "        lead_pred_sic[:,i,:,:] = lead_pred_sic[:,i,:,:] * sea_ice_mask_test\n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    RMSE_ConvLSTM_0, RMSE_ConvLSTM_0_std = RMSE(lead_pred_sic[:,0,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_1, RMSE_ConvLSTM_1_std = RMSE(lead_pred_sic[:-1,1,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_2, RMSE_ConvLSTM_2_std = RMSE(lead_pred_sic[:-2,2,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_3, RMSE_ConvLSTM_3_std = RMSE(lead_pred_sic[:-3,3,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_4, RMSE_ConvLSTM_4_std = RMSE(lead_pred_sic[:-4,4,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_5, RMSE_ConvLSTM_5_std = RMSE(lead_pred_sic[:-5,5,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_6, RMSE_ConvLSTM_6_std = RMSE(lead_pred_sic[:-6,6,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_7, RMSE_ConvLSTM_7_std = RMSE(lead_pred_sic[:-7,7,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_8, RMSE_ConvLSTM_8_std = RMSE(lead_pred_sic[:-8,8,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_9, RMSE_ConvLSTM_9_std = RMSE(lead_pred_sic[:-9,9,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_10, RMSE_ConvLSTM_10_std = RMSE(lead_pred_sic[:-10,10,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_11, RMSE_ConvLSTM_11_std = RMSE(lead_pred_sic[:-11,11,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_12, RMSE_ConvLSTM_12_std = RMSE(lead_pred_sic[:-12,12,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_13, RMSE_ConvLSTM_13_std = RMSE(lead_pred_sic[:-13,13,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_14, RMSE_ConvLSTM_14_std = RMSE(lead_pred_sic[:-14,14,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_15, RMSE_ConvLSTM_15_std = RMSE(lead_pred_sic[:-15,15,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_0),\"+-\",np.mean(RMSE_ConvLSTM_0_std))\n",
    "    print(\"Mean RMSE with testing data - Climatology\")\n",
    "    print(RMSE_climatology,\"+-\",RMSE_climatology_std)\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_0),\"+-\",np.mean(RMSE_persist_0_std))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_1),\"+-\",np.mean(RMSE_ConvLSTM_1_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_1),\"+-\",np.mean(RMSE_persist_1_std))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_2),\"+-\",np.mean(RMSE_ConvLSTM_2_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_2),\"+-\",np.mean(RMSE_persist_2_std))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_3),\"+-\",np.mean(RMSE_ConvLSTM_3_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_3),\"+-\",np.mean(RMSE_persist_3_std))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_4),\"+-\",np.mean(RMSE_ConvLSTM_4_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_4),\"+-\",np.mean(RMSE_persist_4_std))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_5),\"+-\",np.mean(RMSE_ConvLSTM_5_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_5),\"+-\",np.mean(RMSE_persist_5_std))\n",
    "    print(\"*******************     Lead time 6     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_6),\"+-\",np.mean(RMSE_ConvLSTM_6_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_6),\"+-\",np.mean(RMSE_persist_6_std))\n",
    "    print(\"*******************     Lead time 7     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_7),\"+-\",np.mean(RMSE_ConvLSTM_7_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_7),\"+-\",np.mean(RMSE_persist_7_std))\n",
    "    print(\"*******************     Lead time 8     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_8),\"+-\",np.mean(RMSE_ConvLSTM_8_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_8),\"+-\",np.mean(RMSE_persist_8_std))\n",
    "    print(\"*******************     Lead time 9     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_9),\"+-\",np.mean(RMSE_ConvLSTM_9_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_9),\"+-\",np.mean(RMSE_persist_9_std))\n",
    "    print(\"*******************     Lead time 10     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_10),\"+-\",np.mean(RMSE_ConvLSTM_10_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_10),\"+-\",np.mean(RMSE_persist_10_std))\n",
    "    print(\"*******************     Lead time 11     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_11),\"+-\",np.mean(RMSE_ConvLSTM_11_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_11),\"+-\",np.mean(RMSE_persist_11_std))\n",
    "    print(\"*******************     Lead time 12     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_12),\"+-\",np.mean(RMSE_ConvLSTM_12_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_12),\"+-\",np.mean(RMSE_persist_12_std))\n",
    "    print(\"*******************     Lead time 13     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_13),\"+-\",np.mean(RMSE_ConvLSTM_13_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_13),\"+-\",np.mean(RMSE_persist_13_std))\n",
    "    print(\"*******************     Lead time 14     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_14),\"+-\",np.mean(RMSE_ConvLSTM_14_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_14),\"+-\",np.mean(RMSE_persist_14_std))\n",
    "    print(\"*******************     Lead time 15     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_15),\"+-\",np.mean(RMSE_ConvLSTM_15_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_15),\"+-\",np.mean(RMSE_persist_15_std))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_rmse_pred_16weeks.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_0),np.mean(RMSE_ConvLSTM_0_std)))\n",
    "    f.write(\"RMSE - Climatology    {} + - {}\\n\".format(RMSE_climatology, RMSE_climatology_std))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_0),np.mean(RMSE_persist_0_std)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_1),np.mean(RMSE_ConvLSTM_1_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_1),np.mean(RMSE_persist_1_std)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_2),np.mean(RMSE_ConvLSTM_2_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_2),np.mean(RMSE_persist_2_std)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_3),np.mean(RMSE_ConvLSTM_3_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_3),np.mean(RMSE_persist_3_std)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_4),np.mean(RMSE_ConvLSTM_4_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_4),np.mean(RMSE_persist_4_std)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_5),np.mean(RMSE_ConvLSTM_5_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_5),np.mean(RMSE_persist_5_std)))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_6),np.mean(RMSE_ConvLSTM_6_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_6),np.mean(RMSE_persist_6_std)))\n",
    "    f.write(\"*******************     Lead time 7     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_7),np.mean(RMSE_ConvLSTM_7_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_7),np.mean(RMSE_persist_7_std)))\n",
    "    f.write(\"*******************     Lead time 8     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_8),np.mean(RMSE_ConvLSTM_8_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_8),np.mean(RMSE_persist_8_std)))\n",
    "    f.write(\"*******************     Lead time 9     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_9),np.mean(RMSE_ConvLSTM_9_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_9),np.mean(RMSE_persist_9_std)))\n",
    "    f.write(\"*******************     Lead time 10     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_10),np.mean(RMSE_ConvLSTM_10_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_10),np.mean(RMSE_persist_10_std)))\n",
    "    f.write(\"*******************     Lead time 11     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_11),np.mean(RMSE_ConvLSTM_11_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_11),np.mean(RMSE_persist_11_std)))\n",
    "    f.write(\"*******************     Lead time 12     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_12),np.mean(RMSE_ConvLSTM_12_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_12),np.mean(RMSE_persist_12_std)))\n",
    "    f.write(\"*******************     Lead time 13     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_13),np.mean(RMSE_ConvLSTM_13_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_13),np.mean(RMSE_persist_13_std)))\n",
    "    f.write(\"*******************     Lead time 14     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_14),np.mean(RMSE_ConvLSTM_14_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_14),np.mean(RMSE_persist_14_std)))\n",
    "    f.write(\"*******************     Lead time 15     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_15),np.mean(RMSE_ConvLSTM_15_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_15),np.mean(RMSE_persist_15_std)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = week2month(lead_pred_sic[:,0,:,:], i)\n",
    "        persist_monthly_series = week2month(sic_exp_norm[-test_year*12*4-1:-1,:,:], i)\n",
    "        climatology_monthly_series = week2month(climatology_seq, i)\n",
    "        truth_monthly_series = week2month(sic_exp_norm[-test_year*12*4:,:,:], i)\n",
    "        \n",
    "        rmse_ConvLSTM_monthly, rmse_ConvLSTM_monthly_std = RMSE(ConvLSTM_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_persist_monthly, rmse_persist_monthly_std = RMSE(persist_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_climatology_monthly, rmse_climatology_monthly_std = RMSE(climatology_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        \n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"RMSE - ConvLSTM       {} + - {}\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        print(\"RMSE - Climatology    {} + - {}\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        print(\"RMSE - Persistence    {} + - {}\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        f.write(\"RMSE - Climatology    {} + - {}\\n\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        \n",
    "    #f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating accuracy/recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating accuracy/recall/precision  *********************')\n",
    "    def accuracy(pred, label):\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        boolean = (pred==label)\n",
    "        accu_seq = np.mean(np.mean(boolean.astype(float),2),1)\n",
    "        accu_spa = np.mean(boolean.astype(float),0)\n",
    "        \n",
    "        return accu_seq, accu_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########          transfer the sea ice fields into binary data           ########\n",
    "    #################################################################################\n",
    "    # ice concentration below the threshold is regarded as no ice, the value is from\n",
    "    # https://nsidc.org/cryosphere/seaice/data/terminology.html\n",
    "    criterion_0 = 0.15 \n",
    "    # remove the area weight\n",
    "    sic_exp_denorm = np.zeros(sic_exp_norm.shape, dtype=float)\n",
    "    lead_pred_sic_denorm = np.zeros(lead_pred_sic.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        lead_pred_sic_denorm[:,:,i,:] = lead_pred_sic[:,:,i,:] /dx[i+12] * dx[35]\n",
    "        sic_exp_denorm[:,i,:] = sic_exp_norm[:,i,:] / dx[i+12] * dx[35]\n",
    "    # turn sea ice fields into binary data\n",
    "    lead_pred_sic_bin = lead_pred_sic_denorm[:]\n",
    "    sic_exp_bin = sic_exp_denorm[:]\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin <= criterion_0] = 0\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin > criterion_0] = 1\n",
    "    sic_exp_bin[sic_exp_bin <= criterion_0] = 0\n",
    "    sic_exp_bin[sic_exp_bin > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    lead_pred_sic_bin = lead_pred_sic_bin.astype(int)\n",
    "    sic_exp_bin = sic_exp_bin.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9742024739583334\n",
      "Total accuracy with testing data - Climatology\n",
      "0.8557477678571428\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9749193948412698\n",
      "*******************     Lead time 1     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9553493517825979\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9597396222887059\n",
      "*******************     Lead time 2     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9427279135338344\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9483787593984963\n",
      "*******************     Lead time 3     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9320475875535399\n",
      "Total accuracy with testing data - Persistence\n",
      "0.936889802217183\n",
      "*******************     Lead time 4     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9215583839918947\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9253340298885512\n",
      "*******************     Lead time 5     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9106744970715559\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9133960402342755\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "accuracy - ConvLSTM       0.9496372767857143\n",
      "accuracy - Climatology    0.8189174107142857\n",
      "accuracy - Persistence    0.9498697916666667\n",
      "*******************    2     *******************\n",
      "accuracy - ConvLSTM       0.9633556547619048\n",
      "accuracy - Climatology    0.8280784970238095\n",
      "accuracy - Persistence    0.9636811755952381\n",
      "*******************    3     *******************\n",
      "accuracy - ConvLSTM       0.9649367559523809\n",
      "accuracy - Climatology    0.8337983630952381\n",
      "accuracy - Persistence    0.9605189732142858\n",
      "*******************    4     *******************\n",
      "accuracy - ConvLSTM       0.9706566220238095\n",
      "accuracy - Climatology    0.8166852678571428\n",
      "accuracy - Persistence    0.968563988095238\n",
      "*******************    5     *******************\n",
      "accuracy - ConvLSTM       0.9681454613095238\n",
      "accuracy - Climatology    0.7997116815476191\n",
      "accuracy - Persistence    0.9637276785714285\n",
      "*******************    6     *******************\n",
      "accuracy - ConvLSTM       0.9808872767857144\n",
      "accuracy - Climatology    0.8109654017857143\n",
      "accuracy - Persistence    0.9734933035714286\n",
      "*******************    7     *******************\n",
      "accuracy - ConvLSTM       0.9902808779761905\n",
      "accuracy - Climatology    0.905087425595238\n",
      "accuracy - Persistence    0.9868396577380952\n",
      "*******************    8     *******************\n",
      "accuracy - ConvLSTM       0.9959077380952381\n",
      "accuracy - Climatology    0.9574032738095237\n",
      "accuracy - Persistence    0.9961402529761905\n",
      "*******************    9     *******************\n",
      "accuracy - ConvLSTM       0.9962332589285714\n",
      "accuracy - Climatology    0.9603329613095238\n",
      "accuracy - Persistence    0.998046875\n",
      "*******************    10     *******************\n",
      "accuracy - ConvLSTM       0.9816313244047619\n",
      "accuracy - Climatology    0.9025762648809523\n",
      "accuracy - Persistence    0.9930245535714286\n",
      "*******************    11     *******************\n",
      "accuracy - ConvLSTM       0.9683314732142857\n",
      "accuracy - Climatology    0.8140811011904763\n",
      "accuracy - Persistence    0.9844680059523809\n",
      "*******************    12     *******************\n",
      "accuracy - ConvLSTM       0.9604259672619049\n",
      "accuracy - Climatology    0.8213355654761905\n",
      "accuracy - Persistence    0.9606584821428572\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with accuracy               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    accu_climatology, _ = accuracy(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    accu_persist_0, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_persist_1, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_persist_2, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_persist_3, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_persist_4, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_persist_5, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    accu_ConvLSTM_0, _ = accuracy(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_ConvLSTM_1, _ = accuracy(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_ConvLSTM_2, _ = accuracy(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_ConvLSTM_3, _ = accuracy(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_ConvLSTM_4, _ = accuracy(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_ConvLSTM_5, _ = accuracy(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_0))\n",
    "    print(\"Total accuracy with testing data - Climatology\")\n",
    "    print(np.mean(accu_climatology))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_1))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_2))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_3))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_4))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_5))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_accuracy_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_0)))\n",
    "    f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accu_climatology)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_1)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_2)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_3)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_4)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_5)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"accuracy - ConvLSTM       {}\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Climatology    {}\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Persistence    {}\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating recall/precision  *********************')\n",
    "    # positive is sea ice = 1\n",
    "    \n",
    "    def recall(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total actual positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False negative (is 1 but predict 0)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 0] = 2\n",
    "        label_dummy_2[label == 1] = 2\n",
    "        # count False Positive events\n",
    "        falseNegative = (pred_dummy_2 == label_dummy_2)\n",
    "\n",
    "#         recall_seq = np.mean(np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                     (truePositive.astype(float) + falseNegative.astype(float))),2),1)\n",
    "        \n",
    "        recall_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                       np.sum(np.sum(falseNegative.astype(float),2),1))\n",
    "        \n",
    "#         recall_spa = np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                           (truePositive.astype(float) + falseNegative.astype(float))),0)\n",
    "        \n",
    "        recall_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                             np.sum(falseNegative.astype(float),0))\n",
    "        \n",
    "        #return recall_seq, recall_spa\n",
    "        return np.nan_to_num(recall_seq), np.nan_to_num(recall_spa)\n",
    "    \n",
    "    def precision(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total predicted positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False positive (is 0 but predict 1)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 1] = 2\n",
    "        label_dummy_2[label == 0] = 2\n",
    "        # count False Positive events\n",
    "        falsePositive = (pred_dummy_2 == label_dummy_2)\n",
    "        \n",
    "        prec_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                     np.sum(np.sum(falsePositive.astype(float),2),1))\n",
    "        \n",
    "        prec_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                           np.sum(falsePositive.astype(float),0))\n",
    "        \n",
    "        return np.nan_to_num(prec_seq), np.nan_to_num(prec_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7294511802947931\n",
      "Total recall with testing data - Climatology\n",
      "0.7682106239728338\n",
      "Total recall with testing data - Persistence\n",
      "0.6956084994582069\n",
      "*******************     Lead time 1     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7050962750794713\n",
      "Total recall with testing data - Persistence\n",
      "0.6436574679557777\n",
      "*******************     Lead time 2     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6909375651664893\n",
      "Total recall with testing data - Persistence\n",
      "0.610596193945815\n",
      "*******************     Lead time 3     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6749170520140996\n",
      "Total recall with testing data - Persistence\n",
      "0.5745854442827217\n",
      "*******************     Lead time 4     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6594568455857532\n",
      "Total recall with testing data - Persistence\n",
      "0.5492875256810292\n",
      "*******************     Lead time 5     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6446762471590884\n",
      "Total recall with testing data - Persistence\n",
      "0.5259148468998884\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "recall - ConvLSTM       0.860598886960916\n",
      "recall - Climatology    0.9985675754922358\n",
      "recall - Persistence    0.8314841347620789\n",
      "*******************    2     *******************\n",
      "recall - ConvLSTM       0.9453464077344558\n",
      "recall - Climatology    0.9988817097415507\n",
      "recall - Persistence    0.9345966147874042\n",
      "*******************    3     *******************\n",
      "recall - ConvLSTM       0.9346326494497246\n",
      "recall - Climatology    0.9997340425531915\n",
      "recall - Persistence    0.9106219389571815\n",
      "*******************    4     *******************\n",
      "recall - ConvLSTM       0.9685899947294929\n",
      "recall - Climatology    0.998480953251909\n",
      "recall - Persistence    0.967307576852602\n",
      "*******************    5     *******************\n",
      "recall - ConvLSTM       0.9754766488516187\n",
      "recall - Climatology    0.9983039583340307\n",
      "recall - Persistence    0.980571197002486\n",
      "*******************    6     *******************\n",
      "recall - ConvLSTM       0.9473765015441237\n",
      "recall - Climatology    0.99876677278424\n",
      "recall - Persistence    0.97613758406689\n",
      "*******************    7     *******************\n",
      "recall - ConvLSTM       0.6955869319793339\n",
      "recall - Climatology    0.6825157351595051\n",
      "recall - Persistence    0.731919166058951\n",
      "*******************    8     *******************\n",
      "recall - ConvLSTM       0.25801363975690794\n",
      "recall - Climatology    0.17666547941988756\n",
      "recall - Persistence    0.2848559532321151\n",
      "*******************    9     *******************\n",
      "recall - ConvLSTM       0.24191735537190082\n",
      "recall - Climatology    0.08532851239669421\n",
      "recall - Persistence    0.24395041322314048\n",
      "*******************    10     *******************\n",
      "recall - ConvLSTM       0.3462687717754512\n",
      "recall - Climatology    0.31790314876598696\n",
      "recall - Persistence    0.2090523667871815\n",
      "*******************    11     *******************\n",
      "recall - ConvLSTM       0.7261897291259987\n",
      "recall - Climatology    0.96659537082369\n",
      "recall - Persistence    0.5508195922988457\n",
      "*******************    12     *******************\n",
      "recall - ConvLSTM       0.8534166462575934\n",
      "recall - Climatology    0.9967842289510855\n",
      "recall - Persistence    0.7259854554696066\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with recall               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    recall_climatology, _ = recall(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    recall_persist_0, _ = recall(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_persist_1, _ = recall(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_persist_2, _ = recall(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_persist_3, _ = recall(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_persist_4, _ = recall(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_persist_5, _ = recall(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    recall_ConvLSTM_0, _ = recall(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_ConvLSTM_1, _ = recall(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_ConvLSTM_2, _ = recall(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_ConvLSTM_3, _ = recall(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_ConvLSTM_4, _ = recall(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_ConvLSTM_5, _ = recall(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_0))\n",
    "    print(\"Total recall with testing data - Climatology\")\n",
    "    print(np.mean(recall_climatology))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_1))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_2))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_3))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_4))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_5))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_recall_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total recall with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_0)))\n",
    "    f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall_climatology)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_1)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_2)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_3)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_4)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_5)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"recall - ConvLSTM       {}\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Climatology    {}\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Persistence    {}\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6626360787901849\n",
      "Total precision with testing data - Climatology\n",
      "0.4079385856750705\n",
      "Total precision with testing data - Persistence\n",
      "0.690738761572815\n",
      "*******************     Lead time 1     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6107196257319342\n",
      "Total precision with testing data - Persistence\n",
      "0.6356268444470853\n",
      "*******************     Lead time 2     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.579523479629473\n",
      "Total precision with testing data - Persistence\n",
      "0.6105407500493609\n",
      "*******************     Lead time 3     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5556280316807053\n",
      "Total precision with testing data - Persistence\n",
      "0.5833019876273877\n",
      "*******************     Lead time 4     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5321481099096353\n",
      "Total precision with testing data - Persistence\n",
      "0.553914255338053\n",
      "*******************     Lead time 5     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5133675059459688\n",
      "Total precision with testing data - Persistence\n",
      "0.5249623371537447\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "precision - ConvLSTM       0.9026181601166431\n",
      "precision - Climatology    0.5559542418518143\n",
      "precision - Persistence    0.9267649426825348\n",
      "*******************    2     *******************\n",
      "precision - ConvLSTM       0.9191504507078476\n",
      "precision - Climatology    0.6083904361457534\n",
      "precision - Persistence    0.9307656748230683\n",
      "*******************    3     *******************\n",
      "precision - ConvLSTM       0.9389789991144883\n",
      "precision - Climatology    0.625218252792524\n",
      "precision - Persistence    0.9461523115764381\n",
      "*******************    4     *******************\n",
      "precision - ConvLSTM       0.931422338150755\n",
      "precision - Climatology    0.6066702234474803\n",
      "precision - Persistence    0.9246785425436064\n",
      "*******************    5     *******************\n",
      "precision - ConvLSTM       0.8758113133167318\n",
      "precision - Climatology    0.4990962228132416\n",
      "precision - Persistence    0.8529426820551145\n",
      "*******************    6     *******************\n",
      "precision - ConvLSTM       0.8264382811201926\n",
      "precision - Climatology    0.33860928286279546\n",
      "precision - Persistence    0.7542750415249005\n",
      "*******************    7     *******************\n",
      "precision - ConvLSTM       0.5637202053127027\n",
      "precision - Climatology    0.25354943889363424\n",
      "precision - Persistence    0.53082042568696\n",
      "*******************    8     *******************\n",
      "precision - ConvLSTM       0.25508710795141093\n",
      "precision - Climatology    0.25\n",
      "precision - Persistence    0.2554656667257474\n",
      "*******************    9     *******************\n",
      "precision - ConvLSTM       0.2326843886249756\n",
      "precision - Climatology    0.23414179104477612\n",
      "precision - Persistence    0.23465289256198346\n",
      "*******************    10     *******************\n",
      "precision - ConvLSTM       0.24895253003603335\n",
      "precision - Climatology    0.2184269005947958\n",
      "precision - Persistence    0.24059244560980797\n",
      "*******************    11     *******************\n",
      "precision - ConvLSTM       0.44714826265214447\n",
      "precision - Climatology    0.24949192107359747\n",
      "precision - Persistence    0.7659697420634921\n",
      "*******************    12     *******************\n",
      "precision - ConvLSTM       0.8096209083782928\n",
      "precision - Climatology    0.45571431658043265\n",
      "precision - Persistence    0.9257847710201259\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with precision              ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    prec_climatology, _ = precision(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    prec_persist_0, _ = precision(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_persist_1, _ = precision(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_persist_2, _ = precision(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_persist_3, _ = precision(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_persist_4, _ = precision(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_persist_5, _ = precision(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    prec_ConvLSTM_0, _ = precision(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_ConvLSTM_1, _ = precision(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_ConvLSTM_2, _ = precision(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_ConvLSTM_3, _ = precision(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_ConvLSTM_4, _ = precision(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_ConvLSTM_5, _ = precision(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_0))\n",
    "    print(\"Total precision with testing data - Climatology\")\n",
    "    print(np.mean(prec_climatology))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_1))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_2))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_3))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_4))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_5))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_precision_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total precision with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_0)))\n",
    "    f.write(\"precision - Climatology    {}\\n\".format(np.mean(prec_climatology)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_1)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_2)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_3)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_4)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_5)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"precision - ConvLSTM       {}\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Climatology    {}\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Persistence    {}\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Climatology    {}\\n\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Persistence    {}\\n\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
