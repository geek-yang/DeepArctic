{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Predict the Spatial Sea Ice Concentration with BayesConvLSTM at weekly time scale** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2020.03.02 ** <br>\n",
    "** Last Update  : 2020.03.09 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, dlacs, matplotlib **<br>\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. The Bayesian Convolutional Long Short Time Memory neural network is used to deal with this spatial-temporal sequence problem. We use Pytorch as the deep learning framework. <br>\n",
    "<br>\n",
    "** Here we predict sea ice concentration with one extra relevant field from either ocean or atmosphere to test the predictor.** <br>\n",
    "\n",
    "Return Values   : pkl model and figures <br>\n",
    "\n",
    "The regionalization adopted here follows that of the MASIE (Multisensor Analyzed Sea Ice Extent) product available from the National Snow and Ice Data Center:<br>\n",
    "https://nsidc.org/data/masie/browse_regions<br>\n",
    "It is given by paper J.Walsh et. al., 2019. Benchmark seasonal prediction skill estimates based on regional indices.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import numbers\n",
    "\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "#sys.path.append(os.path.join('C:','Users','nosta','ML4Climate','Scripts','DLACs'))\n",
    "#sys.path.append(\"C:\\\\Users\\\\nosta\\\\ML4Climate\\\\Scripts\\\\DLACs\")\n",
    "sys.path.append(\"../../../DLACs\")\n",
    "import dlacs\n",
    "import dlacs.BayesConvLSTM\n",
    "import dlacs.preprocess\n",
    "import dlacs.function\n",
    "\n",
    "# for visualization\n",
    "import dlacs.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import iris # also helps with regriding\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# ignore all the DeprecationWarnings by pytorch\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing device is Dell Inspirion 5680 with Intel Core i7-8700 x64 CPU and Nvidia GTX 1060 6GB GPU.<br>\n",
    "Here is a benchmark about cpu v.s. gtx 1060 <br>\n",
    "https://www.analyticsindiamag.com/deep-learning-tensorflow-benchmark-intel-i5-4210u-vs-geforce-nvidia-1060-6gb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2500000,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data ** <br>\n",
    "Time span of each product included: <br>\n",
    "** Reanalysis ** <br>\n",
    "- **ERA-Interim** 1979 - 2016 (ECMWF)\n",
    "- **ORAS4**       1958 - 2014 (ECMWF)\n",
    "\n",
    "** Index ** <br>\n",
    "- **NINO3.4**     1950 - 2017 (NOAA)\n",
    "- **AO**          1950 - 2017 (NOAA)\n",
    "- **NAO**         1950 - 2017 (NOAA)\n",
    "- **AMO**         1950 - 2017 (NOAA)\n",
    "- **PDO**         1950 - 2017 (University of Washington)\n",
    "\n",
    "!! These index are given by NCEP/NCAR Reanalysis (CDAS) <br>\n",
    "\n",
    "\n",
    "Alternative (not in use yet) <br>\n",
    "** Reanalysis ** <br>\n",
    "- **MERRA2**      1980 - 2016 (NASA)\n",
    "- **JRA55**       1979 - 2015 (JMA)\n",
    "- **GLORYS2V3**   1993 - 2014 (Mercartor Ocean)\n",
    "- **SODA3**       1980 - 2015\n",
    "- **PIOMASS**     1980 - 2015\n",
    "\n",
    "** Observations ** <br>\n",
    "- **NSIDC**       1958 - 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# \n",
    "#########                           datapath                             ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "#datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ERA-Interim'\n",
    "datapath_ERAI = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ERA-Interim'\n",
    "#datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "datapath_ORAS4 = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "#datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "datapath_ORAS4_mask = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "#datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "#datapath_PIOMASS = 'H:\\\\Creator_Zone\\\\Core_Database_AMET_OMET_reanalysis\\\\PIOMASS'\n",
    "#datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "#datapath_clim_index = 'F:\\\\PhD_essential\\\\Core_Database_AMET_OMET_reanalysis\\\\Climate_index'\n",
    "#output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/BayesMaps'\n",
    "output_path = 'C:\\\\Users\\\\nosta\\\\ML4Climate\\\\PredictArctic\\\\BayesMaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** get the key to the datasets *************************\n",
      "*********************** extract variables *************************\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    # weekly variables on ERAI grid\n",
    "    dataset_ERAI_fields_sic = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'sic_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_slp = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       'slp_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_t2m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       't2m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z500 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z500_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z850 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z850_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_uv10m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'uv10m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_rad = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                         'rad_flux_weekly_erai_1979_2017.nc'))\n",
    "    #dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "    #                             'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    # OHC interpolated on ERA-Interim grid\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                'ohc_monthly_oras2erai_1978_2017.nc'))\n",
    "#     dataset_index = Dataset(os.path.join(datapath_clim_index,\n",
    "#                             'index_climate_monthly_regress_1950_2017.nc'))\n",
    "    #dataset_ERAI_fields_flux = Dataset(os.path.join(datapath_ERAI_fields,\n",
    "    #                                  'surface_erai_monthly_regress_1979_2017_radiation.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    #################################################################################\n",
    "    #########                        data gallery                           #########\n",
    "    #################################################################################\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2013\n",
    "    # validation: 2014 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim) - benckmark\n",
    "    SIC_ERAI = dataset_ERAI_fields_sic.variables['sic'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI = dataset_ERAI_fields_sic.variables['year'][:-1]\n",
    "    week_ERAI = dataset_ERAI_fields_sic.variables['week'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_sic.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_sic.variables['longitude'][:]\n",
    "    # T2M (ERA-Interim)\n",
    "#     T2M_ERAI = dataset_ERAI_fields_t2m.variables['t2m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_t2m = dataset_ERAI_fields_t2m.variables['year'][:-1]\n",
    "#     week_ERAI_t2m = dataset_ERAI_fields_t2m.variables['week'][:]\n",
    "#     latitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['latitude'][:]\n",
    "#     longitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['longitude'][:]\n",
    "    # SLP (ERA-Interim)\n",
    "#     SLP_ERAI = dataset_ERAI_fields_slp.variables['slp'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_slp = dataset_ERAI_fields_slp.variables['year'][:-1]\n",
    "#     week_ERAI_slp = dataset_ERAI_fields_slp.variables['week'][:]\n",
    "#     latitude_ERAI_slp = dataset_ERAI_fields_slp.variables['latitude'][:]\n",
    "#     longitude_ERAI_slp = dataset_ERAI_fields_slp.variables['longitude'][:]\n",
    "    # Z500 (ERA-Interim)\n",
    "#     Z500_ERAI = dataset_ERAI_fields_z500.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z500 = dataset_ERAI_fields_z500.variables['year'][:-1]\n",
    "#     week_ERAI_z500 = dataset_ERAI_fields_z500.variables['week'][:]\n",
    "#     latitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['latitude'][:]\n",
    "#     longitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['longitude'][:]\n",
    "    # Z850 (ERA-Interim)\n",
    "#     Z850_ERAI = dataset_ERAI_fields_z850.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z850 = dataset_ERAI_fields_z850.variables['year'][:-1]\n",
    "#     week_ERAI_z850 = dataset_ERAI_fields_z850.variables['week'][:]\n",
    "#     latitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['latitude'][:]\n",
    "#     longitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['longitude'][:]\n",
    "    # UV10M (ERA-Interim)\n",
    "#     U10M_ERAI = dataset_ERAI_fields_uv10m.variables['u10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     V10M_ERAI = dataset_ERAI_fields_uv10m.variables['v10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['year'][:-1]\n",
    "#     week_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['week'][:]\n",
    "#     latitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['latitude'][:]\n",
    "#     longitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['longitude'][:]\n",
    "    # SFlux (ERA-Interim)\n",
    "#     SFlux_ERAI = dataset_ERAI_fields_rad.variables['SFlux'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_SFlux = dataset_ERAI_fields_rad.variables['year'][:-1]\n",
    "#     week_ERAI_SFlux = dataset_ERAI_fields_rad.variables['week'][:]\n",
    "#     latitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['latitude'][:]\n",
    "#     longitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    #SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    #year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # OHC (ORAS4)\n",
    "    # from 1978 - 2017 (for interpolation) / from 90 N upto 40 N\n",
    "    OHC_300_ORAS4 = dataset_ORAS4_OHC.variables['OHC'][:-1,:,:67,:]/1000 # unit Peta Joule\n",
    "    latitude_ORAS4 = dataset_ORAS4_OHC.variables['latitude'][:]\n",
    "    longitude_ORAS4 = dataset_ORAS4_OHC.variables['longitude'][:]\n",
    "    mask_OHC = np.ma.getmask(OHC_300_ORAS4[0,0,:,:])\n",
    "    # AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "#     AO = dataset_index.variables['AO'][348:-1] # from 1979 - 2017\n",
    "#     NAO = dataset_index.variables['NAO'][348:-1]\n",
    "#     NINO = dataset_index.variables['NINO'][348:-1]\n",
    "#     AMO = dataset_index.variables['AMO'][348:-1]\n",
    "#     PDO = dataset_index.variables['PDO'][348:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # first check of grid\n",
    "    print(latitude_ERAI)\n",
    "    print(longitude_ERAI)\n",
    "    print(longitude_ORAS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** create mask *************************\n",
      "*********************** calc mask *************************\n",
      "*********************** packing *************************\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ###########                 global land-sea mask                      ###########\n",
    "    #################################################################################\n",
    "    sea_ice_mask_global = np.ones((len(latitude_ERAI),len(longitude_ERAI)),dtype=float)\n",
    "    sea_ice_mask_global[SIC_ERAI[0,0,:,:]==-1] = 0\n",
    "    #################################################################################\n",
    "    ###########                regionalization sea mask                   ###########\n",
    "    #################################################################################\n",
    "    print ('*********************** create mask *************************')\n",
    "    # W:-156 E:-124 N:80 S:67\n",
    "    mask_Beaufort = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:-156 N:80 S:66\n",
    "    mask_Chukchi = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:146 E:180 N:80 S:67\n",
    "    mask_EastSiberian = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:100 E:146 N:80 S:67\n",
    "    mask_Laptev = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:60 E:100 N:80 S:67\n",
    "    mask_Kara = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:18 E:60 N:80 S:64\n",
    "    mask_Barents = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-44 E:18 N:80 S:55\n",
    "    mask_Greenland = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:180 N:90 S:80\n",
    "    mask_CenArctic = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    print ('*********************** calc mask *************************')\n",
    "    mask_Beaufort[13:31,32:76] = 1\n",
    "\n",
    "    mask_Chukchi[13:32,0:32] = 1\n",
    "    mask_Chukchi[13:32,-1] = 1\n",
    "\n",
    "    mask_EastSiberian[13:31,434:479] = 1\n",
    "\n",
    "    mask_Laptev[13:31,374:434] = 1\n",
    "\n",
    "    mask_Kara[13:31,320:374] = 1\n",
    "\n",
    "    mask_Barents[13:36,264:320] = 1\n",
    "\n",
    "    mask_Greenland[13:47,179:264] = 1\n",
    "    mask_Greenland[26:47,240:264] = 0\n",
    "\n",
    "    mask_CenArctic[:13,:] = 1\n",
    "    print ('*********************** packing *************************')\n",
    "    mask_dict = {'Beaufort': mask_Beaufort[:,:],\n",
    "                 'Chukchi': mask_Chukchi[:,:],\n",
    "                 'EastSiberian': mask_EastSiberian[:,:],\n",
    "                 'Laptev': mask_Laptev[:,:],\n",
    "                 'Kara': mask_Kara[:,:],\n",
    "                 'Barents': mask_Barents[:,:],\n",
    "                 'Greenland': mask_Greenland[:,:],\n",
    "                 'CenArctic': mask_CenArctic[:,:]}\n",
    "    seas_namelist = ['Beaufort','Chukchi','EastSiberian','Laptev',\n",
    "                     'Kara', 'Barents', 'Greenland','CenArctic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########                  temporal interpolation matrix                  ########\n",
    "    #################################################################################\n",
    "    # interpolate from monthly to weekly\n",
    "    # original monthly data will be taken as the last week of the month\n",
    "    OHC_300_ORAS4_weekly_series = np.zeros(SIC_ERAI.reshape(len(year_ERAI)*48,len(latitude_ERAI),len(longitude_ERAI)).shape,\n",
    "                                           dtype=float)\n",
    "    OHC_300_ORAS4_series= dlacs.preprocess.operator.unfold(OHC_300_ORAS4)\n",
    "    # calculate the difference between two months\n",
    "    OHC_300_ORAS4_deviation_series = (OHC_300_ORAS4_series[1:,:,:] - OHC_300_ORAS4_series[:-1,:,:]) / 4\n",
    "    for i in np.arange(4):\n",
    "        OHC_300_ORAS4_weekly_series[3-i::4,:,:] = OHC_300_ORAS4_series[12:,:,:] - i * OHC_300_ORAS4_deviation_series[11:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  calculate extent from spatial fields  *******************\n",
      "================  reshape input data into time series  =================\n"
     ]
    }
   ],
   "source": [
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / 480\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "#     SFlux_ERAI_area = np.zeros(SFlux_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,:,i,:] = SIC_ERAI[:,:,i,:]* dx[i] * dy / 1E+6 # unit km2\n",
    "#         SFlux_ERAI_area[:,:,i,:] = SFlux_ERAI[:,:,i,:]* dx[i] * dy / 1E+12 # unit TeraWatt\n",
    "    SIC_ERAI_area[SIC_ERAI_area<0] = 0 # switch the mask from -1 to 0\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    SIC_ERAI_area_series = dlacs.preprocess.operator.unfold(SIC_ERAI_area)\n",
    "#     T2M_ERAI_series = dlacs.preprocess.operator.unfold(T2M_ERAI)\n",
    "#     SLP_ERAI_series = dlacs.preprocess.operator.unfold(SLP_ERAI)\n",
    "#     Z500_ERAI_series = dlacs.preprocess.operator.unfold(Z500_ERAI)\n",
    "#     Z850_ERAI_series = dlacs.preprocess.operator.unfold(Z850_ERAI)\n",
    "#     U10M_ERAI_series = dlacs.preprocess.operator.unfold(U10M_ERAI)\n",
    "#     V10M_ERAI_series = dlacs.preprocess.operator.unfold(V10M_ERAI)\n",
    "#     SFlux_ERAI_area_series = dlacs.preprocess.operator.unfold(SFlux_ERAI_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  choose the fields from target region  *******************\n",
      "******************  choose the fields from target region  *******************\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n"
     ]
    }
   ],
   "source": [
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select land-sea mask\n",
    "    sea_ice_mask_barents = sea_ice_mask_global[12:36,264:320]\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select the area between greenland and ice land for instance 60-70 N / 44-18 W\n",
    "    sic_exp = SIC_ERAI_area_series[:,12:36,264:320]\n",
    "#     t2m_exp = T2M_ERAI_series[:,12:36,264:320]\n",
    "#     slp_exp = SLP_ERAI_series[:,12:36,264:320]\n",
    "#     z500_exp = Z500_ERAI_series[:,12:36,264:320]\n",
    "#     z850_exp = Z850_ERAI_series[:,12:36,264:320]\n",
    "#     u10m_exp = U10M_ERAI_series[:,12:36,264:320]\n",
    "#     v10m_exp = V10M_ERAI_series[:,12:36,264:320]\n",
    "#     sflux_exp = SFlux_ERAI_area_series[:,12:36,264:320]\n",
    "    ohc_exp = OHC_300_ORAS4_weekly_series[:,12:36,264:320]\n",
    "    print(sic_exp.shape)\n",
    "#     print(t2m_exp.shape)\n",
    "#     print(slp_exp.shape)\n",
    "#     print(z500_exp.shape)\n",
    "#     print(u10m_exp.shape)\n",
    "#     print(v10m_exp.shape)\n",
    "#     print(sflux_exp.shape)\n",
    "    print(ohc_exp.shape)\n",
    "    print(latitude_ERAI[12:36])\n",
    "    print(longitude_ERAI[264:320])\n",
    "    print(latitude_ORAS4[12:36])\n",
    "    print(longitude_ORAS4[264:320])\n",
    "    #print(latitude_ERAI[26:40])\n",
    "    #print(longitude_ERAI[180:216])\n",
    "    #print(sic_exp[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  pre-processing  *********************\n",
      "=========================   normalize data   ===========================\n",
      "================  save the normalizing factor  =================\n",
      "1565.2049481856002 km2\n",
      "0.0 km2\n",
      "====================    A series of time (index)    ====================\n",
      "===================  artificial data for evaluation ====================\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('=========================   normalize data   ===========================')\n",
    "    sic_exp_norm = dlacs.preprocess.operator.normalize(sic_exp)\n",
    "#     t2m_exp_norm = deepclim.preprocess.operator.normalize(t2m_exp)\n",
    "#     slp_exp_norm = deepclim.preprocess.operator.normalize(slp_exp)\n",
    "#     z500_exp_norm = deepclim.preprocess.operator.normalize(z500_exp)\n",
    "#     z850_exp_norm = deepclim.preprocess.operator.normalize(z850_exp)\n",
    "#     u10m_exp_norm = deepclim.preprocess.operator.normalize(u10m_exp)\n",
    "#     v10m_exp_norm = deepclim.preprocess.operator.normalize(v10m_exp)\n",
    "#     sflux_exp_norm = deepclim.preprocess.operator.normalize(sflux_exp)\n",
    "    ohc_exp_norm = dlacs.preprocess.operator.normalize(ohc_exp)\n",
    "    print('================  save the normalizing factor  =================')\n",
    "    sic_max = np.amax(sic_exp)\n",
    "    sic_min = np.amin(sic_exp)\n",
    "    print(sic_max,\"km2\")\n",
    "    print(sic_min,\"km2\")\n",
    "    print ('====================    A series of time (index)    ====================')\n",
    "    _, yy, xx = sic_exp_norm.shape # get the lat lon dimension\n",
    "    year = np.arange(1979,2017,1)\n",
    "    year_cycle = np.repeat(year,48)\n",
    "    month_cycle = np.repeat(np.arange(1,13,1),4)\n",
    "    month_cycle = np.tile(month_cycle,len(year)+1) # one extra repeat for lead time dependent prediction\n",
    "    month_cycle.astype(float)\n",
    "    month_2D = np.repeat(month_cycle[:,np.newaxis],yy,1)\n",
    "    month_exp = np.repeat(month_2D[:,:,np.newaxis],xx,2)\n",
    "    print ('===================  artificial data for evaluation ====================')\n",
    "    # calculate climatology of SIC\n",
    "#     seansonal_cycle_SIC = np.zeros(48,dtype=float)\n",
    "#     for i in np.arange(48):\n",
    "#         seansonal_cycle_SIC[i] = np.mean(SIC_ERAI_sum_norm[i::48],axis=0)\n",
    "    # weight for loss\n",
    "#     weight_month = np.array([0,1,1,\n",
    "#                              1,0,0,\n",
    "#                              1,1,1,\n",
    "#                              0,0,0])\n",
    "    #weight_loss = np.repeat(weight_month,4)\n",
    "    #weight_loss = np.tile(weight_loss,len(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565.2049481856002\n",
      "0.0\n",
      "(1824, 24, 56)\n",
      "[[251.4346045  251.05600413 250.50871172 ... 244.2139345  244.13173904\n",
      "  244.01692955]\n",
      " [246.64799721 246.04600942 245.45885542 ... 245.36829478 245.24715214\n",
      "  245.12353721]\n",
      " [246.7402512  247.31849816 248.16998491 ... 246.47754407 246.37387686\n",
      "  246.26773736]\n",
      " ...\n",
      " [271.39089775 271.56084288 271.75937185 ... 272.40565781 271.30805881\n",
      "  270.25455477]\n",
      " [271.93968032 272.07067828 272.09157428 ... 273.35488503 272.4173081\n",
      "  271.39831464]\n",
      " [272.82825155 272.92514534 272.69329114 ... 274.15672261 273.34753587\n",
      "  272.70551716]]\n",
      "(24, 56)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print(t2m_exp.shape)\n",
    "    ax = plt.contourf(t2m_exp[443,:,:])\n",
    "    print(t2m_exp[443,:,:])\n",
    "    print(month_exp[0,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26267704 0.26046318 0.25661401 ... 0.22950646 0.22708498 0.22442972]\n",
      " [0.20302922 0.20179229 0.20073181 ... 0.23818486 0.23636252 0.23451899]\n",
      " [0.19524795 0.20810658 0.22415041 ... 0.25322432 0.25219251 0.25112267]\n",
      " ...\n",
      " [0.48474634 0.48954066 0.49081374 ... 0.36213768 0.336037   0.3028926 ]\n",
      " [0.49387301 0.5059012  0.52108218 ... 0.37193518 0.34423099 0.31239209]\n",
      " [0.52729484 0.54777388 0.5776589  ... 0.38700331 0.35481465 0.32946398]]\n",
      "(4, 24, 56)\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  pretest input shape  *********************')\n",
    "    x_input = np.stack((sic_exp_norm[1,:,:],\n",
    "                        t2m_exp_norm[1,:,:],\n",
    "                        ohc_exp_norm[1,:,:],\n",
    "                        month_exp[1,:,:]))\n",
    "    print(x_input[1,:,:])\n",
    "    print(x_input[:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  parameter for check  *********************\n",
      "*******************  create basic dimensions for tensor and network  *********************\n",
      "*******************  cross validation and testing data  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  parameter for check  *********************')\n",
    "    choice_exp_norm = ohc_exp_norm\n",
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    # specifications of neural network\n",
    "    input_channels = 3\n",
    "    hidden_channels = [3, 2, 1] # number of channels & hidden layers, the channels of last layer is the channels of output, too\n",
    "    #hidden_channels = [3, 3, 3, 3, 2]\n",
    "    #hidden_channels = [2]\n",
    "    kernel_size = 3\n",
    "    # here we input a sequence and predict the next step only\n",
    "    #step = 1 # how many steps to predict ahead\n",
    "    #effective_step = [0] # step to output\n",
    "    batch_size = 1\n",
    "    #num_layers = 1\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 500\n",
    "    print ('*******************  cross validation and testing data  *********************')\n",
    "    # take 10% data as cross-validation data\n",
    "    cross_valid_year = 4\n",
    "    # take 10% years as testing data\n",
    "    test_year = 4\n",
    "    # minibatch\n",
    "    #iterations = 3 # training data divided into 3 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  check the environment  *********************\n",
      "Pytorch version 1.1.0\n",
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  check the environment  *********************')\n",
    "    print (\"Pytorch version {}\".format(torch.__version__))\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    # CUDA settings torch.__version__ must > 0.4\n",
    "    # !!! This is important for the model!!! The first option is gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  preview of input tensor  *********************\n",
      "1824\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  preview of input tensor  *********************')\n",
    "    #plt.plot(SIC_ERAI_sum)\n",
    "    #print(SIC_ERAI_sum_norm[:-test_year*12])\n",
    "    #print(x_input.shape)\n",
    "    #print(x_input[:,:,:])\n",
    "    sequence_len, _, _ = sic_exp_norm.shape\n",
    "    print(sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print ('*******************  module for calculating accuracy  *********************')\n",
    "#     def accuracy(out, labels):\n",
    "#         outputs = np.argmax(out, axis=1)\n",
    "#     return np.sum(outputs==labels)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%time\n",
    "    print ('*******************  load exsited LSTM model  *********************')\n",
    "    # load model parameters\n",
    "    model = dlacs.BayesConvLSTM.BayesConvLSTM(input_channels, hidden_channels, kernel_size).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(output_path, 'Barents','bayesconvlstm_era_sic_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl')))\n",
    "    # load entire model\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_z850_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_t2m_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_sflux_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_slp_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_z850_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    print(model)\n",
    "    # check the sequence length (dimension in need for post-processing)\n",
    "    sequence_len, height, width = sic_exp_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  run BayesConvLSTM  *********************\n",
      "The model is designed to make many to one prediction.\n",
      "A series of multi-chanel variables will be input to the model.\n",
      "The model learns by verifying the output at each timestep.\n",
      "BayesConvLSTM(\n",
      "  (cell0): BayesConvLSTMCell()\n",
      "  (cell1): BayesConvLSTMCell()\n",
      "  (cell2): BayesConvLSTMCell()\n",
      ")\n",
      "ELBO(\n",
      "  (loss_function): MSELoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  run BayesConvLSTM  *********************')\n",
    "    print ('The model is designed to make many to one prediction.')\n",
    "    print ('A series of multi-chanel variables will be input to the model.')\n",
    "    print ('The model learns by verifying the output at each timestep.')\n",
    "    # check the sequence length\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    # initialize our model\n",
    "    model = dlacs.BayesConvLSTM.BayesConvLSTM(input_channels, hidden_channels, kernel_size).to(device)\n",
    "    # use Evidence Lower Bound (ELBO) to quantify the loss\n",
    "    ELBO = dlacs.function.ELBO(height*width)\n",
    "    # for classification, target must be integers (label)\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=torch.nn.KLDivLoss())\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=nn.CrossEntropyLoss(reduction='mean'))\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=nn.NLLLoss(reduction='mean'))\n",
    "    # penalty for kl\n",
    "    penalty_kl = sequence_len\n",
    "    # stochastic gradient descent\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # Adam optimizer\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(model)\n",
    "    print(ELBO)\n",
    "    print(optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell0.Wxi_mu\n",
      "tensor([[[[ 0.1472,  0.1631,  0.1635],\n",
      "          [ 0.1536,  0.1602, -0.0367],\n",
      "          [ 0.0556,  0.1679, -0.1687]],\n",
      "\n",
      "         [[ 0.0585, -0.0754, -0.0378],\n",
      "          [ 0.0239, -0.0508,  0.0620],\n",
      "          [ 0.0325,  0.1264,  0.1753]],\n",
      "\n",
      "         [[ 0.1912,  0.0960,  0.1077],\n",
      "          [-0.1472, -0.1029,  0.0228],\n",
      "          [-0.1103, -0.1797, -0.1453]]],\n",
      "\n",
      "\n",
      "        [[[-0.0178, -0.0797, -0.0151],\n",
      "          [ 0.0263,  0.0191, -0.0528],\n",
      "          [ 0.0383, -0.1689,  0.1392]],\n",
      "\n",
      "         [[ 0.1851, -0.0268,  0.0123],\n",
      "          [ 0.1285,  0.0129, -0.0792],\n",
      "          [ 0.1199, -0.1594, -0.0588]],\n",
      "\n",
      "         [[-0.0114, -0.0004,  0.0318],\n",
      "          [ 0.1608, -0.1713,  0.0632],\n",
      "          [-0.0520,  0.0220, -0.0518]]],\n",
      "\n",
      "\n",
      "        [[[-0.1778, -0.0458,  0.0262],\n",
      "          [ 0.1148, -0.0951,  0.0099],\n",
      "          [ 0.0147,  0.1785, -0.1870]],\n",
      "\n",
      "         [[-0.1067,  0.0164,  0.1007],\n",
      "          [-0.1683,  0.1554,  0.1911],\n",
      "          [ 0.1381,  0.0382, -0.0504]],\n",
      "\n",
      "         [[ 0.0216, -0.0168,  0.0812],\n",
      "          [-0.1240, -0.0972,  0.0059],\n",
      "          [ 0.1150,  0.1191, -0.1172]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Whi_mu\n",
      "tensor([[[[-0.1424, -0.1024,  0.1191],\n",
      "          [ 0.0620,  0.1463, -0.0310],\n",
      "          [ 0.1907,  0.1054,  0.0085]],\n",
      "\n",
      "         [[-0.0533, -0.1788,  0.0090],\n",
      "          [ 0.1870, -0.1761,  0.1581],\n",
      "          [-0.0746,  0.1283,  0.0324]],\n",
      "\n",
      "         [[ 0.0324,  0.0790,  0.0949],\n",
      "          [-0.1188,  0.1207,  0.1116],\n",
      "          [-0.1719,  0.0109, -0.0014]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0457,  0.1476, -0.1793],\n",
      "          [-0.1500, -0.1236,  0.1724],\n",
      "          [-0.1563,  0.1520, -0.0964]],\n",
      "\n",
      "         [[ 0.1489, -0.0939, -0.0872],\n",
      "          [-0.1464,  0.1483, -0.1607],\n",
      "          [-0.0390,  0.1398, -0.0073]],\n",
      "\n",
      "         [[ 0.1460, -0.0294,  0.0522],\n",
      "          [ 0.1126,  0.0169,  0.0143],\n",
      "          [-0.0215, -0.1537, -0.0406]]],\n",
      "\n",
      "\n",
      "        [[[-0.1907, -0.1181, -0.0532],\n",
      "          [-0.1254,  0.1563,  0.0932],\n",
      "          [ 0.1244, -0.0934, -0.0303]],\n",
      "\n",
      "         [[-0.1048, -0.0643,  0.0156],\n",
      "          [-0.0874,  0.1156, -0.1526],\n",
      "          [ 0.1310,  0.0255,  0.1004]],\n",
      "\n",
      "         [[-0.1209,  0.1790, -0.0493],\n",
      "          [-0.0135, -0.0041, -0.0925],\n",
      "          [ 0.1062,  0.1671, -0.1845]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxf_mu\n",
      "tensor([[[[-8.0336e-02,  4.9797e-02,  5.1258e-03],\n",
      "          [-1.1117e-01, -1.1903e-02, -5.2108e-02],\n",
      "          [-6.6496e-02,  1.3342e-02, -9.5681e-02]],\n",
      "\n",
      "         [[ 1.9111e-01,  1.7320e-02,  4.8998e-03],\n",
      "          [-4.0389e-02,  1.0612e-01, -1.6306e-01],\n",
      "          [-3.4007e-02, -3.8595e-02, -1.0952e-01]],\n",
      "\n",
      "         [[ 1.9164e-01, -1.7762e-01, -7.3920e-02],\n",
      "          [ 1.2625e-01, -8.0251e-03, -1.7872e-02],\n",
      "          [ 5.5807e-02, -4.4968e-02, -3.9086e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.2192e-01,  1.3775e-01,  1.5428e-01],\n",
      "          [-1.6268e-01, -1.2402e-01, -7.1995e-02],\n",
      "          [-1.5688e-01, -9.2473e-02, -3.0567e-02]],\n",
      "\n",
      "         [[-1.6743e-01, -2.0532e-02, -5.6462e-02],\n",
      "          [-2.7816e-02,  1.1723e-01,  5.0999e-02],\n",
      "          [-1.7676e-02, -2.8581e-02, -8.1676e-02]],\n",
      "\n",
      "         [[-9.8658e-02,  1.1178e-01, -9.2411e-02],\n",
      "          [ 1.6523e-01, -1.2852e-01, -1.7878e-01],\n",
      "          [-5.7228e-02,  1.1756e-01,  1.7137e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.5770e-02, -1.0402e-01, -6.1386e-02],\n",
      "          [ 6.5027e-02,  1.4371e-01,  5.2673e-02],\n",
      "          [-1.7750e-01,  1.6021e-01, -8.3060e-02]],\n",
      "\n",
      "         [[-1.0912e-01, -1.4503e-01, -1.3769e-01],\n",
      "          [ 8.2816e-02, -1.5285e-01,  2.6977e-03],\n",
      "          [ 1.3020e-02, -1.8763e-01,  1.4372e-01]],\n",
      "\n",
      "         [[ 1.0072e-02, -7.5087e-02, -9.9492e-02],\n",
      "          [ 7.8327e-02, -3.4549e-02, -1.8216e-01],\n",
      "          [-1.8345e-02,  1.6861e-01,  1.1787e-01]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Whf_mu\n",
      "tensor([[[[ 0.0709,  0.0965,  0.0627],\n",
      "          [-0.1364,  0.0205, -0.0639],\n",
      "          [ 0.1839, -0.1779,  0.0949]],\n",
      "\n",
      "         [[-0.0511, -0.0534, -0.0091],\n",
      "          [ 0.0930, -0.1321, -0.1647],\n",
      "          [ 0.1138, -0.0054,  0.0168]],\n",
      "\n",
      "         [[ 0.1633,  0.1172, -0.0807],\n",
      "          [-0.0331, -0.1644,  0.1094],\n",
      "          [ 0.1883,  0.0380,  0.0413]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0562, -0.0910, -0.1329],\n",
      "          [-0.1188,  0.0336,  0.1324],\n",
      "          [ 0.1712,  0.0715, -0.1466]],\n",
      "\n",
      "         [[ 0.1100,  0.0902,  0.0709],\n",
      "          [ 0.1547,  0.1213,  0.0210],\n",
      "          [-0.1336,  0.0385, -0.0567]],\n",
      "\n",
      "         [[-0.0934,  0.0415, -0.0418],\n",
      "          [-0.0294,  0.1264, -0.0099],\n",
      "          [ 0.0450, -0.1479,  0.1083]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1162,  0.0207, -0.1524],\n",
      "          [ 0.0312, -0.0348,  0.0100],\n",
      "          [ 0.0389,  0.0470,  0.1740]],\n",
      "\n",
      "         [[ 0.1892,  0.1622, -0.1575],\n",
      "          [ 0.1377, -0.0599, -0.1125],\n",
      "          [ 0.0065, -0.1082,  0.0188]],\n",
      "\n",
      "         [[ 0.0104,  0.0223,  0.0542],\n",
      "          [ 0.0644, -0.0765, -0.0686],\n",
      "          [-0.0550,  0.1853,  0.0076]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxc_mu\n",
      "tensor([[[[ 0.0465,  0.0229, -0.0577],\n",
      "          [-0.0756,  0.0999,  0.0334],\n",
      "          [-0.1191,  0.1747, -0.1864]],\n",
      "\n",
      "         [[ 0.1494,  0.1863,  0.1741],\n",
      "          [ 0.0650, -0.1841, -0.1434],\n",
      "          [ 0.1854,  0.0820,  0.1630]],\n",
      "\n",
      "         [[ 0.0153,  0.0205,  0.1145],\n",
      "          [-0.1663,  0.0449,  0.1523],\n",
      "          [ 0.1629,  0.0323, -0.0427]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0642,  0.1196,  0.1523],\n",
      "          [ 0.1681, -0.1151, -0.0775],\n",
      "          [ 0.1392,  0.0665,  0.0472]],\n",
      "\n",
      "         [[-0.0398, -0.1511,  0.1603],\n",
      "          [-0.1393, -0.0766,  0.1612],\n",
      "          [-0.1798,  0.1483, -0.0344]],\n",
      "\n",
      "         [[-0.1214, -0.0821, -0.0965],\n",
      "          [-0.1672,  0.1301,  0.1798],\n",
      "          [ 0.0298, -0.0362,  0.0676]]],\n",
      "\n",
      "\n",
      "        [[[-0.0781,  0.1275,  0.1207],\n",
      "          [-0.0126,  0.1303,  0.0204],\n",
      "          [-0.0322,  0.0255,  0.1395]],\n",
      "\n",
      "         [[ 0.1686,  0.1116, -0.1407],\n",
      "          [ 0.0912, -0.1851, -0.0695],\n",
      "          [ 0.0343, -0.1440,  0.1797]],\n",
      "\n",
      "         [[-0.1047,  0.1806,  0.0995],\n",
      "          [ 0.1398,  0.1758, -0.1808],\n",
      "          [ 0.1852, -0.0649, -0.1516]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Whc_mu\n",
      "tensor([[[[-0.0237, -0.1518,  0.0624],\n",
      "          [ 0.0387, -0.0391, -0.1279],\n",
      "          [-0.1646, -0.1684,  0.0948]],\n",
      "\n",
      "         [[ 0.0627,  0.1040,  0.0499],\n",
      "          [ 0.0912,  0.1384,  0.1732],\n",
      "          [-0.0554, -0.0251, -0.0559]],\n",
      "\n",
      "         [[-0.0234,  0.1048, -0.0135],\n",
      "          [ 0.1406,  0.0141, -0.0527],\n",
      "          [-0.1635, -0.0093,  0.1898]]],\n",
      "\n",
      "\n",
      "        [[[-0.1122,  0.1783,  0.1322],\n",
      "          [-0.1903,  0.1231,  0.0292],\n",
      "          [ 0.1790,  0.1093, -0.1245]],\n",
      "\n",
      "         [[ 0.1801, -0.1429,  0.0408],\n",
      "          [ 0.1521, -0.1292,  0.0663],\n",
      "          [-0.0474,  0.0006,  0.0392]],\n",
      "\n",
      "         [[-0.0951, -0.1353, -0.1359],\n",
      "          [-0.0280,  0.0235, -0.0839],\n",
      "          [-0.0365,  0.0482, -0.0795]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0727,  0.1324,  0.1130],\n",
      "          [ 0.1167, -0.0085, -0.0630],\n",
      "          [ 0.0250,  0.0384, -0.1310]],\n",
      "\n",
      "         [[ 0.1373,  0.1565, -0.0088],\n",
      "          [-0.1259,  0.0799, -0.1781],\n",
      "          [ 0.1591, -0.0932,  0.1130]],\n",
      "\n",
      "         [[ 0.1772,  0.0926,  0.1383],\n",
      "          [ 0.0861,  0.0546,  0.0505],\n",
      "          [-0.0616,  0.0462,  0.0014]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxo_mu\n",
      "tensor([[[[-0.1197,  0.0770, -0.1664],\n",
      "          [-0.1534,  0.0769, -0.1570],\n",
      "          [-0.1522, -0.1521,  0.0352]],\n",
      "\n",
      "         [[-0.0985, -0.1198,  0.1205],\n",
      "          [-0.0536,  0.0882,  0.1246],\n",
      "          [-0.0184, -0.1122, -0.1016]],\n",
      "\n",
      "         [[ 0.1607,  0.1230, -0.1278],\n",
      "          [-0.0954, -0.1526,  0.1564],\n",
      "          [-0.0517,  0.1037, -0.1336]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1840, -0.0006,  0.0088],\n",
      "          [ 0.1216,  0.1335,  0.1846],\n",
      "          [ 0.0588,  0.0292, -0.1454]],\n",
      "\n",
      "         [[ 0.1097,  0.1255, -0.0560],\n",
      "          [ 0.0891,  0.1115, -0.0583],\n",
      "          [ 0.0933,  0.1707,  0.0469]],\n",
      "\n",
      "         [[-0.1866,  0.1104, -0.1167],\n",
      "          [ 0.1566, -0.1565,  0.0055],\n",
      "          [-0.1408,  0.1862,  0.0674]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1107, -0.1852, -0.0465],\n",
      "          [ 0.0273, -0.1646,  0.0003],\n",
      "          [-0.0678, -0.0232, -0.1261]],\n",
      "\n",
      "         [[ 0.0926,  0.1113, -0.0932],\n",
      "          [-0.1893,  0.0476, -0.0771],\n",
      "          [-0.0408, -0.1173,  0.0639]],\n",
      "\n",
      "         [[-0.1266,  0.1218, -0.1206],\n",
      "          [-0.1478, -0.0058,  0.0367],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [-0.0562,  0.0305, -0.0641]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Who_mu\n",
      "tensor([[[[ 0.0510,  0.0575,  0.0728],\n",
      "          [ 0.0852,  0.0240,  0.0848],\n",
      "          [ 0.1046,  0.1347, -0.1198]],\n",
      "\n",
      "         [[ 0.0928,  0.0083,  0.0818],\n",
      "          [ 0.0569,  0.0568,  0.0738],\n",
      "          [ 0.0042,  0.1160, -0.0606]],\n",
      "\n",
      "         [[-0.1859,  0.1042, -0.0772],\n",
      "          [-0.1199,  0.0956, -0.0005],\n",
      "          [-0.1288, -0.1620,  0.1435]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0535,  0.0260, -0.1556],\n",
      "          [ 0.0881,  0.0035, -0.0883],\n",
      "          [ 0.0055,  0.1715, -0.1703]],\n",
      "\n",
      "         [[-0.0051,  0.1553, -0.0396],\n",
      "          [-0.1425,  0.0403,  0.1077],\n",
      "          [ 0.1691, -0.1519,  0.0261]],\n",
      "\n",
      "         [[ 0.0180,  0.1376,  0.0685],\n",
      "          [-0.1662, -0.1610, -0.0775],\n",
      "          [-0.1363,  0.1349, -0.0027]]],\n",
      "\n",
      "\n",
      "        [[[-0.0221, -0.0065, -0.1651],\n",
      "          [ 0.0135,  0.0630,  0.0317],\n",
      "          [-0.0986, -0.0956, -0.0180]],\n",
      "\n",
      "         [[-0.1675, -0.1214,  0.1855],\n",
      "          [ 0.0623, -0.1118,  0.0784],\n",
      "          [-0.0254, -0.1150, -0.1668]],\n",
      "\n",
      "         [[ 0.0637,  0.0780,  0.0631],\n",
      "          [ 0.1767, -0.0680,  0.1822],\n",
      "          [ 0.1514,  0.1026,  0.1753]]]], device='cuda:0')\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxi_bias\n",
      "tensor([-0.0365,  0.0397,  0.1768], device='cuda:0')\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxf_bias\n",
      "tensor([-0.0223, -0.0081, -0.0426], device='cuda:0')\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxc_bias\n",
      "tensor([-0.0865,  0.1610, -0.0568], device='cuda:0')\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxo_bias\n",
      "tensor([-0.0510, -0.0262, -0.1539], device='cuda:0')\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxi_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Whi_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Wxf_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Whf_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Wxc_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Whc_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Wxo_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Who_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxi_mu\n",
      "tensor([[[[-0.0920,  0.1763, -0.1657],\n",
      "          [-0.0920,  0.1637, -0.1683],\n",
      "          [ 0.0395,  0.0831, -0.0554]],\n",
      "\n",
      "         [[-0.1454,  0.1393,  0.0151],\n",
      "          [-0.1880,  0.1647, -0.1229],\n",
      "          [ 0.0278,  0.1267, -0.1274]],\n",
      "\n",
      "         [[ 0.0038, -0.1583,  0.1811],\n",
      "          [ 0.0978,  0.0110,  0.0267],\n",
      "          [-0.1038, -0.0531, -0.1216]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1767,  0.1745,  0.1106],\n",
      "          [-0.0178, -0.1390, -0.1496],\n",
      "          [-0.0921, -0.1685, -0.0109]],\n",
      "\n",
      "         [[ 0.0731, -0.0722,  0.1311],\n",
      "          [-0.0969, -0.0807, -0.0753],\n",
      "          [-0.0462,  0.1791, -0.1817]],\n",
      "\n",
      "         [[-0.1086,  0.1348, -0.0851],\n",
      "          [-0.1764,  0.0686,  0.1893],\n",
      "          [ 0.0461, -0.0412, -0.0584]]]], device='cuda:0')\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Whi_mu\n",
      "tensor([[[[ 0.1442,  0.0137, -0.0975],\n",
      "          [-0.1294,  0.0583,  0.1731],\n",
      "          [ 0.1264,  0.1682,  0.1127]],\n",
      "\n",
      "         [[ 0.1195, -0.0993,  0.0550],\n",
      "          [-0.0879,  0.0234,  0.0995],\n",
      "          [ 0.1182,  0.1671,  0.1138]]],\n",
      "\n",
      "\n",
      "        [[[-0.1656,  0.0477, -0.0019],\n",
      "          [ 0.1252,  0.1417,  0.0980],\n",
      "          [ 0.1620,  0.1518, -0.1461]],\n",
      "\n",
      "         [[-0.1664,  0.1479, -0.0060],\n",
      "          [-0.1109, -0.1522, -0.0318],\n",
      "          [ 0.0687,  0.1152,  0.1507]]]], device='cuda:0')\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxf_mu\n",
      "tensor([[[[ 0.0618,  0.1779, -0.1204],\n",
      "          [-0.0387, -0.0659,  0.1868],\n",
      "          [ 0.1084, -0.1035,  0.1051]],\n",
      "\n",
      "         [[ 0.0616, -0.1243, -0.1495],\n",
      "          [ 0.1608, -0.1368,  0.1666],\n",
      "          [ 0.1901,  0.0297,  0.1538]],\n",
      "\n",
      "         [[-0.1155, -0.1602, -0.1402],\n",
      "          [-0.0509,  0.0750,  0.0220],\n",
      "          [-0.0536,  0.0911,  0.0210]]],\n",
      "\n",
      "\n",
      "        [[[-0.1252, -0.1427,  0.0649],\n",
      "          [ 0.0952, -0.1186, -0.0185],\n",
      "          [-0.1862, -0.1680,  0.1171]],\n",
      "\n",
      "         [[-0.1651, -0.1058, -0.1361],\n",
      "          [-0.1406, -0.1098, -0.1132],\n",
      "          [ 0.0387, -0.0748,  0.1859]],\n",
      "\n",
      "         [[-0.1823, -0.1526,  0.0661],\n",
      "          [ 0.1090,  0.0949, -0.1771],\n",
      "          [-0.1220,  0.1003,  0.0257]]]], device='cuda:0')\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Whf_mu\n",
      "tensor([[[[ 0.0368, -0.1790, -0.1673],\n",
      "          [-0.0468,  0.1717,  0.1589],\n",
      "          [-0.0213,  0.0017, -0.1683]],\n",
      "\n",
      "         [[-0.1388, -0.0284,  0.1091],\n",
      "          [ 0.1562,  0.0175, -0.1144],\n",
      "          [-0.0393,  0.1154,  0.0289]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1300, -0.1072, -0.0014],\n",
      "          [ 0.1588,  0.0150, -0.1911],\n",
      "          [ 0.0872, -0.1096,  0.0153]],\n",
      "\n",
      "         [[-0.0407, -0.0036,  0.1574],\n",
      "          [ 0.1839,  0.0247, -0.0602],\n",
      "          [-0.0828,  0.1573,  0.1744]]]], device='cuda:0')\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxc_mu\n",
      "tensor([[[[ 0.0209, -0.1117, -0.1669],\n",
      "          [ 0.0400, -0.0991, -0.0052],\n",
      "          [ 0.0309,  0.0683,  0.0443]],\n",
      "\n",
      "         [[-0.0717, -0.1433,  0.0607],\n",
      "          [-0.1864,  0.1403, -0.1515],\n",
      "          [ 0.1805, -0.1248, -0.0818]],\n",
      "\n",
      "         [[-0.1718,  0.0253,  0.1293],\n",
      "          [-0.1357, -0.1752,  0.1871],\n",
      "          [-0.0081, -0.0218, -0.1843]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0187, -0.1561, -0.1135],\n",
      "          [-0.1121, -0.1801,  0.1911],\n",
      "          [ 0.0567,  0.0589,  0.1253]],\n",
      "\n",
      "         [[-0.0895,  0.1313,  0.0281],\n",
      "          [ 0.1560,  0.0647,  0.0675],\n",
      "          [-0.0891, -0.1629, -0.1790]],\n",
      "\n",
      "         [[-0.0676,  0.1583, -0.0464],\n",
      "          [ 0.1766, -0.1522,  0.1815],\n",
      "          [-0.0545,  0.1248,  0.0314]]]], device='cuda:0')\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Whc_mu\n",
      "tensor([[[[ 0.0993,  0.1923, -0.1278],\n",
      "          [-0.0567, -0.1329, -0.0921],\n",
      "          [-0.0190, -0.1099, -0.1237]],\n",
      "\n",
      "         [[-0.1693, -0.1846,  0.1819],\n",
      "          [-0.0740,  0.1414, -0.1795],\n",
      "          [-0.0132,  0.1872,  0.1021]]],\n",
      "\n",
      "\n",
      "        [[[-0.0473,  0.1755,  0.1645],\n",
      "          [-0.1077, -0.0536, -0.1059],\n",
      "          [-0.0107, -0.0053,  0.0222]],\n",
      "\n",
      "         [[-0.1294, -0.1590, -0.0684],\n",
      "          [ 0.1108,  0.1741,  0.0740],\n",
      "          [ 0.1236, -0.0799,  0.1010]]]], device='cuda:0')\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxo_mu\n",
      "tensor([[[[-0.1378,  0.0659, -0.0395],\n",
      "          [ 0.1068,  0.1187,  0.1617],\n",
      "          [-0.1747,  0.1295,  0.0476]],\n",
      "\n",
      "         [[-0.1252,  0.1117, -0.1150],\n",
      "          [ 0.0908,  0.0158,  0.0352],\n",
      "          [ 0.0919,  0.0701, -0.0388]],\n",
      "\n",
      "         [[ 0.1369,  0.0412, -0.0232],\n",
      "          [-0.0993, -0.1051, -0.0892],\n",
      "          [-0.1677,  0.1395, -0.1364]]],\n",
      "\n",
      "\n",
      "        [[[-0.1142, -0.0279, -0.1155],\n",
      "          [ 0.0993, -0.1689, -0.1860],\n",
      "          [ 0.0219,  0.0323, -0.1327]],\n",
      "\n",
      "         [[ 0.0972,  0.1364,  0.1635],\n",
      "          [-0.0956, -0.0676, -0.0612],\n",
      "          [ 0.1410, -0.0101, -0.0244]],\n",
      "\n",
      "         [[-0.1765, -0.1879, -0.0969],\n",
      "          [ 0.0673, -0.0312,  0.1923],\n",
      "          [ 0.0037, -0.1843, -0.0214]]]], device='cuda:0')\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Who_mu\n",
      "tensor([[[[-0.1268, -0.1796, -0.0593],\n",
      "          [-0.0867, -0.1569, -0.1185],\n",
      "          [-0.1661,  0.0864,  0.1116]],\n",
      "\n",
      "         [[-0.0611,  0.0344, -0.1042],\n",
      "          [-0.0458,  0.1509,  0.1158],\n",
      "          [-0.1309, -0.0235,  0.0509]]],\n",
      "\n",
      "\n",
      "        [[[-0.0834, -0.0398,  0.0906],\n",
      "          [-0.1279,  0.1452,  0.1104],\n",
      "          [-0.0803,  0.0053,  0.0454]],\n",
      "\n",
      "         [[-0.0920,  0.1516, -0.0459],\n",
      "          [ 0.1215,  0.0864, -0.1679],\n",
      "          [ 0.1254, -0.1025,  0.1200]]]], device='cuda:0')\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxi_bias\n",
      "tensor([-0.1796, -0.0587], device='cuda:0')\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxf_bias\n",
      "tensor([0.1189, 0.1887], device='cuda:0')\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxc_bias\n",
      "tensor([ 0.0535, -0.1040], device='cuda:0')\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxo_bias\n",
      "tensor([-0.0745, -0.0011], device='cuda:0')\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxi_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Whi_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxf_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Whf_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxc_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Whc_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxo_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Who_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxi_mu\n",
      "tensor([[[[ 0.1666, -0.2267, -0.1742],\n",
      "          [-0.0936, -0.2332,  0.1911],\n",
      "          [ 0.1976,  0.2040,  0.1818]],\n",
      "\n",
      "         [[ 0.1678, -0.1137, -0.1911],\n",
      "          [-0.0319,  0.1260, -0.2338],\n",
      "          [-0.1110, -0.1584,  0.1538]]]], device='cuda:0')\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Whi_mu\n",
      "tensor([[[[ 0.0021, -0.0771, -0.0321],\n",
      "          [ 0.0438,  0.1888, -0.0173],\n",
      "          [ 0.1309,  0.0104,  0.0698]]]], device='cuda:0')\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxf_mu\n",
      "tensor([[[[-0.0584, -0.0682,  0.0887],\n",
      "          [-0.1636,  0.1998, -0.1869],\n",
      "          [-0.1293,  0.2061, -0.1627]],\n",
      "\n",
      "         [[-0.1599, -0.1421, -0.1878],\n",
      "          [ 0.2282, -0.0330, -0.0526],\n",
      "          [ 0.0238,  0.2000, -0.1080]]]], device='cuda:0')\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Whf_mu\n",
      "tensor([[[[-0.0712,  0.1134, -0.0607],\n",
      "          [-0.0254, -0.1473,  0.2287],\n",
      "          [-0.1236, -0.0768,  0.1302]]]], device='cuda:0')\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxc_mu\n",
      "tensor([[[[ 0.1615,  0.1081,  0.2285],\n",
      "          [ 0.0359,  0.2030, -0.1281],\n",
      "          [ 0.1802, -0.1782, -0.1276]],\n",
      "\n",
      "         [[ 0.1226, -0.0134, -0.0374],\n",
      "          [-0.1381,  0.1074, -0.0712],\n",
      "          [-0.1491,  0.0387,  0.1379]]]], device='cuda:0')\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Whc_mu\n",
      "tensor([[[[ 0.0486, -0.2262, -0.2313],\n",
      "          [-0.1311,  0.1929, -0.0611],\n",
      "          [-0.1425, -0.1314, -0.1316]]]], device='cuda:0')\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxo_mu\n",
      "tensor([[[[ 0.1476, -0.1143,  0.0852],\n",
      "          [-0.1853,  0.1512, -0.0539],\n",
      "          [-0.2083, -0.1527,  0.0115]],\n",
      "\n",
      "         [[ 0.1529, -0.1840,  0.0621],\n",
      "          [-0.1098,  0.0993,  0.2100],\n",
      "          [ 0.1225, -0.0722, -0.0124]]]], device='cuda:0')\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Who_mu\n",
      "tensor([[[[-0.1580,  0.1118,  0.1539],\n",
      "          [ 0.2193, -0.1849,  0.1518],\n",
      "          [ 0.0211, -0.0174,  0.0946]]]], device='cuda:0')\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxi_bias\n",
      "tensor([0.0796], device='cuda:0')\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxf_bias\n",
      "tensor([-0.0949], device='cuda:0')\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxc_bias\n",
      "tensor([-0.1514], device='cuda:0')\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxo_bias\n",
      "tensor([-0.0528], device='cuda:0')\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxi_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Whi_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxf_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Whf_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxc_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Whc_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxo_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Who_log_alpha\n",
      "tensor([[-3.]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print (name)\n",
    "            print (param.data)\n",
    "            print (param.size())\n",
    "            print (\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('##############################################################')\n",
    "    print('#############  preview model parameters matrix  ###############')\n",
    "    print('##############################################################')\n",
    "    print('Number of parameter matrices: ', len(list(model.parameters())))\n",
    "    for i in range(len(list(model.parameters()))):\n",
    "        print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "##################  start training loop  #####################\n",
      "##############################################################\n",
      "Epoch  0 MSE:  76327.2890625\n",
      "Epoch  5 MSE:  59007.1328125\n",
      "Epoch  10 MSE:  40139.25\n",
      "Epoch  15 MSE:  37822.6171875\n",
      "Epoch  20 MSE:  32092.640625\n",
      "Epoch  25 MSE:  27742.30859375\n",
      "Epoch  30 MSE:  26334.943359375\n",
      "Epoch  35 MSE:  23685.333984375\n",
      "Epoch  40 MSE:  21666.50390625\n",
      "Epoch  45 MSE:  20018.91015625\n",
      "Epoch  50 MSE:  18218.486328125\n",
      "Epoch  55 MSE:  16281.25390625\n",
      "Epoch  60 MSE:  14883.84375\n",
      "Epoch  65 MSE:  13892.1015625\n",
      "Epoch  70 MSE:  13162.3232421875\n",
      "Epoch  75 MSE:  12490.0185546875\n",
      "Epoch  80 MSE:  11915.943359375\n",
      "Epoch  85 MSE:  11469.91015625\n",
      "Epoch  90 MSE:  11038.4921875\n",
      "Epoch  95 MSE:  10605.3486328125\n",
      "Epoch  100 MSE:  10272.958984375\n",
      "Epoch  105 MSE:  9959.9111328125\n",
      "Epoch  110 MSE:  9655.9638671875\n",
      "Epoch  115 MSE:  9365.662109375\n",
      "Epoch  120 MSE:  9133.9267578125\n",
      "Epoch  125 MSE:  8886.38671875\n",
      "Epoch  130 MSE:  8658.5380859375\n",
      "Epoch  135 MSE:  8451.1240234375\n",
      "Epoch  140 MSE:  8197.3203125\n",
      "Epoch  145 MSE:  8015.4931640625\n",
      "Epoch  150 MSE:  7807.36376953125\n",
      "Epoch  155 MSE:  7601.00927734375\n",
      "Epoch  160 MSE:  7431.72900390625\n",
      "Epoch  165 MSE:  7274.115234375\n",
      "Epoch  170 MSE:  7164.5546875\n",
      "Epoch  175 MSE:  7014.5302734375\n",
      "Epoch  180 MSE:  6898.9990234375\n",
      "Epoch  185 MSE:  6790.33447265625\n",
      "Epoch  190 MSE:  6693.31640625\n",
      "Epoch  195 MSE:  6617.24853515625\n",
      "Epoch  200 MSE:  6538.7109375\n",
      "Epoch  205 MSE:  6444.02685546875\n",
      "Epoch  210 MSE:  6376.0712890625\n",
      "Epoch  215 MSE:  6284.78857421875\n",
      "Epoch  220 MSE:  6203.974609375\n",
      "Epoch  225 MSE:  6149.5693359375\n",
      "Epoch  230 MSE:  6104.9326171875\n",
      "Epoch  235 MSE:  6037.89306640625\n",
      "Epoch  240 MSE:  5978.0302734375\n",
      "Epoch  245 MSE:  5918.638671875\n",
      "Epoch  250 MSE:  5858.88232421875\n",
      "Epoch  255 MSE:  5829.99462890625\n",
      "Epoch  260 MSE:  5764.6025390625\n",
      "Epoch  265 MSE:  5719.14990234375\n",
      "Epoch  270 MSE:  5963.15087890625\n",
      "Epoch  275 MSE:  5605.7685546875\n",
      "Epoch  280 MSE:  5592.60595703125\n",
      "Epoch  285 MSE:  5537.29638671875\n",
      "Epoch  290 MSE:  5506.1279296875\n",
      "Epoch  295 MSE:  5474.75146484375\n",
      "Epoch  300 MSE:  5434.3837890625\n",
      "Epoch  305 MSE:  5390.029296875\n",
      "Epoch  310 MSE:  5362.89306640625\n",
      "Epoch  315 MSE:  5334.96826171875\n",
      "Epoch  320 MSE:  5337.12890625\n",
      "Epoch  325 MSE:  5756.30078125\n",
      "Epoch  330 MSE:  5267.3837890625\n",
      "Epoch  335 MSE:  5312.92333984375\n",
      "Epoch  340 MSE:  5309.59716796875\n",
      "Epoch  345 MSE:  5261.59619140625\n",
      "Epoch  350 MSE:  5203.11865234375\n",
      "Epoch  355 MSE:  5149.70556640625\n",
      "Epoch  360 MSE:  5099.1767578125\n",
      "Epoch  365 MSE:  5082.35400390625\n",
      "Epoch  370 MSE:  5054.67431640625\n",
      "Epoch  375 MSE:  5013.91015625\n",
      "Epoch  380 MSE:  5005.2626953125\n",
      "Epoch  385 MSE:  5205.17138671875\n",
      "Epoch  390 MSE:  5138.177734375\n",
      "Epoch  395 MSE:  4953.0283203125\n",
      "Epoch  400 MSE:  4978.5341796875\n",
      "Epoch  405 MSE:  5018.5244140625\n",
      "Epoch  410 MSE:  4938.85498046875\n",
      "Epoch  415 MSE:  4932.74609375\n",
      "Epoch  420 MSE:  4872.55810546875\n",
      "Epoch  425 MSE:  4856.51806640625\n",
      "Epoch  430 MSE:  4871.30859375\n",
      "Epoch  435 MSE:  4823.56396484375\n",
      "Epoch  440 MSE:  4802.4990234375\n",
      "Epoch  445 MSE:  4812.76123046875\n",
      "Epoch  450 MSE:  5089.87255859375\n",
      "Epoch  455 MSE:  4958.49072265625\n",
      "Epoch  460 MSE:  4868.4501953125\n",
      "Epoch  465 MSE:  4772.95068359375\n",
      "Epoch  470 MSE:  4721.9736328125\n",
      "Epoch  475 MSE:  4735.21484375\n",
      "Epoch  480 MSE:  4699.919921875\n",
      "Epoch  485 MSE:  4702.03662109375\n",
      "Epoch  490 MSE:  4701.0185546875\n",
      "Epoch  495 MSE:  4692.48583984375\n",
      "Wall time: 21h 37min 23s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print('##############################################################')\n",
    "    print('##################  start training loop  #####################')\n",
    "    print('##############################################################')\n",
    "    # track training loss\n",
    "    hist = np.zeros(num_epochs)\n",
    "    # loop of epoch\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # loop of timestep\n",
    "        for timestep in range(sequence_len - cross_valid_year*12*4 - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #################################################################################\n",
    "            ########          create input tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                choice_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).to(device)\n",
    "            #################################################################################\n",
    "            ########       create training tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            y_train_stack = sic_exp_norm[timestep+1,:,:] #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).to(device)\n",
    "            #################################################################################   \n",
    "            # Forward pass\n",
    "            y_pred, kl_loss, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_target = y_var\n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            if timestep == 0:\n",
    "                loss = ELBO(y_pred, y_target, kl_loss, 1 / (len(hidden_channels) * 8 * penalty_kl * kernel_size**2))\n",
    "            else:\n",
    "                loss += ELBO(y_pred, y_target, kl_loss, 1 / (len(hidden_channels) * 8 * penalty_kl * kernel_size**2))\n",
    "            #print (timestep)\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 5 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            #print(y_pred)\n",
    "            # gradient check\n",
    "            # Gradcheck requires double precision numbers to run\n",
    "            #res = torch.autograd.gradcheck(loss_fn, (y_pred.double(), y_train.double()), eps=1e-6, raise_exception=True)\n",
    "            #print(res)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    # save the model\n",
    "    # (recommended) save the model parameters only\n",
    "    torch.save(model.state_dict(), os.path.join(output_path,'bayesconvlstm.pkl'))\n",
    "    # save the entire model\n",
    "    # torch.save(model, os.path.join(output_path,'bayesconvlstm.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  Loss with time  **********************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print (\"*******************  Loss with time  **********************\")\n",
    "    fig00 = plt.figure()\n",
    "    try:\n",
    "        plt.plot(hist, label=\"Training loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig00.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred_error.png'),dpi=200)\n",
    "    except:\n",
    "        print('Model is reloaded instead of trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  evaluation matrix  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  evaluation matrix  *********************')\n",
    "    # The prediction will be evaluated through RMSE against climatology\n",
    "    \n",
    "    # error score for temporal-spatial fields, without keeping spatial pattern\n",
    "    def RMSE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the RMSE. x is input series and y is reference series.\n",
    "        It calculates RMSE over the domain, not over time. The spatial structure\n",
    "        will not be kept.\n",
    "        Parameter\n",
    "        ----------------------\n",
    "        x: input time series with the shape [time, lat, lon]\n",
    "        \"\"\"\n",
    "        x_series = x.reshape(x.shape[0],-1)\n",
    "        y_series = y.reshape(y.shape[0],-1)\n",
    "        rmse = np.sqrt(np.mean((x_series - y_series)**2,1))\n",
    "        rmse_std = np.sqrt(np.std((x_series - y_series)**2,1))\n",
    "    \n",
    "        return rmse, rmse_std\n",
    "    \n",
    "    # error score for temporal-spatial fields, keeping spatial pattern\n",
    "    def MAE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the MAE. x is input series and y is reference series.\n",
    "        It calculate MAE over time and keeps the spatial structure.\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(x-y),0)\n",
    "        \n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "###################  start prediction loop ###################\n",
      "##############################################################\n",
      "*******************************  one step ahead forecast  *********************************\n",
      "************  the last 4 years of total time series are treated as test data  ************\n",
      "Wall time: 34min 18s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    #################################################################################\n",
    "    ########  operational lead time dependent prediction with testing data   ########\n",
    "    #################################################################################\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(test_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 16 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    choice_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1, training=False)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1, training=False)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week2month(series, m):\n",
    "    \"\"\"\n",
    "    Select certain month from yearly data at weekly resolution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        Three-dimensional numeric arrays with time as the first dimenison [time, lat, lon]\n",
    "    m: int\n",
    "        Month (from 1 to 12).\n",
    "    \"\"\"\n",
    "    time_year, lat, lon = series.shape\n",
    "    time_month = time_year // 12\n",
    "    series_month = np.zeros((time_month, lat, lon), dtype=float)\n",
    "    series_month[::4,:,:] = series[(m-1)*4::48,:,:]\n",
    "    series_month[1::4,:,:] = series[(m-1)*4+1::48,:,:]\n",
    "    series_month[2::4,:,:] = series[(m-1)*4+2::48,:,:]\n",
    "    series_month[3::4,:,:] = series[(m-1)*4+3::48,:,:]\n",
    "    \n",
    "    return series_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   land-sea correction for sic prediction    #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "50.706676528696136 +- 101.52656804735905\n",
      "Mean RMSE with testing data - Climatology\n",
      "137.91125915081744 +- 212.30182952186146\n",
      "Mean RMSE with testing data - Persistence\n",
      "50.170794466755474 +- 104.00027668956385\n",
      "*******************     Lead time 1     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "73.80067980850241 +- 143.49283429508566\n",
      "Mean RMSE with testing data - Persistence\n",
      "75.9458837476023 +- 153.19988772374379\n",
      "*******************     Lead time 2     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "87.97012328792476 +- 167.7852734894779\n",
      "Mean RMSE with testing data - Persistence\n",
      "93.91398900099325 +- 186.0275468269465\n",
      "*******************     Lead time 3     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "98.5188860402766 +- 185.6837423753026\n",
      "Mean RMSE with testing data - Persistence\n",
      "110.1283583502817 +- 213.094235734779\n",
      "*******************     Lead time 4     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "107.31380808774915 +- 198.38616487734134\n",
      "Mean RMSE with testing data - Persistence\n",
      "124.67600441157826 +- 234.89027045606795\n",
      "*******************     Lead time 5     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "114.11048065520377 +- 207.20630612685207\n",
      "Mean RMSE with testing data - Persistence\n",
      "138.19688180429785 +- 253.81677913415027\n",
      "*******************     Lead time 6     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "120.01947589345633 +- 214.58713240940858\n",
      "Mean RMSE with testing data - Persistence\n",
      "149.77746834844268 +- 270.6962810970926\n",
      "*******************     Lead time 7     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "124.84219094751715 +- 218.869082473999\n",
      "Mean RMSE with testing data - Persistence\n",
      "159.60545845774672 +- 284.95967429502997\n",
      "*******************     Lead time 8     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "128.7138520548639 +- 221.84704752394927\n",
      "Mean RMSE with testing data - Persistence\n",
      "169.0739008031674 +- 298.8606259213634\n",
      "*******************     Lead time 9     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "131.58849734108065 +- 222.60721890135395\n",
      "Mean RMSE with testing data - Persistence\n",
      "178.18772775645525 +- 312.3710063626479\n",
      "*******************     Lead time 10     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "134.20592838035523 +- 222.89458852948098\n",
      "Mean RMSE with testing data - Persistence\n",
      "186.4387677456621 +- 324.1197528679833\n",
      "*******************     Lead time 11     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "136.4485921157947 +- 222.887362112187\n",
      "Mean RMSE with testing data - Persistence\n",
      "193.67677658258384 +- 334.85034457777834\n",
      "*******************     Lead time 12     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "138.1939411275557 +- 221.5000900636044\n",
      "Mean RMSE with testing data - Persistence\n",
      "200.32998646154147 +- 343.7878046378051\n",
      "*******************     Lead time 13     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "139.60593380026867 +- 219.41595016377573\n",
      "Mean RMSE with testing data - Persistence\n",
      "206.51543766766818 +- 351.4327196584504\n",
      "*******************     Lead time 14     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "140.67190120656565 +- 217.0785990279794\n",
      "Mean RMSE with testing data - Persistence\n",
      "212.35298561146928 +- 358.3893891527145\n",
      "*******************     Lead time 15     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "142.5877510277676 +- 215.99804451496826\n",
      "Mean RMSE with testing data - Persistence\n",
      "218.3251307542433 +- 365.56173747786585\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "RMSE - ConvLSTM       90.03199503565503 + - 176.79836619229437\n",
      "RMSE - Climatology    183.7509720006571 + - 280.49158911076336\n",
      "RMSE - Persistence    91.5997479178463 + - 180.9916370668999\n",
      "*******************    2     *******************\n",
      "RMSE - ConvLSTM       85.42853607325918 + - 170.12721098131493\n",
      "RMSE - Climatology    183.30330023934812 + - 273.347176573773\n",
      "RMSE - Persistence    82.42039524259681 + - 160.9626777381962\n",
      "*******************    3     *******************\n",
      "RMSE - ConvLSTM       80.57466662639172 + - 148.43041863129613\n",
      "RMSE - Climatology    199.89157834353045 + - 304.6694702911925\n",
      "RMSE - Persistence    83.91081918286545 + - 157.80740953153088\n",
      "*******************    4     *******************\n",
      "RMSE - ConvLSTM       73.26888508374091 + - 139.66337783029985\n",
      "RMSE - Climatology    207.80614115696767 + - 313.8306190145979\n",
      "RMSE - Persistence    76.4423650624924 + - 151.35030443127886\n",
      "*******************    5     *******************\n",
      "RMSE - ConvLSTM       60.01419233058145 + - 114.76646554868893\n",
      "RMSE - Climatology    182.62746359795213 + - 261.6904610096677\n",
      "RMSE - Persistence    69.48336388255558 + - 138.76485236989456\n",
      "*******************    6     *******************\n",
      "RMSE - ConvLSTM       36.12961583718595 + - 83.99245611134056\n",
      "RMSE - Climatology    149.31103629366999 + - 224.27726748630144\n",
      "RMSE - Persistence    46.7494984784352 + - 108.87355954907032\n",
      "*******************    7     *******************\n",
      "RMSE - ConvLSTM       18.469664978555283 + - 44.83353200868966\n",
      "RMSE - Climatology    73.29991659481595 + - 129.15866781119783\n",
      "RMSE - Persistence    23.531599159186072 + - 60.06396625220598\n",
      "*******************    8     *******************\n",
      "RMSE - ConvLSTM       17.788191297756605 + - 42.45268822018319\n",
      "RMSE - Climatology    46.699229356606295 + - 95.94623801740428\n",
      "RMSE - Persistence    10.98501001991176 + - 29.099367694308427\n",
      "*******************    9     *******************\n",
      "RMSE - ConvLSTM       17.088487755091954 + - 35.9739502981632\n",
      "RMSE - Climatology    42.771801967839494 + - 84.80483251772198\n",
      "RMSE - Persistence    5.635869541447041 + - 14.67266230343604\n",
      "*******************    10     *******************\n",
      "RMSE - ConvLSTM       28.363484686519065 + - 53.370271119785706\n",
      "RMSE - Climatology    72.14880256647942 + - 122.6752667121632\n",
      "RMSE - Persistence    14.224962737838037 + - 31.34599103819741\n",
      "*******************    11     *******************\n",
      "RMSE - ConvLSTM       36.86205923514393 + - 78.88086771393324\n",
      "RMSE - Climatology    147.2286115015819 + - 210.92864385138375\n",
      "RMSE - Persistence    30.528658826634373 + - 76.59773465361295\n",
      "*******************    12     *******************\n",
      "RMSE - ConvLSTM       64.46033940447249 + - 129.02921191231889\n",
      "RMSE - Climatology    166.09625619036072 + - 245.80172186617085\n",
      "RMSE - Persistence    66.53724354925671 + - 137.47315764613458\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with RMSE                 ########\n",
    "    ########              RMSE over time, and sum over domain                ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    RMSE_climatology, RMSE_climatology_std  = RMSE(climatology_seq * sic_max,sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_climatology = np.mean(RMSE_climatology)\n",
    "    RMSE_climatology_std = np.mean(RMSE_climatology_std)\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    RMSE_persist_0, RMSE_persist_0_std = RMSE(sic_exp_norm[-test_year*12*4-1:-1,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_persist_1, RMSE_persist_1_std = RMSE(sic_exp_norm[-test_year*12*4-1:-2,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_persist_2, RMSE_persist_2_std = RMSE(sic_exp_norm[-test_year*12*4-1:-3,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_persist_3, RMSE_persist_3_std = RMSE(sic_exp_norm[-test_year*12*4-1:-4,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_persist_4, RMSE_persist_4_std = RMSE(sic_exp_norm[-test_year*12*4-1:-5,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_persist_5, RMSE_persist_5_std = RMSE(sic_exp_norm[-test_year*12*4-1:-6,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_persist_6, RMSE_persist_6_std = RMSE(sic_exp_norm[-test_year*12*4-1:-7,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_persist_7, RMSE_persist_7_std = RMSE(sic_exp_norm[-test_year*12*4-1:-8,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_persist_8, RMSE_persist_8_std = RMSE(sic_exp_norm[-test_year*12*4-1:-9,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_persist_9, RMSE_persist_9_std = RMSE(sic_exp_norm[-test_year*12*4-1:-10,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_persist_10, RMSE_persist_10_std = RMSE(sic_exp_norm[-test_year*12*4-1:-11,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_persist_11, RMSE_persist_11_std = RMSE(sic_exp_norm[-test_year*12*4-1:-12,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_persist_12, RMSE_persist_12_std = RMSE(sic_exp_norm[-test_year*12*4-1:-13,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_persist_13, RMSE_persist_13_std = RMSE(sic_exp_norm[-test_year*12*4-1:-14,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_persist_14, RMSE_persist_14_std = RMSE(sic_exp_norm[-test_year*12*4-1:-15,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_persist_15, RMSE_persist_15_std = RMSE(sic_exp_norm[-test_year*12*4-1:-16,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print('##############################################################')\n",
    "    print('########   land-sea correction for sic prediction    #########')\n",
    "    print('##############################################################')\n",
    "    # correction for float point at 0\n",
    "    lead_pred_sic[lead_pred_sic<0] = 0\n",
    "    # extend the dimension of sea ice mask\n",
    "    sea_ice_mask_test = np.repeat(sea_ice_mask_barents[np.newaxis,:,:],test_year*48,0)\n",
    "    # correct the land cells in the prediction\n",
    "    for i in range(step_lead):\n",
    "        lead_pred_sic[:,i,:,:] = lead_pred_sic[:,i,:,:] * sea_ice_mask_test\n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    RMSE_ConvLSTM_0, RMSE_ConvLSTM_0_std = RMSE(lead_pred_sic[:,0,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_1, RMSE_ConvLSTM_1_std = RMSE(lead_pred_sic[:-1,1,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_2, RMSE_ConvLSTM_2_std = RMSE(lead_pred_sic[:-2,2,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_3, RMSE_ConvLSTM_3_std = RMSE(lead_pred_sic[:-3,3,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_4, RMSE_ConvLSTM_4_std = RMSE(lead_pred_sic[:-4,4,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_5, RMSE_ConvLSTM_5_std = RMSE(lead_pred_sic[:-5,5,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_6, RMSE_ConvLSTM_6_std = RMSE(lead_pred_sic[:-6,6,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_7, RMSE_ConvLSTM_7_std = RMSE(lead_pred_sic[:-7,7,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_8, RMSE_ConvLSTM_8_std = RMSE(lead_pred_sic[:-8,8,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_9, RMSE_ConvLSTM_9_std = RMSE(lead_pred_sic[:-9,9,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_10, RMSE_ConvLSTM_10_std = RMSE(lead_pred_sic[:-10,10,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_11, RMSE_ConvLSTM_11_std = RMSE(lead_pred_sic[:-11,11,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_12, RMSE_ConvLSTM_12_std = RMSE(lead_pred_sic[:-12,12,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_13, RMSE_ConvLSTM_13_std = RMSE(lead_pred_sic[:-13,13,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_14, RMSE_ConvLSTM_14_std = RMSE(lead_pred_sic[:-14,14,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_15, RMSE_ConvLSTM_15_std = RMSE(lead_pred_sic[:-15,15,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_0),\"+-\",np.mean(RMSE_ConvLSTM_0_std))\n",
    "    print(\"Mean RMSE with testing data - Climatology\")\n",
    "    print(RMSE_climatology,\"+-\",RMSE_climatology_std)\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_0),\"+-\",np.mean(RMSE_persist_0_std))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_1),\"+-\",np.mean(RMSE_ConvLSTM_1_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_1),\"+-\",np.mean(RMSE_persist_1_std))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_2),\"+-\",np.mean(RMSE_ConvLSTM_2_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_2),\"+-\",np.mean(RMSE_persist_2_std))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_3),\"+-\",np.mean(RMSE_ConvLSTM_3_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_3),\"+-\",np.mean(RMSE_persist_3_std))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_4),\"+-\",np.mean(RMSE_ConvLSTM_4_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_4),\"+-\",np.mean(RMSE_persist_4_std))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_5),\"+-\",np.mean(RMSE_ConvLSTM_5_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_5),\"+-\",np.mean(RMSE_persist_5_std))\n",
    "    print(\"*******************     Lead time 6     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_6),\"+-\",np.mean(RMSE_ConvLSTM_6_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_6),\"+-\",np.mean(RMSE_persist_6_std))\n",
    "    print(\"*******************     Lead time 7     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_7),\"+-\",np.mean(RMSE_ConvLSTM_7_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_7),\"+-\",np.mean(RMSE_persist_7_std))\n",
    "    print(\"*******************     Lead time 8     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_8),\"+-\",np.mean(RMSE_ConvLSTM_8_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_8),\"+-\",np.mean(RMSE_persist_8_std))\n",
    "    print(\"*******************     Lead time 9     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_9),\"+-\",np.mean(RMSE_ConvLSTM_9_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_9),\"+-\",np.mean(RMSE_persist_9_std))\n",
    "    print(\"*******************     Lead time 10     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_10),\"+-\",np.mean(RMSE_ConvLSTM_10_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_10),\"+-\",np.mean(RMSE_persist_10_std))\n",
    "    print(\"*******************     Lead time 11     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_11),\"+-\",np.mean(RMSE_ConvLSTM_11_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_11),\"+-\",np.mean(RMSE_persist_11_std))\n",
    "    print(\"*******************     Lead time 12     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_12),\"+-\",np.mean(RMSE_ConvLSTM_12_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_12),\"+-\",np.mean(RMSE_persist_12_std))\n",
    "    print(\"*******************     Lead time 13     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_13),\"+-\",np.mean(RMSE_ConvLSTM_13_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_13),\"+-\",np.mean(RMSE_persist_13_std))\n",
    "    print(\"*******************     Lead time 14     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_14),\"+-\",np.mean(RMSE_ConvLSTM_14_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_14),\"+-\",np.mean(RMSE_persist_14_std))\n",
    "    print(\"*******************     Lead time 15     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_15),\"+-\",np.mean(RMSE_ConvLSTM_15_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_15),\"+-\",np.mean(RMSE_persist_15_std))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_rmse_pred_16weeks.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_0),np.mean(RMSE_ConvLSTM_0_std)))\n",
    "    f.write(\"RMSE - Climatology    {} + - {}\\n\".format(RMSE_climatology, RMSE_climatology_std))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_0),np.mean(RMSE_persist_0_std)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_1),np.mean(RMSE_ConvLSTM_1_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_1),np.mean(RMSE_persist_1_std)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_2),np.mean(RMSE_ConvLSTM_2_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_2),np.mean(RMSE_persist_2_std)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_3),np.mean(RMSE_ConvLSTM_3_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_3),np.mean(RMSE_persist_3_std)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_4),np.mean(RMSE_ConvLSTM_4_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_4),np.mean(RMSE_persist_4_std)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_5),np.mean(RMSE_ConvLSTM_5_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_5),np.mean(RMSE_persist_5_std)))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_6),np.mean(RMSE_ConvLSTM_6_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_6),np.mean(RMSE_persist_6_std)))\n",
    "    f.write(\"*******************     Lead time 7     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_7),np.mean(RMSE_ConvLSTM_7_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_7),np.mean(RMSE_persist_7_std)))\n",
    "    f.write(\"*******************     Lead time 8     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_8),np.mean(RMSE_ConvLSTM_8_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_8),np.mean(RMSE_persist_8_std)))\n",
    "    f.write(\"*******************     Lead time 9     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_9),np.mean(RMSE_ConvLSTM_9_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_9),np.mean(RMSE_persist_9_std)))\n",
    "    f.write(\"*******************     Lead time 10     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_10),np.mean(RMSE_ConvLSTM_10_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_10),np.mean(RMSE_persist_10_std)))\n",
    "    f.write(\"*******************     Lead time 11     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_11),np.mean(RMSE_ConvLSTM_11_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_11),np.mean(RMSE_persist_11_std)))\n",
    "    f.write(\"*******************     Lead time 12     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_12),np.mean(RMSE_ConvLSTM_12_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_12),np.mean(RMSE_persist_12_std)))\n",
    "    f.write(\"*******************     Lead time 13     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_13),np.mean(RMSE_ConvLSTM_13_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_13),np.mean(RMSE_persist_13_std)))\n",
    "    f.write(\"*******************     Lead time 14     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_14),np.mean(RMSE_ConvLSTM_14_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_14),np.mean(RMSE_persist_14_std)))\n",
    "    f.write(\"*******************     Lead time 15     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_15),np.mean(RMSE_ConvLSTM_15_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_15),np.mean(RMSE_persist_15_std)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = week2month(lead_pred_sic[:,0,:,:], i)\n",
    "        persist_monthly_series = week2month(sic_exp_norm[-test_year*12*4-1:-1,:,:], i)\n",
    "        climatology_monthly_series = week2month(climatology_seq, i)\n",
    "        truth_monthly_series = week2month(sic_exp_norm[-test_year*12*4:,:,:], i)\n",
    "        \n",
    "        rmse_ConvLSTM_monthly, rmse_ConvLSTM_monthly_std = RMSE(ConvLSTM_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_persist_monthly, rmse_persist_monthly_std = RMSE(persist_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_climatology_monthly, rmse_climatology_monthly_std = RMSE(climatology_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        \n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"RMSE - ConvLSTM       {} + - {}\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        print(\"RMSE - Climatology    {} + - {}\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        print(\"RMSE - Persistence    {} + - {}\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        f.write(\"RMSE - Climatology    {} + - {}\\n\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        \n",
    "    #f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating accuracy/recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating accuracy/recall/precision  *********************')\n",
    "    def accuracy(pred, label):\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        boolean = (pred==label)\n",
    "        accu_seq = np.mean(np.mean(boolean.astype(float),2),1)\n",
    "        accu_spa = np.mean(boolean.astype(float),0)\n",
    "        \n",
    "        return accu_seq, accu_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########          transfer the sea ice fields into binary data           ########\n",
    "    #################################################################################\n",
    "    # ice concentration below the threshold is regarded as no ice, the value is from\n",
    "    # https://nsidc.org/cryosphere/seaice/data/terminology.html\n",
    "    criterion_0 = 0.15 \n",
    "    # remove the area weight\n",
    "    sic_exp_denorm = np.zeros(sic_exp_norm.shape, dtype=float)\n",
    "    lead_pred_sic_denorm = np.zeros(lead_pred_sic.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        lead_pred_sic_denorm[:,:,i,:] = lead_pred_sic[:,:,i,:] /dx[i+12] * dx[35]\n",
    "        sic_exp_denorm[:,i,:] = sic_exp_norm[:,i,:] / dx[i+12] * dx[35]\n",
    "    # turn sea ice fields into binary data\n",
    "    lead_pred_sic_bin = lead_pred_sic_denorm[:]\n",
    "    sic_exp_bin = sic_exp_denorm[:]\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin <= criterion_0] = 0\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin > criterion_0] = 1\n",
    "    sic_exp_bin[sic_exp_bin <= criterion_0] = 0\n",
    "    sic_exp_bin[sic_exp_bin > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    lead_pred_sic_bin = lead_pred_sic_bin.astype(int)\n",
    "    sic_exp_bin = sic_exp_bin.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9742024739583334\n",
      "Total accuracy with testing data - Climatology\n",
      "0.8557477678571428\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9749193948412698\n",
      "*******************     Lead time 1     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9553493517825979\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9597396222887059\n",
      "*******************     Lead time 2     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9427279135338344\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9483787593984963\n",
      "*******************     Lead time 3     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9320475875535399\n",
      "Total accuracy with testing data - Persistence\n",
      "0.936889802217183\n",
      "*******************     Lead time 4     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9215583839918947\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9253340298885512\n",
      "*******************     Lead time 5     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9106744970715559\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9133960402342755\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "accuracy - ConvLSTM       0.9496372767857143\n",
      "accuracy - Climatology    0.8189174107142857\n",
      "accuracy - Persistence    0.9498697916666667\n",
      "*******************    2     *******************\n",
      "accuracy - ConvLSTM       0.9633556547619048\n",
      "accuracy - Climatology    0.8280784970238095\n",
      "accuracy - Persistence    0.9636811755952381\n",
      "*******************    3     *******************\n",
      "accuracy - ConvLSTM       0.9649367559523809\n",
      "accuracy - Climatology    0.8337983630952381\n",
      "accuracy - Persistence    0.9605189732142858\n",
      "*******************    4     *******************\n",
      "accuracy - ConvLSTM       0.9706566220238095\n",
      "accuracy - Climatology    0.8166852678571428\n",
      "accuracy - Persistence    0.968563988095238\n",
      "*******************    5     *******************\n",
      "accuracy - ConvLSTM       0.9681454613095238\n",
      "accuracy - Climatology    0.7997116815476191\n",
      "accuracy - Persistence    0.9637276785714285\n",
      "*******************    6     *******************\n",
      "accuracy - ConvLSTM       0.9808872767857144\n",
      "accuracy - Climatology    0.8109654017857143\n",
      "accuracy - Persistence    0.9734933035714286\n",
      "*******************    7     *******************\n",
      "accuracy - ConvLSTM       0.9902808779761905\n",
      "accuracy - Climatology    0.905087425595238\n",
      "accuracy - Persistence    0.9868396577380952\n",
      "*******************    8     *******************\n",
      "accuracy - ConvLSTM       0.9959077380952381\n",
      "accuracy - Climatology    0.9574032738095237\n",
      "accuracy - Persistence    0.9961402529761905\n",
      "*******************    9     *******************\n",
      "accuracy - ConvLSTM       0.9962332589285714\n",
      "accuracy - Climatology    0.9603329613095238\n",
      "accuracy - Persistence    0.998046875\n",
      "*******************    10     *******************\n",
      "accuracy - ConvLSTM       0.9816313244047619\n",
      "accuracy - Climatology    0.9025762648809523\n",
      "accuracy - Persistence    0.9930245535714286\n",
      "*******************    11     *******************\n",
      "accuracy - ConvLSTM       0.9683314732142857\n",
      "accuracy - Climatology    0.8140811011904763\n",
      "accuracy - Persistence    0.9844680059523809\n",
      "*******************    12     *******************\n",
      "accuracy - ConvLSTM       0.9604259672619049\n",
      "accuracy - Climatology    0.8213355654761905\n",
      "accuracy - Persistence    0.9606584821428572\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with accuracy               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    accu_climatology, _ = accuracy(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    accu_persist_0, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_persist_1, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_persist_2, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_persist_3, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_persist_4, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_persist_5, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    accu_ConvLSTM_0, _ = accuracy(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_ConvLSTM_1, _ = accuracy(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_ConvLSTM_2, _ = accuracy(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_ConvLSTM_3, _ = accuracy(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_ConvLSTM_4, _ = accuracy(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_ConvLSTM_5, _ = accuracy(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_0))\n",
    "    print(\"Total accuracy with testing data - Climatology\")\n",
    "    print(np.mean(accu_climatology))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_1))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_2))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_3))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_4))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_5))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_accuracy_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_0)))\n",
    "    f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accu_climatology)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_1)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_2)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_3)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_4)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_5)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"accuracy - ConvLSTM       {}\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Climatology    {}\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Persistence    {}\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating recall/precision  *********************')\n",
    "    # positive is sea ice = 1\n",
    "    \n",
    "    def recall(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total actual positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False negative (is 1 but predict 0)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 0] = 2\n",
    "        label_dummy_2[label == 1] = 2\n",
    "        # count False Positive events\n",
    "        falseNegative = (pred_dummy_2 == label_dummy_2)\n",
    "\n",
    "#         recall_seq = np.mean(np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                     (truePositive.astype(float) + falseNegative.astype(float))),2),1)\n",
    "        \n",
    "        recall_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                       np.sum(np.sum(falseNegative.astype(float),2),1))\n",
    "        \n",
    "#         recall_spa = np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                           (truePositive.astype(float) + falseNegative.astype(float))),0)\n",
    "        \n",
    "        recall_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                             np.sum(falseNegative.astype(float),0))\n",
    "        \n",
    "        #return recall_seq, recall_spa\n",
    "        return np.nan_to_num(recall_seq), np.nan_to_num(recall_spa)\n",
    "    \n",
    "    def precision(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total predicted positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False positive (is 0 but predict 1)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 1] = 2\n",
    "        label_dummy_2[label == 0] = 2\n",
    "        # count False Positive events\n",
    "        falsePositive = (pred_dummy_2 == label_dummy_2)\n",
    "        \n",
    "        prec_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                     np.sum(np.sum(falsePositive.astype(float),2),1))\n",
    "        \n",
    "        prec_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                           np.sum(falsePositive.astype(float),0))\n",
    "        \n",
    "        return np.nan_to_num(prec_seq), np.nan_to_num(prec_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7294511802947931\n",
      "Total recall with testing data - Climatology\n",
      "0.7682106239728338\n",
      "Total recall with testing data - Persistence\n",
      "0.6956084994582069\n",
      "*******************     Lead time 1     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7050962750794713\n",
      "Total recall with testing data - Persistence\n",
      "0.6436574679557777\n",
      "*******************     Lead time 2     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6909375651664893\n",
      "Total recall with testing data - Persistence\n",
      "0.610596193945815\n",
      "*******************     Lead time 3     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6749170520140996\n",
      "Total recall with testing data - Persistence\n",
      "0.5745854442827217\n",
      "*******************     Lead time 4     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6594568455857532\n",
      "Total recall with testing data - Persistence\n",
      "0.5492875256810292\n",
      "*******************     Lead time 5     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6446762471590884\n",
      "Total recall with testing data - Persistence\n",
      "0.5259148468998884\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "recall - ConvLSTM       0.860598886960916\n",
      "recall - Climatology    0.9985675754922358\n",
      "recall - Persistence    0.8314841347620789\n",
      "*******************    2     *******************\n",
      "recall - ConvLSTM       0.9453464077344558\n",
      "recall - Climatology    0.9988817097415507\n",
      "recall - Persistence    0.9345966147874042\n",
      "*******************    3     *******************\n",
      "recall - ConvLSTM       0.9346326494497246\n",
      "recall - Climatology    0.9997340425531915\n",
      "recall - Persistence    0.9106219389571815\n",
      "*******************    4     *******************\n",
      "recall - ConvLSTM       0.9685899947294929\n",
      "recall - Climatology    0.998480953251909\n",
      "recall - Persistence    0.967307576852602\n",
      "*******************    5     *******************\n",
      "recall - ConvLSTM       0.9754766488516187\n",
      "recall - Climatology    0.9983039583340307\n",
      "recall - Persistence    0.980571197002486\n",
      "*******************    6     *******************\n",
      "recall - ConvLSTM       0.9473765015441237\n",
      "recall - Climatology    0.99876677278424\n",
      "recall - Persistence    0.97613758406689\n",
      "*******************    7     *******************\n",
      "recall - ConvLSTM       0.6955869319793339\n",
      "recall - Climatology    0.6825157351595051\n",
      "recall - Persistence    0.731919166058951\n",
      "*******************    8     *******************\n",
      "recall - ConvLSTM       0.25801363975690794\n",
      "recall - Climatology    0.17666547941988756\n",
      "recall - Persistence    0.2848559532321151\n",
      "*******************    9     *******************\n",
      "recall - ConvLSTM       0.24191735537190082\n",
      "recall - Climatology    0.08532851239669421\n",
      "recall - Persistence    0.24395041322314048\n",
      "*******************    10     *******************\n",
      "recall - ConvLSTM       0.3462687717754512\n",
      "recall - Climatology    0.31790314876598696\n",
      "recall - Persistence    0.2090523667871815\n",
      "*******************    11     *******************\n",
      "recall - ConvLSTM       0.7261897291259987\n",
      "recall - Climatology    0.96659537082369\n",
      "recall - Persistence    0.5508195922988457\n",
      "*******************    12     *******************\n",
      "recall - ConvLSTM       0.8534166462575934\n",
      "recall - Climatology    0.9967842289510855\n",
      "recall - Persistence    0.7259854554696066\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with recall               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    recall_climatology, _ = recall(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    recall_persist_0, _ = recall(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_persist_1, _ = recall(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_persist_2, _ = recall(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_persist_3, _ = recall(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_persist_4, _ = recall(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_persist_5, _ = recall(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    recall_ConvLSTM_0, _ = recall(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_ConvLSTM_1, _ = recall(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_ConvLSTM_2, _ = recall(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_ConvLSTM_3, _ = recall(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_ConvLSTM_4, _ = recall(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_ConvLSTM_5, _ = recall(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_0))\n",
    "    print(\"Total recall with testing data - Climatology\")\n",
    "    print(np.mean(recall_climatology))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_1))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_2))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_3))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_4))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_5))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_recall_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total recall with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_0)))\n",
    "    f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall_climatology)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_1)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_2)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_3)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_4)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_5)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"recall - ConvLSTM       {}\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Climatology    {}\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Persistence    {}\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6626360787901849\n",
      "Total precision with testing data - Climatology\n",
      "0.4079385856750705\n",
      "Total precision with testing data - Persistence\n",
      "0.690738761572815\n",
      "*******************     Lead time 1     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6107196257319342\n",
      "Total precision with testing data - Persistence\n",
      "0.6356268444470853\n",
      "*******************     Lead time 2     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.579523479629473\n",
      "Total precision with testing data - Persistence\n",
      "0.6105407500493609\n",
      "*******************     Lead time 3     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5556280316807053\n",
      "Total precision with testing data - Persistence\n",
      "0.5833019876273877\n",
      "*******************     Lead time 4     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5321481099096353\n",
      "Total precision with testing data - Persistence\n",
      "0.553914255338053\n",
      "*******************     Lead time 5     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5133675059459688\n",
      "Total precision with testing data - Persistence\n",
      "0.5249623371537447\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "precision - ConvLSTM       0.9026181601166431\n",
      "precision - Climatology    0.5559542418518143\n",
      "precision - Persistence    0.9267649426825348\n",
      "*******************    2     *******************\n",
      "precision - ConvLSTM       0.9191504507078476\n",
      "precision - Climatology    0.6083904361457534\n",
      "precision - Persistence    0.9307656748230683\n",
      "*******************    3     *******************\n",
      "precision - ConvLSTM       0.9389789991144883\n",
      "precision - Climatology    0.625218252792524\n",
      "precision - Persistence    0.9461523115764381\n",
      "*******************    4     *******************\n",
      "precision - ConvLSTM       0.931422338150755\n",
      "precision - Climatology    0.6066702234474803\n",
      "precision - Persistence    0.9246785425436064\n",
      "*******************    5     *******************\n",
      "precision - ConvLSTM       0.8758113133167318\n",
      "precision - Climatology    0.4990962228132416\n",
      "precision - Persistence    0.8529426820551145\n",
      "*******************    6     *******************\n",
      "precision - ConvLSTM       0.8264382811201926\n",
      "precision - Climatology    0.33860928286279546\n",
      "precision - Persistence    0.7542750415249005\n",
      "*******************    7     *******************\n",
      "precision - ConvLSTM       0.5637202053127027\n",
      "precision - Climatology    0.25354943889363424\n",
      "precision - Persistence    0.53082042568696\n",
      "*******************    8     *******************\n",
      "precision - ConvLSTM       0.25508710795141093\n",
      "precision - Climatology    0.25\n",
      "precision - Persistence    0.2554656667257474\n",
      "*******************    9     *******************\n",
      "precision - ConvLSTM       0.2326843886249756\n",
      "precision - Climatology    0.23414179104477612\n",
      "precision - Persistence    0.23465289256198346\n",
      "*******************    10     *******************\n",
      "precision - ConvLSTM       0.24895253003603335\n",
      "precision - Climatology    0.2184269005947958\n",
      "precision - Persistence    0.24059244560980797\n",
      "*******************    11     *******************\n",
      "precision - ConvLSTM       0.44714826265214447\n",
      "precision - Climatology    0.24949192107359747\n",
      "precision - Persistence    0.7659697420634921\n",
      "*******************    12     *******************\n",
      "precision - ConvLSTM       0.8096209083782928\n",
      "precision - Climatology    0.45571431658043265\n",
      "precision - Persistence    0.9257847710201259\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with precision              ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    prec_climatology, _ = precision(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    prec_persist_0, _ = precision(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_persist_1, _ = precision(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_persist_2, _ = precision(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_persist_3, _ = precision(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_persist_4, _ = precision(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_persist_5, _ = precision(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    prec_ConvLSTM_0, _ = precision(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_ConvLSTM_1, _ = precision(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_ConvLSTM_2, _ = precision(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_ConvLSTM_3, _ = precision(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_ConvLSTM_4, _ = precision(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_ConvLSTM_5, _ = precision(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_0))\n",
    "    print(\"Total precision with testing data - Climatology\")\n",
    "    print(np.mean(prec_climatology))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_1))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_2))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_3))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_4))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_5))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_precision_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total precision with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_0)))\n",
    "    f.write(\"precision - Climatology    {}\\n\".format(np.mean(prec_climatology)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_1)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_2)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_3)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_4)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_5)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"precision - ConvLSTM       {}\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Climatology    {}\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Persistence    {}\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Climatology    {}\\n\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Persistence    {}\\n\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
