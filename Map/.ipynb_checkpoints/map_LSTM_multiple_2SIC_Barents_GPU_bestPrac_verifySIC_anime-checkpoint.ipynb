{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Predict the Spatial Sea Ice Concentration with ConvLSTM at weekly time scale** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2019.05.21 ** <br>\n",
    "** Last Update  : 2019.07.21 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, deepclim, matplotlib **<br>\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. We also include many climate index (to represent the forcing from atmosphere). The convolutional Long Short Time Memory neural network is used to deal with this spatial-temporal sequence problem. We use Pytorch as the deep learning framework. <br>\n",
    "<br>\n",
    "** Here we predict sea ice concentration with one extra relevant field from either ocean or atmosphere to test the predictor.** <br>\n",
    "\n",
    "Return Values   : Time series and figures <br>\n",
    "\n",
    "Here is the information of climate index in details:<br>\n",
    "**NAO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/nao.shtml <br>\n",
    "**ENSO - NINO 3.4 SST** @ 1950 Jan - 2018 Jan (817 records) <br>\n",
    "https://www.esrl.noaa.gov/psd/gcos_wgsp/Timeseries/Nino34/ <br>\n",
    "**AO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/ao.shtml <br>\n",
    "**AMO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "AMO unsmoothed, detrended from the Kaplan SST V2. The result is standarised.<br>\n",
    "https://www.esrl.noaa.gov/psd/data/timeseries/AMO/<br>\n",
    "\n",
    "(All the NOAA index shown above are given by NCEP/NCAR Reanalysis (CDAS))<br>\n",
    "\n",
    "**PDO** @ 1900 Jan - 2018 Feb (1418 records)<br>\n",
    "This PDO index comes from University of Washington, it contains SST data from the following 3 datasets:<br>\n",
    "- UKMO Historical SST data set for 1900-81;\n",
    "- Reynold's Optimally Interpolated SST (V1) for January 1982-Dec 2001)\n",
    "- OI SST Version 2 (V2) beginning January 2002 -<br>\n",
    "\n",
    "http://research.jisao.washington.edu/pdo/PDO.latest<br>\n",
    "\n",
    "The regionalization adopted here follows that of the MASIE (Multisensor Analyzed Sea Ice Extent) product available from the National Snow and Ice Data Center:<br>\n",
    "https://nsidc.org/data/masie/browse_regions<br>\n",
    "It is given by paper J.Walsh et. al., 2019. Benchmark seasonal prediction skill estimates based on regional indices.<br>\n",
    "\n",
    "The method comes from the study by Shi et. al. (2015) Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import numbers\n",
    "\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "sys.path.append(os.path.join('/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/Scripts/DeepClim'))\n",
    "#sys.path.append(\"C:\\\\Users\\\\nosta\\\\ML4Climate\\\\Scripts\\\\DeepClim\")\n",
    "import deepclim\n",
    "import deepclim.preprocess\n",
    "import deepclim.deepSeries\n",
    "import deepclim.deepArray_GPU\n",
    "#import deepclim.deepArrayStep\n",
    "#import deepclim.function\n",
    "\n",
    "# for visualization\n",
    "import deepclim.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import iris # also helps with regriding\n",
    "import iris.plot as iplt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# for animation\n",
    "import imageio\n",
    "import matplotlib.image as mgimg\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing device is Dell Inspirion 5680 with Intel Core i7-8700 x64 CPU and Nvidia GTX 1060 6GB GPU.<br>\n",
    "Here is a benchmark about cpu v.s. gtx 1060 <br>\n",
    "https://www.analyticsindiamag.com/deep-learning-tensorflow-benchmark-intel-i5-4210u-vs-geforce-nvidia-1060-6gb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2264670,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data ** <br>\n",
    "Time span of each product included: <br>\n",
    "** Reanalysis ** <br>\n",
    "- **ERA-Interim** 1979 - 2016 (ECMWF)\n",
    "- **ORAS4**       1958 - 2014 (ECMWF)\n",
    "\n",
    "** Index ** <br>\n",
    "- **NINO3.4**     1950 - 2017 (NOAA)\n",
    "- **AO**          1950 - 2017 (NOAA)\n",
    "- **NAO**         1950 - 2017 (NOAA)\n",
    "- **AMO**         1950 - 2017 (NOAA)\n",
    "- **PDO**         1950 - 2017 (University of Washington)\n",
    "\n",
    "!! These index are given by NCEP/NCAR Reanalysis (CDAS) <br>\n",
    "\n",
    "\n",
    "Alternative (not in use yet) <br>\n",
    "** Reanalysis ** <br>\n",
    "- **MERRA2**      1980 - 2016 (NASA)\n",
    "- **JRA55**       1979 - 2015 (JMA)\n",
    "- **GLORYS2V3**   1993 - 2014 (Mercartor Ocean)\n",
    "- **SODA3**       1980 - 2015\n",
    "- **PIOMASS**     1980 - 2015\n",
    "\n",
    "** Observations ** <br>\n",
    "- **NSIDC**       1958 - 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# \n",
    "#########                           datapath                             ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ERA-Interim'\n",
    "#datapath_ERAI = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ERA-Interim'\n",
    "datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4 = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/ORAS4'\n",
    "#datapath_ORAS4_mask = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "#datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "#datapath_PIOMASS = 'H:\\\\Creator_Zone\\\\Core_Database_AMET_OMET_reanalysis\\\\PIOMASS'\n",
    "#datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "#datapath_clim_index = 'F:\\\\PhD_essential\\\\Core_Database_AMET_OMET_reanalysis\\\\Climate_index'\n",
    "output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/Maps/Barents/Anime'\n",
    "#output_path = 'C:\\\\Users\\\\nosta\\\\ML4Climate\\\\PredictArctic\\\\Maps\\\\Barents\\\\Anime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    # weekly variables on ERAI grid\n",
    "    dataset_ERAI_fields_sic = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'sic_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_slp = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'slp_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_t2m = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      't2m_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_z500 = Dataset(os.path.join(datapath_ERAI,\n",
    "                                       'z500_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_z850 = Dataset(os.path.join(datapath_ERAI,\n",
    "                                       'z850_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_uv10m = Dataset(os.path.join(datapath_ERAI,\n",
    "                                       'uv10m_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_rad = Dataset(os.path.join(datapath_ERAI,\n",
    "                                        'rad_flux_weekly_erai_1979_2017.nc'))\n",
    "    #dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "    #                             'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    # OHC interpolated on ERA-Interim grid\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                'ohc_monthly_oras2erai_1978_2017.nc'))\n",
    "    dataset_index = Dataset(os.path.join(datapath_clim_index,\n",
    "                            'index_climate_monthly_regress_1950_2017.nc'))\n",
    "    #dataset_ERAI_fields_flux = Dataset(os.path.join(datapath_ERAI_fields,\n",
    "    #                                  'surface_erai_monthly_regress_1979_2017_radiation.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    #################################################################################\n",
    "    #########                        data gallery                           #########\n",
    "    #################################################################################\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2013\n",
    "    # validation: 2014 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim) - benckmark\n",
    "    SIC_ERAI = dataset_ERAI_fields_sic.variables['sic'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI = dataset_ERAI_fields_sic.variables['year'][:-1]\n",
    "    week_ERAI = dataset_ERAI_fields_sic.variables['week'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_sic.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_sic.variables['longitude'][:]\n",
    "    # T2M (ERA-Interim)\n",
    "    T2M_ERAI = dataset_ERAI_fields_t2m.variables['t2m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_t2m = dataset_ERAI_fields_t2m.variables['year'][:-1]\n",
    "    week_ERAI_t2m = dataset_ERAI_fields_t2m.variables['week'][:]\n",
    "    latitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['latitude'][:]\n",
    "    longitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['longitude'][:]\n",
    "    # SLP (ERA-Interim)\n",
    "    SLP_ERAI = dataset_ERAI_fields_slp.variables['slp'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_slp = dataset_ERAI_fields_slp.variables['year'][:-1]\n",
    "    week_ERAI_slp = dataset_ERAI_fields_slp.variables['week'][:]\n",
    "    latitude_ERAI_slp = dataset_ERAI_fields_slp.variables['latitude'][:]\n",
    "    longitude_ERAI_slp = dataset_ERAI_fields_slp.variables['longitude'][:]\n",
    "    # Z500 (ERA-Interim)\n",
    "    Z500_ERAI = dataset_ERAI_fields_z500.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_z500 = dataset_ERAI_fields_z500.variables['year'][:-1]\n",
    "    week_ERAI_z500 = dataset_ERAI_fields_z500.variables['week'][:]\n",
    "    latitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['latitude'][:]\n",
    "    longitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['longitude'][:]\n",
    "    # Z850 (ERA-Interim)\n",
    "    Z850_ERAI = dataset_ERAI_fields_z850.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_z850 = dataset_ERAI_fields_z850.variables['year'][:-1]\n",
    "    week_ERAI_z850 = dataset_ERAI_fields_z850.variables['week'][:]\n",
    "    latitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['latitude'][:]\n",
    "    longitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['longitude'][:]\n",
    "    # UV10M (ERA-Interim)\n",
    "    U10M_ERAI = dataset_ERAI_fields_uv10m.variables['u10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    V10M_ERAI = dataset_ERAI_fields_uv10m.variables['v10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['year'][:-1]\n",
    "    week_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['week'][:]\n",
    "    latitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['latitude'][:]\n",
    "    longitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['longitude'][:]\n",
    "    # SFlux (ERA-Interim)\n",
    "    SFlux_ERAI = dataset_ERAI_fields_rad.variables['SFlux'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_SFlux = dataset_ERAI_fields_rad.variables['year'][:-1]\n",
    "    week_ERAI_SFlux = dataset_ERAI_fields_rad.variables['week'][:]\n",
    "    latitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['latitude'][:]\n",
    "    longitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    #SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    #year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # OHC (ORAS4)\n",
    "    # from 1978 - 2017 (for interpolation) / from 90 N upto 40 N\n",
    "    OHC_300_ORAS4 = dataset_ORAS4_OHC.variables['OHC'][:-1,:,:67,:]/1000 # unit Peta Joule\n",
    "    latitude_ORAS4 = dataset_ORAS4_OHC.variables['latitude'][:]\n",
    "    longitude_ORAS4 = dataset_ORAS4_OHC.variables['longitude'][:]\n",
    "    mask_OHC = np.ma.getmask(OHC_300_ORAS4[0,0,:,:])\n",
    "    # AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    AO = dataset_index.variables['AO'][348:-1] # from 1979 - 2017\n",
    "    NAO = dataset_index.variables['NAO'][348:-1]\n",
    "    NINO = dataset_index.variables['NINO'][348:-1]\n",
    "    AMO = dataset_index.variables['AMO'][348:-1]\n",
    "    PDO = dataset_index.variables['PDO'][348:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # first check of grid\n",
    "    print(latitude_ERAI)\n",
    "    print(longitude_ERAI)\n",
    "    print(latitude_ERAI_t2m)\n",
    "    print(longitude_ERAI_t2m)\n",
    "    print(longitude_ORAS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ###########                 global land-sea mask                      ###########\n",
    "    #################################################################################\n",
    "    sea_ice_mask_global = np.ones((len(latitude_ERAI),len(longitude_ERAI)),dtype=float)\n",
    "    sea_ice_mask_global[SIC_ERAI[0,0,:,:]==-1] = 0\n",
    "    #################################################################################\n",
    "    ###########                regionalization sea mask                   ###########\n",
    "    #################################################################################\n",
    "    print ('*********************** create mask *************************')\n",
    "    # W:-156 E:-124 N:80 S:67\n",
    "    mask_Beaufort = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:-156 N:80 S:66\n",
    "    mask_Chukchi = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:146 E:180 N:80 S:67\n",
    "    mask_EastSiberian = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:100 E:146 N:80 S:67\n",
    "    mask_Laptev = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:60 E:100 N:80 S:67\n",
    "    mask_Kara = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:18 E:60 N:80 S:64\n",
    "    mask_Barents = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-44 E:18 N:80 S:55\n",
    "    mask_Greenland = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:180 N:90 S:80\n",
    "    mask_CenArctic = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    print ('*********************** calc mask *************************')\n",
    "    mask_Beaufort[13:31,32:76] = 1\n",
    "\n",
    "    mask_Chukchi[13:32,0:32] = 1\n",
    "    mask_Chukchi[13:32,-1] = 1\n",
    "\n",
    "    mask_EastSiberian[13:31,434:479] = 1\n",
    "\n",
    "    mask_Laptev[13:31,374:434] = 1\n",
    "\n",
    "    mask_Kara[13:31,320:374] = 1\n",
    "\n",
    "    mask_Barents[13:36,264:320] = 1\n",
    "\n",
    "    mask_Greenland[13:47,179:264] = 1\n",
    "    mask_Greenland[26:47,240:264] = 0\n",
    "\n",
    "    mask_CenArctic[:13,:] = 1\n",
    "    print ('*********************** packing *************************')\n",
    "    mask_dict = {'Beaufort': mask_Beaufort[:,:],\n",
    "                 'Chukchi': mask_Chukchi[:,:],\n",
    "                 'EastSiberian': mask_EastSiberian[:,:],\n",
    "                 'Laptev': mask_Laptev[:,:],\n",
    "                 'Kara': mask_Kara[:,:],\n",
    "                 'Barents': mask_Barents[:,:],\n",
    "                 'Greenland': mask_Greenland[:,:],\n",
    "                 'CenArctic': mask_CenArctic[:,:]}\n",
    "    seas_namelist = ['Beaufort','Chukchi','EastSiberian','Laptev',\n",
    "                     'Kara', 'Barents', 'Greenland','CenArctic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########                  temporal interpolation matrix                  ########\n",
    "    #################################################################################\n",
    "    # interpolate from monthly to weekly\n",
    "    # original monthly data will be taken as the last week of the month\n",
    "    OHC_300_ORAS4_weekly_series = np.zeros(SIC_ERAI.reshape(len(year_ERAI)*48,len(latitude_ERAI),len(longitude_ERAI)).shape,\n",
    "                                           dtype=float)\n",
    "    OHC_300_ORAS4_series= deepclim.preprocess.operator.unfold(OHC_300_ORAS4)\n",
    "    # calculate the difference between two months\n",
    "    OHC_300_ORAS4_deviation_series = (OHC_300_ORAS4_series[1:,:,:] - OHC_300_ORAS4_series[:-1,:,:]) / 4\n",
    "    for i in np.arange(4):\n",
    "        OHC_300_ORAS4_weekly_series[3-i::4,:,:] = OHC_300_ORAS4_series[12:,:,:] - i * OHC_300_ORAS4_deviation_series[11:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / 480\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "    SFlux_ERAI_area = np.zeros(SFlux_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,:,i,:] = SIC_ERAI[:,:,i,:]* dx[i] * dy / 1E+6 # unit km2\n",
    "        SFlux_ERAI_area[:,:,i,:] = SFlux_ERAI[:,:,i,:]* dx[i] * dy / 1E+12 # unit TeraWatt\n",
    "    SIC_ERAI_area[SIC_ERAI_area<0] = 0 # switch the mask from -1 to 0\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    SIC_ERAI_area_series = deepclim.preprocess.operator.unfold(SIC_ERAI_area)\n",
    "    T2M_ERAI_series = deepclim.preprocess.operator.unfold(T2M_ERAI)\n",
    "    SLP_ERAI_series = deepclim.preprocess.operator.unfold(SLP_ERAI)\n",
    "    Z500_ERAI_series = deepclim.preprocess.operator.unfold(Z500_ERAI)\n",
    "    Z850_ERAI_series = deepclim.preprocess.operator.unfold(Z850_ERAI)\n",
    "    U10M_ERAI_series = deepclim.preprocess.operator.unfold(U10M_ERAI)\n",
    "    V10M_ERAI_series = deepclim.preprocess.operator.unfold(V10M_ERAI)\n",
    "    SFlux_ERAI_area_series = deepclim.preprocess.operator.unfold(SFlux_ERAI_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select land-sea mask\n",
    "    sea_ice_mask_barents = sea_ice_mask_global[12:36,264:320]\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select the area between greenland and ice land for instance 60-70 N / 44-18 W\n",
    "    sic_exp = SIC_ERAI_area_series[:,12:36,264:320]\n",
    "    t2m_exp = T2M_ERAI_series[:,12:36,264:320]\n",
    "    slp_exp = SLP_ERAI_series[:,12:36,264:320]\n",
    "    z500_exp = Z500_ERAI_series[:,12:36,264:320]\n",
    "    z850_exp = Z850_ERAI_series[:,12:36,264:320]\n",
    "    u10m_exp = U10M_ERAI_series[:,12:36,264:320]\n",
    "    v10m_exp = V10M_ERAI_series[:,12:36,264:320]\n",
    "    sflux_exp = SFlux_ERAI_area_series[:,12:36,264:320]\n",
    "    ohc_exp = OHC_300_ORAS4_weekly_series[:,12:36,264:320]\n",
    "    print(sic_exp.shape)\n",
    "    print(t2m_exp.shape)\n",
    "    print(slp_exp.shape)\n",
    "    print(z500_exp.shape)\n",
    "    print(u10m_exp.shape)\n",
    "    print(v10m_exp.shape)\n",
    "    print(sflux_exp.shape)\n",
    "    print(ohc_exp.shape)\n",
    "    print(latitude_ERAI[12:36])\n",
    "    print(longitude_ERAI[264:320])\n",
    "    print(latitude_ORAS4[12:36])\n",
    "    print(longitude_ORAS4[264:320])\n",
    "    #print(latitude_ERAI[26:40])\n",
    "    #print(longitude_ERAI[180:216])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('=========================   normalize data   ===========================')\n",
    "    sic_exp_norm = deepclim.preprocess.operator.normalize(sic_exp)\n",
    "    t2m_exp_norm = deepclim.preprocess.operator.normalize(t2m_exp)\n",
    "    slp_exp_norm = deepclim.preprocess.operator.normalize(slp_exp)\n",
    "    z500_exp_norm = deepclim.preprocess.operator.normalize(z500_exp)\n",
    "    z850_exp_norm = deepclim.preprocess.operator.normalize(z850_exp)\n",
    "    u10m_exp_norm = deepclim.preprocess.operator.normalize(u10m_exp)\n",
    "    v10m_exp_norm = deepclim.preprocess.operator.normalize(v10m_exp)\n",
    "    sflux_exp_norm = deepclim.preprocess.operator.normalize(sflux_exp)\n",
    "    ohc_exp_norm = deepclim.preprocess.operator.normalize(ohc_exp)\n",
    "    print('================  save the normalizing factor  =================')\n",
    "    sic_max = np.amax(sic_exp)\n",
    "    sic_min = np.amin(sic_exp)\n",
    "    print(sic_max,\"km2\")\n",
    "    print(sic_min,\"km2\")\n",
    "    ohc_max = np.amax(ohc_exp)\n",
    "    ohc_min = np.amin(ohc_exp)\n",
    "    t2m_max = np.amax(t2m_exp)\n",
    "    t2m_min = np.amin(t2m_exp)\n",
    "    slp_max = np.amax(slp_exp)\n",
    "    slp_min = np.amin(slp_exp)\n",
    "    z500_max = np.amax(z500_exp)\n",
    "    z500_min = np.amin(z500_exp)\n",
    "    z850_max = np.amax(z850_exp)\n",
    "    z850_min = np.amin(z850_exp)\n",
    "    u10m_max = np.amax(u10m_exp)\n",
    "    u10m_min = np.amin(u10m_exp)\n",
    "    v10m_max = np.amax(v10m_exp)\n",
    "    v10m_min = np.amin(v10m_exp)\n",
    "    sflux_max = np.amax(sflux_exp)\n",
    "    sflux_min = np.amin(sflux_exp)    \n",
    "    print ('====================    A series of time (index)    ====================')\n",
    "    _, yy, xx = sic_exp_norm.shape # get the lat lon dimension\n",
    "    year = np.arange(1979,2017,1)\n",
    "    year_cycle = np.repeat(year,48)\n",
    "    month_cycle = np.repeat(np.arange(1,13,1),4)\n",
    "    month_cycle = np.tile(month_cycle,len(year)+1) # one extra repeat for lead time dependent prediction\n",
    "    month_cycle.astype(float)\n",
    "    month_2D = np.repeat(month_cycle[:,np.newaxis],yy,1)\n",
    "    month_exp = np.repeat(month_2D[:,:,np.newaxis],xx,2)\n",
    "    print ('===================  artificial data for evaluation ====================')\n",
    "    # calculate climatology of SIC\n",
    "#     seansonal_cycle_SIC = np.zeros(48,dtype=float)\n",
    "#     for i in np.arange(48):\n",
    "#         seansonal_cycle_SIC[i] = np.mean(SIC_ERAI_sum_norm[i::48],axis=0)\n",
    "    # weight for loss\n",
    "#     weight_month = np.array([0,1,1,\n",
    "#                              1,0,0,\n",
    "#                              1,1,1,\n",
    "#                              0,0,0])\n",
    "    #weight_loss = np.repeat(weight_month,4)\n",
    "    #weight_loss = np.tile(weight_loss,len(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(t2m_exp.shape)\n",
    "    ax = plt.contourf(t2m_exp[443,:,:])\n",
    "    print(t2m_exp[443,:,:])\n",
    "    print(month_exp[0,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_input = np.stack((sic_exp_norm[1,:,:],\n",
    "                        t2m_exp_norm[1,:,:],\n",
    "                        ohc_exp_norm[1,:,:],\n",
    "                        month_exp[1,:,:]))\n",
    "    print(x_input[1,:,:])\n",
    "    print(x_input[:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  parameter for check  *********************')\n",
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    # specifications of neural network\n",
    "    input_channels = 10\n",
    "    hidden_channels = [10, 9, 1] # number of channels & hidden layers, the channels of last layer is the channels of output, too\n",
    "    #hidden_channels = [3, 3, 3, 3, 2]\n",
    "    #hidden_channels = [2]\n",
    "    kernel_size = 3\n",
    "    # here we input a sequence and predict the next step only\n",
    "    #step = 1 # how many steps to predict ahead\n",
    "    #effective_step = [0] # step to output\n",
    "    batch_size = 1\n",
    "    #num_layers = 1\n",
    "    learning_rate = 0.005\n",
    "    num_epochs = 1500\n",
    "    print (torch.__version__)\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    print ('*******************  cross validation and testing data  *********************')\n",
    "    # take 10% data as cross-validation data\n",
    "    cross_valid_year = 4\n",
    "    # take 10% years as testing data\n",
    "    test_year = 4\n",
    "    # minibatch\n",
    "    #iterations = 3 # training data divided into 3 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  preview of input tensor  *********************')\n",
    "    #plt.plot(SIC_ERAI_sum)\n",
    "    #print(SIC_ERAI_sum_norm[:-test_year*12])\n",
    "    #print(x_input.shape)\n",
    "    #print(x_input[:,:,:])\n",
    "    sequence_len, _, _ = sic_exp_norm.shape\n",
    "    print(sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print ('*******************  module for calculating accuracy  *********************')\n",
    "#     def accuracy(out, labels):\n",
    "#         outputs = np.argmax(out, axis=1)\n",
    "#     return np.sum(outputs==labels)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%time\n",
    "    print ('*******************  load exsited LSTM model  *********************')\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500.pkl'))\n",
    "    model = torch.load(os.path.join(output_path,'convlstm_era_sic_t2m_slp_z500_z850_uv10m_sflux_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    print(model)\n",
    "     # check the sequence length (dimension in need for post-processing)\n",
    "    sequence_len, height, width = sic_exp_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  evaluation matrix  *********************')\n",
    "    # The prediction will be evaluated through RMSE against climatology\n",
    "    \n",
    "    # error score for temporal-spatial fields, without keeping spatial pattern\n",
    "    def RMSE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the RMSE. x is input series and y is reference series.\n",
    "        It calculates RMSE over the domain, not over time. The spatial structure\n",
    "        will not be kept.\n",
    "        Parameter\n",
    "        ----------------------\n",
    "        x: input time series with the shape [time, lat, lon]\n",
    "        \"\"\"\n",
    "        x_series = x.reshape(x.shape[0],-1)\n",
    "        y_series = y.reshape(y.shape[0],-1)\n",
    "        rmse = np.sqrt(np.mean((x_series - y_series)**2,1))\n",
    "        rmse_std = np.sqrt(np.std((x_series - y_series)**2,1))\n",
    "    \n",
    "        return rmse, rmse_std\n",
    "    \n",
    "    # error score for temporal-spatial fields, keeping spatial pattern\n",
    "    def MAE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the MAE. x is input series and y is reference series.\n",
    "        It calculate MAE over time and keeps the spatial structure.\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(x-y),0)\n",
    "        \n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%time\n",
    "    #################################################################################\n",
    "    ########       lead time depedent prediction for cross-validation        ########\n",
    "    #################################################################################\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(cross_valid_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[-cross_valid_year*12*4-test_year*12*4:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 6 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((cross_valid_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(cross_valid_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-cross_valid_year*12*4-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-cross_valid_year*12*4-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    ohc_exp_norm[i-1,:,:],\n",
    "                                    t2m_exp_norm[i-1,:,:],\n",
    "                                    slp_exp_norm[i-1,:,:],\n",
    "                                    z500_exp_norm[i-1,:,:],\n",
    "                                    z850_exp_norm[i-1,:,:],\n",
    "                                    u10m_exp_norm[i-1,:,:],\n",
    "                                    v10m_exp_norm[i-1,:,:],\n",
    "                                    sflux_exp_norm[i-1,:,:],                                    \n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-cross_valid_year*12*4-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                # use the predicted data to make new prediction\n",
    "                x_input = np.stack((lead_pred_sic[step,i-(sequence_len-cross_valid_year*12*4-test_year*12*4 + step +1),:,:],\n",
    "                                    ohc_exp_norm[i-1,:,:],\n",
    "                                    t2m_exp_norm[i-1,:,:],\n",
    "                                    slp_exp_norm[i-1,:,:],\n",
    "                                    z500_exp_norm[i-1,:,:],\n",
    "                                    z850_exp_norm[i-1,:,:],\n",
    "                                    u10m_exp_norm[i-1,:,:],\n",
    "                                    v10m_exp_norm[i-1,:,:],\n",
    "                                    sflux_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #############################################################################################################\n",
    "    ########        visualization of lead time dependent prediction with cross-validation data           ########\n",
    "    #############################################################################################################\n",
    "    index_plot = np.arange(cross_valid_year*12*4)\n",
    "    index_plot_step = np.arange(cross_valid_year*12*4+1)\n",
    "    year_index = np.arange(2009,2013,1)\n",
    "    # repeat climatology as reference\n",
    "    #climatology = np.tile(seansonal_cycle_SIC,len(year_ERAI))\n",
    "    # create index\n",
    "    # correction for float point at 0\n",
    "    lead_pred_sic[lead_pred_sic<0] = 0\n",
    "    # extend the dimension of sea ice mask\n",
    "    sea_ice_mask_crossValid = np.repeat(sea_ice_mask_barents[np.newaxis,:,:],cross_valid_year*48,0)\n",
    "    # correct the land cells in the prediction\n",
    "    for i in range(step_lead):\n",
    "        lead_pred_sic[:,i,:,:] = lead_pred_sic[:,i,:,:] * sea_ice_mask_crossValid\n",
    "    print (\"*******************  Predicted Ice Extent  **********************\")\n",
    "    # include text box in the figure\n",
    "    #text_content = '$RMSE=%.3f$ ' % (error_pred)\n",
    "    sic_extend_lead = np.sum(np.sum(lead_pred_sic,3),2)\n",
    "    \n",
    "    colormap=cm.autumn(range(cross_valid_year*12*4))\n",
    "    \n",
    "    fig0 = plt.figure(figsize=(12,6))\n",
    "    for i in range(cross_valid_year*12*4):\n",
    "        plt.plot(np.arange(index_plot[i],index_plot[i]+step_lead), sic_extend_lead[i,:] * sic_max / 1E+6, color=colormap[i])\n",
    "    plt.scatter(index_plot, sic_extend_lead[:,0] * sic_max / 1E+6, color='r', label=\"Lead 0\")\n",
    "    plt.scatter(index_plot_step[1:], sic_extend_lead[:,1] * sic_max / 1E+6, color='g', label=\"Lead 1\")\n",
    "    plt.plot(index_plot, np.sum(np.sum(sic_exp_norm[-cross_valid_year*12*4-test_year*12*4:-test_year*12*4,:,:],2),1) * sic_max / 1E+6,\n",
    "             'b', label=\"Observation\")\n",
    "    plt.scatter(index_plot, np.sum(np.sum(sic_exp_norm[-cross_valid_year*12*4-test_year*12*4:-test_year*12*4,:,:],2),1) * sic_max / 1E+6,\n",
    "                color='b')\n",
    "    #plt.plot(index_plot, climatology, 'c--',label=\"climatology\")\n",
    "    plt.xlabel('Time (week)',fontsize = 14)\n",
    "    plt.ylabel('Sea ice extent (million square kilometers)',fontsize = 14)\n",
    "    plt.xticks(np.arange(0,cross_valid_year*12*4,6*4),(['200901', '200907', '201001', '201007',\n",
    "                                                        '201101', '201107', '201201', '201207']),\n",
    "               fontsize = 12)\n",
    "    plt.yticks(np.arange(0,0.5,0.1),fontsize = 12)\n",
    "    plt.legend(frameon=False, loc=1, prop={'size': 12})\n",
    "    #props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "    #ax = plt.gca()\n",
    "    #ax.text(0.03,0.2,text_content,transform=ax.transAxes,fontsize=10,verticalalignment='top',bbox=props)\n",
    "    plt.show()\n",
    "    fig0.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred_lead_crossValid.png'),dpi=200)\n",
    "\n",
    "    print (\"*******************  Prediction Ice Distribution  **********************\")\n",
    "    mae = MAE(lead_pred_sic[:,0,:,:],\n",
    "              sic_exp_norm[-cross_valid_year*12*4-test_year*12*4:-test_year*12*4,:,:])\n",
    "    label = 'MAE (square kilometers)'\n",
    "    ticks = [i for i in np.linspace(0,200,11)]\n",
    "    deepclim.visual.plots.geograph(latitude_ERAI[12:36], longitude_ERAI[264:320],\n",
    "                                   mae * sic_max, label, ticks,\n",
    "                                   os.path.join(output_path,'spatial_sic_mae_avg_crossValid.png'),\n",
    "                                   boundary='Barents_Polar', colormap='Blues')\n",
    "    print (\"*******************  Other variables (Prediction with testing sets only) **********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    %%time\n",
    "    #################################################################################\n",
    "    ########  operational lead time dependent prediction with testing data   ########\n",
    "    #################################################################################\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(test_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 6 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    ohc_exp_norm[i-1,:,:],\n",
    "                                    t2m_exp_norm[i-1,:,:],\n",
    "                                    slp_exp_norm[i-1,:,:],\n",
    "                                    z500_exp_norm[i-1,:,:],\n",
    "                                    z850_exp_norm[i-1,:,:],\n",
    "                                    u10m_exp_norm[i-1,:,:],\n",
    "                                    v10m_exp_norm[i-1,:,:],\n",
    "                                    sflux_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[i-1,:,:],\n",
    "                                        t2m_exp_norm[i-1,:,:],\n",
    "                                        slp_exp_norm[i-1,:,:],\n",
    "                                        z500_exp_norm[i-1,:,:],\n",
    "                                        z850_exp_norm[i-1,:,:],\n",
    "                                        u10m_exp_norm[i-1,:,:],\n",
    "                                        v10m_exp_norm[i-1,:,:],\n",
    "                                        sflux_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[-1,:,:],\n",
    "                                        t2m_exp_norm[-1,:,:],\n",
    "                                        slp_exp_norm[-1,:,:],\n",
    "                                        z500_exp_norm[-1,:,:],\n",
    "                                        z850_exp_norm[-1,:,:],\n",
    "                                        u10m_exp_norm[-1,:,:],\n",
    "                                        v10m_exp_norm[-1,:,:],\n",
    "                                        sflux_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########        correction of lead time dependent prediction          ########\n",
    "    #################################################################################\n",
    "    ######################    data cleaner   ######################\n",
    "    # repeat climatology as reference\n",
    "    #climatology = np.tile(seansonal_cycle_SIC,len(year_ERAI))\n",
    "    # create index\n",
    "    # correction for float point at 0\n",
    "    lead_pred_sic[lead_pred_sic<0] = 0\n",
    "    # extend the dimension of sea ice mask\n",
    "    sea_ice_mask_test = np.repeat(sea_ice_mask_barents[np.newaxis,:,:],test_year*48,0)\n",
    "    # correct the land cells in the prediction\n",
    "    for i in range(step_lead):\n",
    "        lead_pred_sic[:,i,:,:] = lead_pred_sic[:,i,:,:] * sea_ice_mask_test\n",
    "    #################################################################################\n",
    "    ########          transfer the sea ice fields into binary data           ########\n",
    "    #################################################################################\n",
    "    criterion_0 = 0.15 # ice concentration below the threshold is regarded as no ice\n",
    "    # remove the area weight\n",
    "    sic_exp_denorm = np.zeros(sic_exp_norm.shape, dtype=float)\n",
    "    lead_pred_sic_denorm = np.zeros(lead_pred_sic.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        lead_pred_sic_denorm[:,:,i,:] = lead_pred_sic[:,:,i,:] / dx[i+12] * dx[35]\n",
    "        sic_exp_denorm[:,i,:] = sic_exp_norm[:,i,:] / dx[i+12] * dx[35]\n",
    "    # turn sea ice fields into binary data\n",
    "    lead_pred_sic_bin = lead_pred_sic_denorm[:]\n",
    "    sic_exp_bin = sic_exp_denorm[:]\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin <= criterion_0] = 0\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin > criterion_0] = 1\n",
    "    sic_exp_bin[sic_exp_bin <= criterion_0] = 0\n",
    "    sic_exp_bin[sic_exp_bin > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    lead_pred_sic_bin = lead_pred_sic_bin.astype(int)\n",
    "    sic_exp_bin = sic_exp_bin.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########        visualization of lead time dependent prediction          ########\n",
    "    #################################################################################\n",
    "    year_index = np.arange(2013,2017,1)\n",
    "    index_plot = np.arange(test_year*12*4)\n",
    "\n",
    "    # first construct iris coordinate\n",
    "    lat_iris = iris.coords.DimCoord(latitude_ERAI[12:36], standard_name='latitude', long_name='latitude',\n",
    "                                    var_name='lat', units='degrees')\n",
    "    lon_iris = iris.coords.DimCoord(longitude_ERAI[264:320], standard_name='longitude', long_name='longitude',\n",
    "                                    var_name='lon', units='degrees')\n",
    "    # take obs sic from certain period\n",
    "    sic_select_norm = sic_exp_norm[-test_year*12*4:,:,:]\n",
    "    sic_select_bin = sic_exp_bin[-test_year*12*4:,:,:]\n",
    "    sic_persist_norm = sic_exp_norm[-test_year*12*4-1:-1,:,:]\n",
    "    \n",
    "    for i in index_plot:\n",
    "        year = year_index[i//48]\n",
    "        month = i//4 - (year-2013)*12 + 1\n",
    "        week = i - (month-1)*4 - (year-2013) * 48 + 1\n",
    "        # figure\n",
    "        fig = plt.figure(figsize=(12,10))\n",
    "        fig.suptitle('ConvLSTM SIC prediction year year {} month {} week {}'.format(year, month, week))\n",
    "        \n",
    "        # submap 1 - configure map\n",
    "        ax1 = plt.subplot(2, 2, 1, projection=ccrs.EquidistantConic(central_longitude=39.0, central_latitude=72.0))\n",
    "        ax1.set_extent([16,60,60,82],ccrs.PlateCarree()) # W:18 E:60 S:64 N:80    \n",
    "        ax1.set_aspect('1')\n",
    "        ax1.coastlines()\n",
    "        gl = ax1.gridlines(linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "        # assemble cube iris\n",
    "        cube_iris = iris.cube.Cube(lead_pred_sic[i,0,:,:], long_name='geographical field', var_name='field', \n",
    "                                   units='1', dim_coords_and_dims=[(lat_iris, 0), (lon_iris, 1)])\n",
    "        # other set-ups\n",
    "        ticks = [i for i in np.linspace(0.0,1.0,11)]\n",
    "        label = 'Pred SIC'\n",
    "        # make plots\n",
    "        cs = iplt.contourf(cube_iris, cmap='Blues',levels=ticks, extend='both', vmin=ticks[0], vmax=ticks[-1])\n",
    "        cbar = fig.colorbar(cs,extend='both', orientation='horizontal',\n",
    "                            shrink =0.8, pad=0.05)#, format=\"%.1f\")\n",
    "        cbar.set_label(label,size = 12)\n",
    "        cbar.set_ticks(ticks)\n",
    "        cbar.ax.tick_params(labelsize = 9)\n",
    "        \n",
    "        # submap 2 - configure map\n",
    "        ax2 = plt.subplot(2, 2, 2, projection=ccrs.EquidistantConic(central_longitude=39.0, central_latitude=72.0))\n",
    "        ax2.set_extent([16,60,60,82],ccrs.PlateCarree()) # W:18 E:60 S:64 N:80    \n",
    "        ax2.set_aspect('1')\n",
    "        ax2.coastlines()\n",
    "        gl = ax2.gridlines(linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "        # assemble cube iris\n",
    "        cube_iris = iris.cube.Cube(sic_select_norm[i,:,:], long_name='geographical field', var_name='field', \n",
    "                                   units='1', dim_coords_and_dims=[(lat_iris, 0), (lon_iris, 1)])\n",
    "        # other set-ups\n",
    "        ticks = [i for i in np.linspace(0.0,1.0,11)]\n",
    "        label = 'Obs SIC'\n",
    "        # make plots       \n",
    "        cs = iplt.contourf(cube_iris, cmap='Blues',levels=ticks, extend='both', vmin=ticks[0], vmax=ticks[-1])\n",
    "        cbar = fig.colorbar(cs,extend='both', orientation='horizontal',\n",
    "                            shrink =0.8, pad=0.05)#, format=\"%.1f\")\n",
    "        cbar.set_label(label,size = 12)\n",
    "        cbar.set_ticks(ticks)\n",
    "        cbar.ax.tick_params(labelsize = 9)\n",
    "\n",
    "        # submap 3 - configure map\n",
    "        ax3 = plt.subplot(2, 2, 3, projection=ccrs.EquidistantConic(central_longitude=39.0, central_latitude=72.0))\n",
    "        ax3.set_extent([16,60,60,82],ccrs.PlateCarree()) # W:18 E:60 S:64 N:80    \n",
    "        ax3.set_aspect('1')\n",
    "        ax3.coastlines()\n",
    "        gl = ax3.gridlines(linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "        # assemble cube iris\n",
    "        cube_iris = iris.cube.Cube(lead_pred_sic[i,0,:,:] - sic_select_norm[i,:,:], long_name='geographical field', var_name='field', \n",
    "                                   units='1', dim_coords_and_dims=[(lat_iris, 0), (lon_iris, 1)])\n",
    "        # other set-ups\n",
    "        ticks = [i for i in np.linspace(-0.2,0.2,11)]\n",
    "        label = 'Pred - Obs (SIC)'\n",
    "        # make plots\n",
    "        cs = iplt.contourf(cube_iris, cmap='coolwarm',levels=ticks, extend='both', vmin=ticks[0], vmax=ticks[-1])\n",
    "        cbar = fig.colorbar(cs,extend='both', orientation='horizontal',\n",
    "                            shrink =0.8, pad=0.05)#, format=\"%.1f\")\n",
    "        cbar.set_label(label,size = 12)\n",
    "        cbar.set_ticks(ticks)\n",
    "        cbar.ax.tick_params(labelsize = 9)\n",
    "\n",
    "        # submap 4 - configure map\n",
    "        ax4 = plt.subplot(2, 2, 4, projection=ccrs.EquidistantConic(central_longitude=39.0, central_latitude=72.0))\n",
    "        ax4.set_extent([16,60,60,82],ccrs.PlateCarree()) # W:18 E:60 S:64 N:80    \n",
    "        ax4.set_aspect('1')\n",
    "        ax4.coastlines()\n",
    "        gl = ax4.gridlines(linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "        # assemble cube iris\n",
    "        cube_iris = iris.cube.Cube(np.abs(lead_pred_sic[i,0,:,:] - sic_select_norm[i,:,:]) - np.abs(sic_persist_norm[i,:,:] - sic_select_norm[i,:,:]),\n",
    "                                   long_name='geographical field', var_name='field', \n",
    "                                   units='1', dim_coords_and_dims=[(lat_iris, 0), (lon_iris, 1)])\n",
    "        # other set-ups\n",
    "        ticks = [i for i in np.linspace(-0.1,0.1,11)]\n",
    "        label = 'Pred - Persist (Absolute Error)'\n",
    "        # make plots\n",
    "        cs = iplt.contourf(cube_iris, cmap='coolwarm',levels=ticks, extend='both', vmin=ticks[0], vmax=ticks[-1])\n",
    "        cbar = fig.colorbar(cs,extend='both', orientation='horizontal',\n",
    "                            shrink =0.8, pad=0.05)#, format=\"%.1f\")\n",
    "        cbar.set_label(label,size = 12)\n",
    "        cbar.set_ticks(ticks)\n",
    "        cbar.ax.tick_params(labelsize = 9)\n",
    "\n",
    "        # adjust location of subplots\n",
    "        plt.subplots_adjust(left= 0.05, right= 0.95, bottom=0.05, top=0.95, wspace = 0.02, hspace = 0.05)\n",
    "        \n",
    "        fig.savefig(os.path.join(output_path,'sic_pred_obs_anime_week_{}.png'.format(i)), dpi=200)\n",
    "        iplt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "step 1\n",
      "step 2\n",
      "step 3\n",
      "step 4\n",
      "step 5\n",
      "step 6\n",
      "step 7\n",
      "step 8\n",
      "step 9\n",
      "step 10\n",
      "step 11\n",
      "step 12\n",
      "step 13\n",
      "step 14\n",
      "step 15\n",
      "step 16\n",
      "step 17\n",
      "step 18\n",
      "step 19\n",
      "step 20\n",
      "step 21\n",
      "step 22\n",
      "step 23\n",
      "step 24\n",
      "step 25\n",
      "step 26\n",
      "step 27\n",
      "step 28\n",
      "step 29\n",
      "step 30\n",
      "step 31\n",
      "step 32\n",
      "step 33\n",
      "step 34\n",
      "step 35\n",
      "step 36\n",
      "step 37\n",
      "step 38\n",
      "step 39\n",
      "step 40\n",
      "step 41\n",
      "step 42\n",
      "step 43\n",
      "step 44\n",
      "step 45\n",
      "step 46\n",
      "step 47\n",
      "step 48\n",
      "step 49\n",
      "step 50\n",
      "step 51\n",
      "step 52\n",
      "step 53\n",
      "step 54\n",
      "step 55\n",
      "step 56\n",
      "step 57\n",
      "step 58\n",
      "step 59\n",
      "step 60\n",
      "step 61\n",
      "step 62\n",
      "step 63\n",
      "step 64\n",
      "step 65\n",
      "step 66\n",
      "step 67\n",
      "step 68\n",
      "step 69\n",
      "step 70\n",
      "step 71\n",
      "step 72\n",
      "step 73\n",
      "step 74\n",
      "step 75\n",
      "step 76\n",
      "step 77\n",
      "step 78\n",
      "step 79\n",
      "step 80\n",
      "step 81\n",
      "step 82\n",
      "step 83\n",
      "step 84\n",
      "step 85\n",
      "step 86\n",
      "step 87\n",
      "step 88\n",
      "step 89\n",
      "step 90\n",
      "step 91\n",
      "step 92\n",
      "step 93\n",
      "step 94\n",
      "step 95\n",
      "step 96\n",
      "step 97\n",
      "step 98\n",
      "step 99\n",
      "step 100\n",
      "step 101\n",
      "step 102\n",
      "step 103\n",
      "step 104\n",
      "step 105\n",
      "step 106\n",
      "step 107\n",
      "step 108\n",
      "step 109\n",
      "step 110\n",
      "step 111\n",
      "step 112\n",
      "step 113\n",
      "step 114\n",
      "step 115\n",
      "step 116\n",
      "step 117\n",
      "step 118\n",
      "step 119\n",
      "step 120\n",
      "step 121\n",
      "step 122\n",
      "step 123\n",
      "step 124\n",
      "step 125\n",
      "step 126\n",
      "step 127\n",
      "step 128\n",
      "step 129\n",
      "step 130\n",
      "step 131\n",
      "step 132\n",
      "step 133\n",
      "step 134\n",
      "step 135\n",
      "step 136\n",
      "step 137\n",
      "step 138\n",
      "step 139\n",
      "step 140\n",
      "step 141\n",
      "step 142\n",
      "step 143\n",
      "step 144\n",
      "step 145\n",
      "step 146\n",
      "step 147\n",
      "step 148\n",
      "step 149\n",
      "step 150\n",
      "step 151\n",
      "step 152\n",
      "step 153\n",
      "step 154\n",
      "step 155\n",
      "step 156\n",
      "step 157\n",
      "step 158\n",
      "step 159\n",
      "step 160\n",
      "step 161\n",
      "step 162\n",
      "step 163\n",
      "step 164\n",
      "step 165\n",
      "step 166\n",
      "step 167\n",
      "step 168\n",
      "step 169\n",
      "step 170\n",
      "step 171\n",
      "step 172\n",
      "step 173\n",
      "step 174\n",
      "step 175\n",
      "step 176\n",
      "step 177\n",
      "step 178\n",
      "step 179\n",
      "step 180\n",
      "step 181\n",
      "step 182\n",
      "step 183\n",
      "step 184\n",
      "step 185\n",
      "step 186\n",
      "step 187\n",
      "step 188\n",
      "step 189\n",
      "step 190\n",
      "step 191\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                       Animation Generator                       ########\n",
    "    #################################################################################\n",
    "    #For long movie (save memory)\n",
    "    test_year = 4\n",
    "    index_plot = np.arange(test_year*12*4)\n",
    "    # output path\n",
    "    #output_path = 'C:\\\\Users\\\\nosta\\\\ML4Climate\\\\PredictArctic\\\\Maps\\\\Barents\\\\Anime'\n",
    "    # quantizer determines the quality of gif, there are options like 'nq' 'wu'\n",
    "    with imageio.get_writer(os.path.join(output_path, 'ConvLSTM_sic.gif'), mode='I',duration=0.3,quantizer='wu') as writer:\n",
    "    #with imageio.get_writer(os.path.join(output_path, 'ConvLSTM_sic.gif'), mode='I',duration=0.5) as writer:\n",
    "        for i in index_plot:\n",
    "            print(\"step {}\".format(i))\n",
    "            images = imageio.imread(os.path.join(output_path,'sic_pred_obs_anime_week_{}.png'.format(i)))\n",
    "            writer.append_data(images[:])\n",
    "    writer.close()\n",
    "    # for short gif\n",
    "#     images = []\n",
    "#     for i in index_plot:\n",
    "#         images.append(imageio.imread(os.path.join(output_path,'sic_pred_obs_anime_week_{}.png'.format(i))))\n",
    "#     imageio.mimwrite(os.path.join(output_path, 'ConvLSTM_sic.gif'), images[:], duration=0.3,quantizer='nq')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
