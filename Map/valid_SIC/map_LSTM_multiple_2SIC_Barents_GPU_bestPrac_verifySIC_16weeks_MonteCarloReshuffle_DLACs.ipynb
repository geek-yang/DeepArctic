{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Monte Carlo Reshuffling test on the predictors for ConvLSTM forecast** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2020.07.22 ** <br>\n",
    "** Last Update  : 2020.07.22 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, deepclim, matplotlib **<br>\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. We also include many climate index (to represent the forcing from atmosphere). The convolutional Long Short Time Memory neural network is used to deal with this spatial-temporal sequence problem. We use Pytorch as the deep learning framework. <br>\n",
    "\n",
    "In order to test the signal to noise ratio of many predictors, we apply Monte Carlo reshuffling of all the predictors except for the sea ice.\n",
    "<br>\n",
    "** Here we predict sea ice concentration with one extra relevant field from either ocean or atmosphere to test the predictor.** <br>\n",
    "\n",
    "Return Values   : Time series and figures <br>\n",
    "\n",
    "http://research.jisao.washington.edu/pdo/PDO.latest<br>\n",
    "\n",
    "The regionalization adopted here follows that of the MASIE (Multisensor Analyzed Sea Ice Extent) product available from the National Snow and Ice Data Center:<br>\n",
    "https://nsidc.org/data/masie/browse_regions<br>\n",
    "It is given by paper J.Walsh et. al., 2019. Benchmark seasonal prediction skill estimates based on regional indices.<br>\n",
    "\n",
    "The method comes from the study by Shi et. al. (2015) Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import numbers\n",
    "\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "#sys.path.append(os.path.join('C:','Users','nosta','ML4Climate','Scripts','DeepClim'))\n",
    "#sys.path.append(\"C:\\\\Users\\\\nosta\\\\ML4Climate\\\\Scripts\\\\DLACs\")\n",
    "sys.path.append(\"/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/Scripts/DLACs\")\n",
    "import dlacs\n",
    "import dlacs.ConvLSTM\n",
    "import dlacs.preprocess\n",
    "import dlacs.function\n",
    "import dlacs.saveNetCDF\n",
    "\n",
    "# for visualization\n",
    "import dlacs.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import iris # also helps with regriding\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing device is Dell Inspirion 5680 with Intel Core i7-8700 x64 CPU and Nvidia GTX 1060 6GB GPU.<br>\n",
    "Here is a benchmark about cpu v.s. gtx 1060 <br>\n",
    "https://www.analyticsindiamag.com/deep-learning-tensorflow-benchmark-intel-i5-4210u-vs-geforce-nvidia-1060-6gb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2264670,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data ** <br>\n",
    "Time span of each product included: <br>\n",
    "** Reanalysis ** <br>\n",
    "- **ERA-Interim** 1979 - 2016 (ECMWF)\n",
    "- **ORAS4**       1958 - 2014 (ECMWF)\n",
    "\n",
    "** Index ** <br>\n",
    "- **NINO3.4**     1950 - 2017 (NOAA)\n",
    "- **AO**          1950 - 2017 (NOAA)\n",
    "- **NAO**         1950 - 2017 (NOAA)\n",
    "- **AMO**         1950 - 2017 (NOAA)\n",
    "- **PDO**         1950 - 2017 (University of Washington)\n",
    "\n",
    "!! These index are given by NCEP/NCAR Reanalysis (CDAS) <br>\n",
    "\n",
    "\n",
    "Alternative (not in use yet) <br>\n",
    "** Reanalysis ** <br>\n",
    "- **MERRA2**      1980 - 2016 (NASA)\n",
    "- **JRA55**       1979 - 2015 (JMA)\n",
    "- **GLORYS2V3**   1993 - 2014 (Mercartor Ocean)\n",
    "- **SODA3**       1980 - 2015\n",
    "- **PIOMASS**     1980 - 2015\n",
    "\n",
    "** Observations ** <br>\n",
    "- **NSIDC**       1958 - 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# \n",
    "#########                           datapath                             ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ERA-Interim'\n",
    "#datapath_ERAI = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ERA-Interim'\n",
    "datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4 = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/ORAS4'\n",
    "#datapath_ORAS4_mask = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "#datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "#datapath_PIOMASS = 'H:\\\\Creator_Zone\\\\Core_Database_AMET_OMET_reanalysis\\\\PIOMASS'\n",
    "#datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "#datapath_clim_index = 'F:\\\\PhD_essential\\\\Core_Database_AMET_OMET_reanalysis\\\\Climate_index'\n",
    "output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/Maps'\n",
    "#output_path = 'C:\\\\Users\\\\nosta\\\\ML4Climate\\\\PredictArctic\\\\Maps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** get the key to the datasets *************************\n",
      "*********************** extract variables *************************\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    # weekly variables on ERAI grid\n",
    "    dataset_ERAI_fields_sic = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'sic_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_slp = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'slp_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_t2m = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      't2m_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_z500 = Dataset(os.path.join(datapath_ERAI,\n",
    "                                       'z500_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_z850 = Dataset(os.path.join(datapath_ERAI,\n",
    "                                       'z850_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_uv10m = Dataset(os.path.join(datapath_ERAI,\n",
    "                                       'uv10m_weekly_erai_1979_2017.nc'))\n",
    "    dataset_ERAI_fields_rad = Dataset(os.path.join(datapath_ERAI,\n",
    "                                        'rad_flux_weekly_erai_1979_2017.nc'))\n",
    "    #dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "    #                             'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    # OHC interpolated on ERA-Interim grid\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                'ohc_monthly_oras2erai_1978_2017.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    #################################################################################\n",
    "    #########                        data gallery                           #########\n",
    "    #################################################################################\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2013\n",
    "    # validation: 2014 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim) - benckmark\n",
    "    SIC_ERAI = dataset_ERAI_fields_sic.variables['sic'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI = dataset_ERAI_fields_sic.variables['year'][:-1]\n",
    "    week_ERAI = dataset_ERAI_fields_sic.variables['week'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_sic.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_sic.variables['longitude'][:]\n",
    "    # T2M (ERA-Interim)\n",
    "    T2M_ERAI = dataset_ERAI_fields_t2m.variables['t2m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_t2m = dataset_ERAI_fields_t2m.variables['year'][:-1]\n",
    "    week_ERAI_t2m = dataset_ERAI_fields_t2m.variables['week'][:]\n",
    "    latitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['latitude'][:]\n",
    "    longitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['longitude'][:]\n",
    "    # SLP (ERA-Interim)\n",
    "    SLP_ERAI = dataset_ERAI_fields_slp.variables['slp'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_slp = dataset_ERAI_fields_slp.variables['year'][:-1]\n",
    "    week_ERAI_slp = dataset_ERAI_fields_slp.variables['week'][:]\n",
    "    latitude_ERAI_slp = dataset_ERAI_fields_slp.variables['latitude'][:]\n",
    "    longitude_ERAI_slp = dataset_ERAI_fields_slp.variables['longitude'][:]\n",
    "    # Z500 (ERA-Interim)\n",
    "    Z500_ERAI = dataset_ERAI_fields_z500.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_z500 = dataset_ERAI_fields_z500.variables['year'][:-1]\n",
    "    week_ERAI_z500 = dataset_ERAI_fields_z500.variables['week'][:]\n",
    "    latitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['latitude'][:]\n",
    "    longitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['longitude'][:]\n",
    "    # Z850 (ERA-Interim)\n",
    "    Z850_ERAI = dataset_ERAI_fields_z850.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_z850 = dataset_ERAI_fields_z850.variables['year'][:-1]\n",
    "    week_ERAI_z850 = dataset_ERAI_fields_z850.variables['week'][:]\n",
    "    latitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['latitude'][:]\n",
    "    longitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['longitude'][:]\n",
    "    # UV10M (ERA-Interim)\n",
    "    U10M_ERAI = dataset_ERAI_fields_uv10m.variables['u10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    V10M_ERAI = dataset_ERAI_fields_uv10m.variables['v10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['year'][:-1]\n",
    "    week_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['week'][:]\n",
    "    latitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['latitude'][:]\n",
    "    longitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['longitude'][:]\n",
    "    # SFlux (ERA-Interim)\n",
    "    SFlux_ERAI = dataset_ERAI_fields_rad.variables['SFlux'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI_SFlux = dataset_ERAI_fields_rad.variables['year'][:-1]\n",
    "    week_ERAI_SFlux = dataset_ERAI_fields_rad.variables['week'][:]\n",
    "    latitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['latitude'][:]\n",
    "    longitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    #SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    #year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # OHC (ORAS4)\n",
    "    # from 1978 - 2017 (for interpolation) / from 90 N upto 40 N\n",
    "    OHC_300_ORAS4 = dataset_ORAS4_OHC.variables['OHC'][:-1,:,:67,:]/1000 # unit Peta Joule\n",
    "    latitude_ORAS4 = dataset_ORAS4_OHC.variables['latitude'][:]\n",
    "    longitude_ORAS4 = dataset_ORAS4_OHC.variables['longitude'][:]\n",
    "    mask_OHC = np.ma.getmask(OHC_300_ORAS4[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.5  88.75 88.   87.25 86.5  85.75 85.   84.25 83.5  82.75 82.   81.25\n",
      " 80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25\n",
      " 62.5  61.75 61.   60.25 59.5  58.75 58.   57.25 56.5  55.75 55.   54.25\n",
      " 53.5  52.75 52.   51.25 50.5  49.75 49.   48.25 47.5  46.75 46.   45.25\n",
      " 44.5  43.75 43.   42.25 41.5  40.75 40.  ]\n",
      "[-180.   -179.25 -178.5  -177.75 -177.   -176.25 -175.5  -174.75 -174.\n",
      " -173.25 -172.5  -171.75 -171.   -170.25 -169.5  -168.75 -168.   -167.25\n",
      " -166.5  -165.75 -165.   -164.25 -163.5  -162.75 -162.   -161.25 -160.5\n",
      " -159.75 -159.   -158.25 -157.5  -156.75 -156.   -155.25 -154.5  -153.75\n",
      " -153.   -152.25 -151.5  -150.75 -150.   -149.25 -148.5  -147.75 -147.\n",
      " -146.25 -145.5  -144.75 -144.   -143.25 -142.5  -141.75 -141.   -140.25\n",
      " -139.5  -138.75 -138.   -137.25 -136.5  -135.75 -135.   -134.25 -133.5\n",
      " -132.75 -132.   -131.25 -130.5  -129.75 -129.   -128.25 -127.5  -126.75\n",
      " -126.   -125.25 -124.5  -123.75 -123.   -122.25 -121.5  -120.75 -120.\n",
      " -119.25 -118.5  -117.75 -117.   -116.25 -115.5  -114.75 -114.   -113.25\n",
      " -112.5  -111.75 -111.   -110.25 -109.5  -108.75 -108.   -107.25 -106.5\n",
      " -105.75 -105.   -104.25 -103.5  -102.75 -102.   -101.25 -100.5   -99.75\n",
      "  -99.    -98.25  -97.5   -96.75  -96.    -95.25  -94.5   -93.75  -93.\n",
      "  -92.25  -91.5   -90.75  -90.    -89.25  -88.5   -87.75  -87.    -86.25\n",
      "  -85.5   -84.75  -84.    -83.25  -82.5   -81.75  -81.    -80.25  -79.5\n",
      "  -78.75  -78.    -77.25  -76.5   -75.75  -75.    -74.25  -73.5   -72.75\n",
      "  -72.    -71.25  -70.5   -69.75  -69.    -68.25  -67.5   -66.75  -66.\n",
      "  -65.25  -64.5   -63.75  -63.    -62.25  -61.5   -60.75  -60.    -59.25\n",
      "  -58.5   -57.75  -57.    -56.25  -55.5   -54.75  -54.    -53.25  -52.5\n",
      "  -51.75  -51.    -50.25  -49.5   -48.75  -48.    -47.25  -46.5   -45.75\n",
      "  -45.    -44.25  -43.5   -42.75  -42.    -41.25  -40.5   -39.75  -39.\n",
      "  -38.25  -37.5   -36.75  -36.    -35.25  -34.5   -33.75  -33.    -32.25\n",
      "  -31.5   -30.75  -30.    -29.25  -28.5   -27.75  -27.    -26.25  -25.5\n",
      "  -24.75  -24.    -23.25  -22.5   -21.75  -21.    -20.25  -19.5   -18.75\n",
      "  -18.    -17.25  -16.5   -15.75  -15.    -14.25  -13.5   -12.75  -12.\n",
      "  -11.25  -10.5    -9.75   -9.     -8.25   -7.5    -6.75   -6.     -5.25\n",
      "   -4.5    -3.75   -3.     -2.25   -1.5    -0.75    0.      0.75    1.5\n",
      "    2.25    3.      3.75    4.5     5.25    6.      6.75    7.5     8.25\n",
      "    9.      9.75   10.5    11.25   12.     12.75   13.5    14.25   15.\n",
      "   15.75   16.5    17.25   18.     18.75   19.5    20.25   21.     21.75\n",
      "   22.5    23.25   24.     24.75   25.5    26.25   27.     27.75   28.5\n",
      "   29.25   30.     30.75   31.5    32.25   33.     33.75   34.5    35.25\n",
      "   36.     36.75   37.5    38.25   39.     39.75   40.5    41.25   42.\n",
      "   42.75   43.5    44.25   45.     45.75   46.5    47.25   48.     48.75\n",
      "   49.5    50.25   51.     51.75   52.5    53.25   54.     54.75   55.5\n",
      "   56.25   57.     57.75   58.5    59.25   60.     60.75   61.5    62.25\n",
      "   63.     63.75   64.5    65.25   66.     66.75   67.5    68.25   69.\n",
      "   69.75   70.5    71.25   72.     72.75   73.5    74.25   75.     75.75\n",
      "   76.5    77.25   78.     78.75   79.5    80.25   81.     81.75   82.5\n",
      "   83.25   84.     84.75   85.5    86.25   87.     87.75   88.5    89.25\n",
      "   90.     90.75   91.5    92.25   93.     93.75   94.5    95.25   96.\n",
      "   96.75   97.5    98.25   99.     99.75  100.5   101.25  102.    102.75\n",
      "  103.5   104.25  105.    105.75  106.5   107.25  108.    108.75  109.5\n",
      "  110.25  111.    111.75  112.5   113.25  114.    114.75  115.5   116.25\n",
      "  117.    117.75  118.5   119.25  120.    120.75  121.5   122.25  123.\n",
      "  123.75  124.5   125.25  126.    126.75  127.5   128.25  129.    129.75\n",
      "  130.5   131.25  132.    132.75  133.5   134.25  135.    135.75  136.5\n",
      "  137.25  138.    138.75  139.5   140.25  141.    141.75  142.5   143.25\n",
      "  144.    144.75  145.5   146.25  147.    147.75  148.5   149.25  150.\n",
      "  150.75  151.5   152.25  153.    153.75  154.5   155.25  156.    156.75\n",
      "  157.5   158.25  159.    159.75  160.5   161.25  162.    162.75  163.5\n",
      "  164.25  165.    165.75  166.5   167.25  168.    168.75  169.5   170.25\n",
      "  171.    171.75  172.5   173.25  174.    174.75  175.5   176.25  177.\n",
      "  177.75  178.5   179.25]\n",
      "[89.5  88.75 88.   87.25 86.5  85.75 85.   84.25 83.5  82.75 82.   81.25\n",
      " 80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25\n",
      " 62.5  61.75 61.   60.25 59.5  58.75 58.   57.25 56.5  55.75 55.   54.25\n",
      " 53.5  52.75 52.   51.25 50.5  49.75 49.   48.25 47.5  46.75 46.   45.25\n",
      " 44.5  43.75 43.   42.25 41.5  40.75 40.  ]\n",
      "[-180.   -179.25 -178.5  -177.75 -177.   -176.25 -175.5  -174.75 -174.\n",
      " -173.25 -172.5  -171.75 -171.   -170.25 -169.5  -168.75 -168.   -167.25\n",
      " -166.5  -165.75 -165.   -164.25 -163.5  -162.75 -162.   -161.25 -160.5\n",
      " -159.75 -159.   -158.25 -157.5  -156.75 -156.   -155.25 -154.5  -153.75\n",
      " -153.   -152.25 -151.5  -150.75 -150.   -149.25 -148.5  -147.75 -147.\n",
      " -146.25 -145.5  -144.75 -144.   -143.25 -142.5  -141.75 -141.   -140.25\n",
      " -139.5  -138.75 -138.   -137.25 -136.5  -135.75 -135.   -134.25 -133.5\n",
      " -132.75 -132.   -131.25 -130.5  -129.75 -129.   -128.25 -127.5  -126.75\n",
      " -126.   -125.25 -124.5  -123.75 -123.   -122.25 -121.5  -120.75 -120.\n",
      " -119.25 -118.5  -117.75 -117.   -116.25 -115.5  -114.75 -114.   -113.25\n",
      " -112.5  -111.75 -111.   -110.25 -109.5  -108.75 -108.   -107.25 -106.5\n",
      " -105.75 -105.   -104.25 -103.5  -102.75 -102.   -101.25 -100.5   -99.75\n",
      "  -99.    -98.25  -97.5   -96.75  -96.    -95.25  -94.5   -93.75  -93.\n",
      "  -92.25  -91.5   -90.75  -90.    -89.25  -88.5   -87.75  -87.    -86.25\n",
      "  -85.5   -84.75  -84.    -83.25  -82.5   -81.75  -81.    -80.25  -79.5\n",
      "  -78.75  -78.    -77.25  -76.5   -75.75  -75.    -74.25  -73.5   -72.75\n",
      "  -72.    -71.25  -70.5   -69.75  -69.    -68.25  -67.5   -66.75  -66.\n",
      "  -65.25  -64.5   -63.75  -63.    -62.25  -61.5   -60.75  -60.    -59.25\n",
      "  -58.5   -57.75  -57.    -56.25  -55.5   -54.75  -54.    -53.25  -52.5\n",
      "  -51.75  -51.    -50.25  -49.5   -48.75  -48.    -47.25  -46.5   -45.75\n",
      "  -45.    -44.25  -43.5   -42.75  -42.    -41.25  -40.5   -39.75  -39.\n",
      "  -38.25  -37.5   -36.75  -36.    -35.25  -34.5   -33.75  -33.    -32.25\n",
      "  -31.5   -30.75  -30.    -29.25  -28.5   -27.75  -27.    -26.25  -25.5\n",
      "  -24.75  -24.    -23.25  -22.5   -21.75  -21.    -20.25  -19.5   -18.75\n",
      "  -18.    -17.25  -16.5   -15.75  -15.    -14.25  -13.5   -12.75  -12.\n",
      "  -11.25  -10.5    -9.75   -9.     -8.25   -7.5    -6.75   -6.     -5.25\n",
      "   -4.5    -3.75   -3.     -2.25   -1.5    -0.75    0.      0.75    1.5\n",
      "    2.25    3.      3.75    4.5     5.25    6.      6.75    7.5     8.25\n",
      "    9.      9.75   10.5    11.25   12.     12.75   13.5    14.25   15.\n",
      "   15.75   16.5    17.25   18.     18.75   19.5    20.25   21.     21.75\n",
      "   22.5    23.25   24.     24.75   25.5    26.25   27.     27.75   28.5\n",
      "   29.25   30.     30.75   31.5    32.25   33.     33.75   34.5    35.25\n",
      "   36.     36.75   37.5    38.25   39.     39.75   40.5    41.25   42.\n",
      "   42.75   43.5    44.25   45.     45.75   46.5    47.25   48.     48.75\n",
      "   49.5    50.25   51.     51.75   52.5    53.25   54.     54.75   55.5\n",
      "   56.25   57.     57.75   58.5    59.25   60.     60.75   61.5    62.25\n",
      "   63.     63.75   64.5    65.25   66.     66.75   67.5    68.25   69.\n",
      "   69.75   70.5    71.25   72.     72.75   73.5    74.25   75.     75.75\n",
      "   76.5    77.25   78.     78.75   79.5    80.25   81.     81.75   82.5\n",
      "   83.25   84.     84.75   85.5    86.25   87.     87.75   88.5    89.25\n",
      "   90.     90.75   91.5    92.25   93.     93.75   94.5    95.25   96.\n",
      "   96.75   97.5    98.25   99.     99.75  100.5   101.25  102.    102.75\n",
      "  103.5   104.25  105.    105.75  106.5   107.25  108.    108.75  109.5\n",
      "  110.25  111.    111.75  112.5   113.25  114.    114.75  115.5   116.25\n",
      "  117.    117.75  118.5   119.25  120.    120.75  121.5   122.25  123.\n",
      "  123.75  124.5   125.25  126.    126.75  127.5   128.25  129.    129.75\n",
      "  130.5   131.25  132.    132.75  133.5   134.25  135.    135.75  136.5\n",
      "  137.25  138.    138.75  139.5   140.25  141.    141.75  142.5   143.25\n",
      "  144.    144.75  145.5   146.25  147.    147.75  148.5   149.25  150.\n",
      "  150.75  151.5   152.25  153.    153.75  154.5   155.25  156.    156.75\n",
      "  157.5   158.25  159.    159.75  160.5   161.25  162.    162.75  163.5\n",
      "  164.25  165.    165.75  166.5   167.25  168.    168.75  169.5   170.25\n",
      "  171.    171.75  172.5   173.25  174.    174.75  175.5   176.25  177.\n",
      "  177.75  178.5   179.25]\n",
      "[-180.   -179.25 -178.5  -177.75 -177.   -176.25 -175.5  -174.75 -174.\n",
      " -173.25 -172.5  -171.75 -171.   -170.25 -169.5  -168.75 -168.   -167.25\n",
      " -166.5  -165.75 -165.   -164.25 -163.5  -162.75 -162.   -161.25 -160.5\n",
      " -159.75 -159.   -158.25 -157.5  -156.75 -156.   -155.25 -154.5  -153.75\n",
      " -153.   -152.25 -151.5  -150.75 -150.   -149.25 -148.5  -147.75 -147.\n",
      " -146.25 -145.5  -144.75 -144.   -143.25 -142.5  -141.75 -141.   -140.25\n",
      " -139.5  -138.75 -138.   -137.25 -136.5  -135.75 -135.   -134.25 -133.5\n",
      " -132.75 -132.   -131.25 -130.5  -129.75 -129.   -128.25 -127.5  -126.75\n",
      " -126.   -125.25 -124.5  -123.75 -123.   -122.25 -121.5  -120.75 -120.\n",
      " -119.25 -118.5  -117.75 -117.   -116.25 -115.5  -114.75 -114.   -113.25\n",
      " -112.5  -111.75 -111.   -110.25 -109.5  -108.75 -108.   -107.25 -106.5\n",
      " -105.75 -105.   -104.25 -103.5  -102.75 -102.   -101.25 -100.5   -99.75\n",
      "  -99.    -98.25  -97.5   -96.75  -96.    -95.25  -94.5   -93.75  -93.\n",
      "  -92.25  -91.5   -90.75  -90.    -89.25  -88.5   -87.75  -87.    -86.25\n",
      "  -85.5   -84.75  -84.    -83.25  -82.5   -81.75  -81.    -80.25  -79.5\n",
      "  -78.75  -78.    -77.25  -76.5   -75.75  -75.    -74.25  -73.5   -72.75\n",
      "  -72.    -71.25  -70.5   -69.75  -69.    -68.25  -67.5   -66.75  -66.\n",
      "  -65.25  -64.5   -63.75  -63.    -62.25  -61.5   -60.75  -60.    -59.25\n",
      "  -58.5   -57.75  -57.    -56.25  -55.5   -54.75  -54.    -53.25  -52.5\n",
      "  -51.75  -51.    -50.25  -49.5   -48.75  -48.    -47.25  -46.5   -45.75\n",
      "  -45.    -44.25  -43.5   -42.75  -42.    -41.25  -40.5   -39.75  -39.\n",
      "  -38.25  -37.5   -36.75  -36.    -35.25  -34.5   -33.75  -33.    -32.25\n",
      "  -31.5   -30.75  -30.    -29.25  -28.5   -27.75  -27.    -26.25  -25.5\n",
      "  -24.75  -24.    -23.25  -22.5   -21.75  -21.    -20.25  -19.5   -18.75\n",
      "  -18.    -17.25  -16.5   -15.75  -15.    -14.25  -13.5   -12.75  -12.\n",
      "  -11.25  -10.5    -9.75   -9.     -8.25   -7.5    -6.75   -6.     -5.25\n",
      "   -4.5    -3.75   -3.     -2.25   -1.5    -0.75    0.      0.75    1.5\n",
      "    2.25    3.      3.75    4.5     5.25    6.      6.75    7.5     8.25\n",
      "    9.      9.75   10.5    11.25   12.     12.75   13.5    14.25   15.\n",
      "   15.75   16.5    17.25   18.     18.75   19.5    20.25   21.     21.75\n",
      "   22.5    23.25   24.     24.75   25.5    26.25   27.     27.75   28.5\n",
      "   29.25   30.     30.75   31.5    32.25   33.     33.75   34.5    35.25\n",
      "   36.     36.75   37.5    38.25   39.     39.75   40.5    41.25   42.\n",
      "   42.75   43.5    44.25   45.     45.75   46.5    47.25   48.     48.75\n",
      "   49.5    50.25   51.     51.75   52.5    53.25   54.     54.75   55.5\n",
      "   56.25   57.     57.75   58.5    59.25   60.     60.75   61.5    62.25\n",
      "   63.     63.75   64.5    65.25   66.     66.75   67.5    68.25   69.\n",
      "   69.75   70.5    71.25   72.     72.75   73.5    74.25   75.     75.75\n",
      "   76.5    77.25   78.     78.75   79.5    80.25   81.     81.75   82.5\n",
      "   83.25   84.     84.75   85.5    86.25   87.     87.75   88.5    89.25\n",
      "   90.     90.75   91.5    92.25   93.     93.75   94.5    95.25   96.\n",
      "   96.75   97.5    98.25   99.     99.75  100.5   101.25  102.    102.75\n",
      "  103.5   104.25  105.    105.75  106.5   107.25  108.    108.75  109.5\n",
      "  110.25  111.    111.75  112.5   113.25  114.    114.75  115.5   116.25\n",
      "  117.    117.75  118.5   119.25  120.    120.75  121.5   122.25  123.\n",
      "  123.75  124.5   125.25  126.    126.75  127.5   128.25  129.    129.75\n",
      "  130.5   131.25  132.    132.75  133.5   134.25  135.    135.75  136.5\n",
      "  137.25  138.    138.75  139.5   140.25  141.    141.75  142.5   143.25\n",
      "  144.    144.75  145.5   146.25  147.    147.75  148.5   149.25  150.\n",
      "  150.75  151.5   152.25  153.    153.75  154.5   155.25  156.    156.75\n",
      "  157.5   158.25  159.    159.75  160.5   161.25  162.    162.75  163.5\n",
      "  164.25  165.    165.75  166.5   167.25  168.    168.75  169.5   170.25\n",
      "  171.    171.75  172.5   173.25  174.    174.75  175.5   176.25  177.\n",
      "  177.75  178.5   179.25]\n"
     ]
    }
   ],
   "source": [
    "    # first check of grid\n",
    "    print(latitude_ERAI)\n",
    "    print(longitude_ERAI)\n",
    "    print(latitude_ERAI_t2m)\n",
    "    print(longitude_ERAI_t2m)\n",
    "    print(longitude_ORAS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** create mask *************************\n",
      "*********************** calc mask *************************\n",
      "*********************** packing *************************\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ###########                 global land-sea mask                      ###########\n",
    "    #################################################################################\n",
    "    sea_ice_mask_global = np.ones((len(latitude_ERAI),len(longitude_ERAI)),dtype=float)\n",
    "    sea_ice_mask_global[SIC_ERAI[0,0,:,:]==-1] = 0\n",
    "    #################################################################################\n",
    "    ###########                regionalization sea mask                   ###########\n",
    "    #################################################################################\n",
    "    print ('*********************** create mask *************************')\n",
    "    # W:-156 E:-124 N:80 S:67\n",
    "    mask_Beaufort = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:-156 N:80 S:66\n",
    "    mask_Chukchi = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:146 E:180 N:80 S:67\n",
    "    mask_EastSiberian = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:100 E:146 N:80 S:67\n",
    "    mask_Laptev = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:60 E:100 N:80 S:67\n",
    "    mask_Kara = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:18 E:60 N:80 S:64\n",
    "    mask_Barents = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-44 E:18 N:80 S:55\n",
    "    mask_Greenland = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:180 N:90 S:80\n",
    "    mask_CenArctic = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    print ('*********************** calc mask *************************')\n",
    "    mask_Beaufort[13:31,32:76] = 1\n",
    "\n",
    "    mask_Chukchi[13:32,0:32] = 1\n",
    "    mask_Chukchi[13:32,-1] = 1\n",
    "\n",
    "    mask_EastSiberian[13:31,434:479] = 1\n",
    "\n",
    "    mask_Laptev[13:31,374:434] = 1\n",
    "\n",
    "    mask_Kara[13:31,320:374] = 1\n",
    "\n",
    "    mask_Barents[13:36,264:320] = 1\n",
    "\n",
    "    mask_Greenland[13:47,179:264] = 1\n",
    "    mask_Greenland[26:47,240:264] = 0\n",
    "\n",
    "    mask_CenArctic[:13,:] = 1\n",
    "    print ('*********************** packing *************************')\n",
    "    mask_dict = {'Beaufort': mask_Beaufort[:,:],\n",
    "                 'Chukchi': mask_Chukchi[:,:],\n",
    "                 'EastSiberian': mask_EastSiberian[:,:],\n",
    "                 'Laptev': mask_Laptev[:,:],\n",
    "                 'Kara': mask_Kara[:,:],\n",
    "                 'Barents': mask_Barents[:,:],\n",
    "                 'Greenland': mask_Greenland[:,:],\n",
    "                 'CenArctic': mask_CenArctic[:,:]}\n",
    "    seas_namelist = ['Beaufort','Chukchi','EastSiberian','Laptev',\n",
    "                     'Kara', 'Barents', 'Greenland','CenArctic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########                  temporal interpolation matrix                  ########\n",
    "    #################################################################################\n",
    "    # interpolate from monthly to weekly\n",
    "    # original monthly data will be taken as the last week of the month\n",
    "    OHC_300_ORAS4_weekly_series = np.zeros(SIC_ERAI.reshape(len(year_ERAI)*48,len(latitude_ERAI),len(longitude_ERAI)).shape,\n",
    "                                           dtype=float)\n",
    "    OHC_300_ORAS4_series= dlacs.preprocess.operator.unfold(OHC_300_ORAS4)\n",
    "    # calculate the difference between two months\n",
    "    OHC_300_ORAS4_deviation_series = (OHC_300_ORAS4_series[1:,:,:] - OHC_300_ORAS4_series[:-1,:,:]) / 4\n",
    "    for i in np.arange(4):\n",
    "        OHC_300_ORAS4_weekly_series[3-i::4,:,:] = OHC_300_ORAS4_series[12:,:,:] - i * OHC_300_ORAS4_deviation_series[11:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  calculate extent from spatial fields  *******************\n",
      "================  reshape input data into time series  =================\n"
     ]
    }
   ],
   "source": [
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / 480\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "    SFlux_ERAI_area = np.zeros(SFlux_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,:,i,:] = SIC_ERAI[:,:,i,:]* dx[i] * dy / 1E+6 # unit km2\n",
    "        SFlux_ERAI_area[:,:,i,:] = SFlux_ERAI[:,:,i,:]* dx[i] * dy / 1E+12 # unit TeraWatt\n",
    "    SIC_ERAI_area[SIC_ERAI_area<0] = 0 # switch the mask from -1 to 0\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    SIC_ERAI_area_series = dlacs.preprocess.operator.unfold(SIC_ERAI_area)\n",
    "    T2M_ERAI_series = dlacs.preprocess.operator.unfold(T2M_ERAI)\n",
    "    SLP_ERAI_series = dlacs.preprocess.operator.unfold(SLP_ERAI)\n",
    "    Z500_ERAI_series = dlacs.preprocess.operator.unfold(Z500_ERAI)\n",
    "    Z850_ERAI_series = dlacs.preprocess.operator.unfold(Z850_ERAI)\n",
    "    U10M_ERAI_series = dlacs.preprocess.operator.unfold(U10M_ERAI)\n",
    "    V10M_ERAI_series = dlacs.preprocess.operator.unfold(V10M_ERAI)\n",
    "    SFlux_ERAI_area_series = dlacs.preprocess.operator.unfold(SFlux_ERAI_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  choose the fields from target region  *******************\n",
      "******************  choose the fields from target region  *******************\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n"
     ]
    }
   ],
   "source": [
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select land-sea mask\n",
    "    sea_ice_mask_barents = sea_ice_mask_global[12:36,264:320]\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select the area between greenland and ice land for instance 60-70 N / 44-18 W\n",
    "    sic_exp = SIC_ERAI_area_series[:,12:36,264:320]\n",
    "    t2m_exp = T2M_ERAI_series[:,12:36,264:320]\n",
    "    slp_exp = SLP_ERAI_series[:,12:36,264:320]\n",
    "    z500_exp = Z500_ERAI_series[:,12:36,264:320]\n",
    "    z850_exp = Z850_ERAI_series[:,12:36,264:320]\n",
    "    u10m_exp = U10M_ERAI_series[:,12:36,264:320]\n",
    "    v10m_exp = V10M_ERAI_series[:,12:36,264:320]\n",
    "    sflux_exp = SFlux_ERAI_area_series[:,12:36,264:320]\n",
    "    ohc_exp = OHC_300_ORAS4_weekly_series[:,12:36,264:320]\n",
    "    print(sic_exp.shape)\n",
    "    print(t2m_exp.shape)\n",
    "    print(slp_exp.shape)\n",
    "    print(z500_exp.shape)\n",
    "    print(u10m_exp.shape)\n",
    "    print(v10m_exp.shape)\n",
    "    print(sflux_exp.shape)\n",
    "    print(ohc_exp.shape)\n",
    "    print(latitude_ERAI[12:36])\n",
    "    print(longitude_ERAI[264:320])\n",
    "    print(latitude_ORAS4[12:36])\n",
    "    print(longitude_ORAS4[264:320])\n",
    "    #print(latitude_ERAI[26:40])\n",
    "    #print(longitude_ERAI[180:216])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  pre-processing  *********************\n",
      "=========================   normalize data   ===========================\n",
      "================  save the normalizing factor  =================\n",
      "1565.2049481856002 km2\n",
      "0.0 km2\n",
      "====================    A series of time (index)    ====================\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('=========================   normalize data   ===========================')\n",
    "    sic_exp_norm = dlacs.preprocess.operator.normalize(sic_exp)\n",
    "    t2m_exp_norm = dlacs.preprocess.operator.normalize(t2m_exp)\n",
    "    slp_exp_norm = dlacs.preprocess.operator.normalize(slp_exp)\n",
    "    z500_exp_norm = dlacs.preprocess.operator.normalize(z500_exp)\n",
    "    z850_exp_norm = dlacs.preprocess.operator.normalize(z850_exp)\n",
    "    u10m_exp_norm = dlacs.preprocess.operator.normalize(u10m_exp)\n",
    "    v10m_exp_norm = dlacs.preprocess.operator.normalize(v10m_exp)\n",
    "    sflux_exp_norm = dlacs.preprocess.operator.normalize(sflux_exp)\n",
    "    ohc_exp_norm = dlacs.preprocess.operator.normalize(ohc_exp)\n",
    "    print('================  save the normalizing factor  =================')\n",
    "    sic_max = np.amax(sic_exp)\n",
    "    sic_min = np.amin(sic_exp)\n",
    "    print(sic_max,\"km2\")\n",
    "    print(sic_min,\"km2\")\n",
    "    ohc_max = np.amax(ohc_exp)\n",
    "    ohc_min = np.amin(ohc_exp)\n",
    "    t2m_max = np.amax(t2m_exp)\n",
    "    t2m_min = np.amin(t2m_exp)\n",
    "    slp_max = np.amax(slp_exp)\n",
    "    slp_min = np.amin(slp_exp)\n",
    "    z500_max = np.amax(z500_exp)\n",
    "    z500_min = np.amin(z500_exp)\n",
    "    z850_max = np.amax(z850_exp)\n",
    "    z850_min = np.amin(z850_exp)\n",
    "    u10m_max = np.amax(u10m_exp)\n",
    "    u10m_min = np.amin(u10m_exp)\n",
    "    v10m_max = np.amax(v10m_exp)\n",
    "    v10m_min = np.amin(v10m_exp)\n",
    "    sflux_max = np.amax(sflux_exp)\n",
    "    sflux_min = np.amin(sflux_exp)    \n",
    "    print ('====================    A series of time (index)    ====================')\n",
    "    _, yy, xx = sic_exp_norm.shape # get the lat lon dimension\n",
    "    year = np.arange(1979,2017,1)\n",
    "    year_cycle = np.repeat(year,48)\n",
    "    month_cycle = np.repeat(np.arange(1,13,1),4)\n",
    "    month_cycle = np.tile(month_cycle,len(year)+1) # one extra repeat for lead time dependent prediction\n",
    "    month_cycle.astype(float)\n",
    "    month_2D = np.repeat(month_cycle[:,np.newaxis],yy,1)\n",
    "    month_exp = np.repeat(month_2D[:,:,np.newaxis],xx,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================   Monte Carlo reshuffling   ===========================\n"
     ]
    }
   ],
   "source": [
    "    print ('=========================   Monte Carlo reshuffling   ===========================')\n",
    "    # reshuffle a multi-dimension array with numpy.random.shuffle\n",
    "    # This function only shuffles the array along the first axis of a multi-dimensional array.\n",
    "    # The order of sub-arrays is changed but their contents remains the same.\n",
    "    t2m_exp_norm_shuffle = np.random.shuffle(t2m_exp_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1824, 24, 56)\n",
      "[[251.4346045  251.05600413 250.50871172 ... 244.2139345  244.13173904\n",
      "  244.01692955]\n",
      " [246.64799721 246.04600942 245.45885542 ... 245.36829478 245.24715214\n",
      "  245.12353721]\n",
      " [246.7402512  247.31849816 248.16998491 ... 246.47754407 246.37387686\n",
      "  246.26773736]\n",
      " ...\n",
      " [271.39089775 271.56084288 271.75937185 ... 272.40565781 271.30805881\n",
      "  270.25455477]\n",
      " [271.93968032 272.07067828 272.09157428 ... 273.35488503 272.4173081\n",
      "  271.39831464]\n",
      " [272.82825155 272.92514534 272.69329114 ... 274.15672261 273.34753587\n",
      "  272.70551716]]\n",
      "(24, 56)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print(t2m_exp.shape)\n",
    "    ax = plt.contourf(t2m_exp[443,:,:])\n",
    "    print(t2m_exp[443,:,:])\n",
    "    print(month_exp[0,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26267704 0.26046318 0.25661401 ... 0.22950646 0.22708498 0.22442972]\n",
      " [0.20302922 0.20179229 0.20073181 ... 0.23818486 0.23636252 0.23451899]\n",
      " [0.19524795 0.20810658 0.22415041 ... 0.25322432 0.25219251 0.25112267]\n",
      " ...\n",
      " [0.48474634 0.48954066 0.49081374 ... 0.36213768 0.336037   0.3028926 ]\n",
      " [0.49387301 0.5059012  0.52108218 ... 0.37193518 0.34423099 0.31239209]\n",
      " [0.52729484 0.54777388 0.5776589  ... 0.38700331 0.35481465 0.32946398]]\n",
      "(4, 24, 56)\n"
     ]
    }
   ],
   "source": [
    "    x_input = np.stack((sic_exp_norm[1,:,:],\n",
    "                        t2m_exp_norm[1,:,:],\n",
    "                        ohc_exp_norm[1,:,:],\n",
    "                        month_exp[1,:,:]))\n",
    "    print(x_input[1,:,:])\n",
    "    print(x_input[:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  parameter for check  *********************\n",
      "*******************  create basic dimensions for tensor and network  *********************\n",
      "1.1.0\n",
      "Is CUDA available? True\n",
      "*******************  cross validation and testing data  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  parameter for check  *********************')\n",
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    # specifications of neural network\n",
    "    input_channels = 10\n",
    "    hidden_channels = [10, 9, 1] # number of channels & hidden layers, the channels of last layer is the channels of output, too\n",
    "    #hidden_channels = [3, 3, 3, 3, 2]\n",
    "    #hidden_channels = [2]\n",
    "    kernel_size = 3\n",
    "    # here we input a sequence and predict the next step only\n",
    "    #step = 1 # how many steps to predict ahead\n",
    "    #effective_step = [0] # step to output\n",
    "    batch_size = 1\n",
    "    #num_layers = 1\n",
    "    learning_rate = 0.005\n",
    "    num_epochs = 1500\n",
    "    print (torch.__version__)\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    print ('*******************  cross validation and testing data  *********************')\n",
    "    # take 10% data as cross-validation data\n",
    "    cross_valid_year = 4\n",
    "    # take 10% years as testing data\n",
    "    test_year = 4\n",
    "    # minibatch\n",
    "    #iterations = 3 # training data divided into 3 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  preview of input tensor  *********************\n",
      "1824\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  preview of input tensor  *********************')\n",
    "    #plt.plot(SIC_ERAI_sum)\n",
    "    #print(SIC_ERAI_sum_norm[:-test_year*12])\n",
    "    #print(x_input.shape)\n",
    "    #print(x_input[:,:,:])\n",
    "    sequence_len, _, _ = sic_exp_norm.shape\n",
    "    print(sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print ('*******************  module for calculating accuracy  *********************')\n",
    "#     def accuracy(out, labels):\n",
    "#         outputs = np.argmax(out, axis=1)\n",
    "#     return np.sum(outputs==labels)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  load exsited LSTM model  *********************\n",
      "ConvLSTM(\n",
      "  (cell0): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell1): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell2): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "Wall time: 8.26 s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print ('*******************  load exsited LSTM model  *********************')\n",
    "    #model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500.pkl'))\n",
    "    model = torch.load(os.path.join(output_path, 'Barents','convlstm_era_sic_t2m_slp_z500_z850_uv10m_sflux_oras_ohc_Barents_hl_3_kernel_3_lr_0.005_epoch_1500_validSIC.pkl'))\n",
    "    print(model)\n",
    "     # check the sequence length (dimension in need for post-processing)\n",
    "    sequence_len, height, width = sic_exp_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  run LSTM  *********************\n",
      "The model is designed to make many to one prediction.\n",
      "A series of multi-chanel variables will be input to the model.\n",
      "The model learns by verifying the output at each timestep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\torch\\nn\\_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM(\n",
      "  (cell0): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell1): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell2): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.005\n",
      "    weight_decay: 0\n",
      ")\n",
      "##############################################################\n",
      "##################  start training loop  #####################\n",
      "##############################################################\n",
      "Epoch  0 MSE:  56.72515869140625\n",
      "Epoch  5 MSE:  43.79138946533203\n",
      "Epoch  10 MSE:  40.66634750366211\n",
      "Epoch  15 MSE:  35.654052734375\n",
      "Epoch  20 MSE:  30.078166961669922\n",
      "Epoch  25 MSE:  23.74620246887207\n",
      "Epoch  30 MSE:  32.7795524597168\n",
      "Epoch  35 MSE:  24.216459274291992\n",
      "Epoch  40 MSE:  19.359378814697266\n",
      "Epoch  45 MSE:  17.6132869720459\n",
      "Epoch  50 MSE:  15.806286811828613\n",
      "Epoch  55 MSE:  14.861740112304688\n",
      "Epoch  60 MSE:  13.484221458435059\n",
      "Epoch  65 MSE:  12.710884094238281\n",
      "Epoch  70 MSE:  11.67790699005127\n",
      "Epoch  75 MSE:  10.779419898986816\n",
      "Epoch  80 MSE:  9.967140197753906\n",
      "Epoch  85 MSE:  9.09227180480957\n",
      "Epoch  90 MSE:  8.484979629516602\n",
      "Epoch  95 MSE:  8.008442878723145\n",
      "Epoch  100 MSE:  7.52720832824707\n",
      "Epoch  105 MSE:  7.185723304748535\n",
      "Epoch  110 MSE:  6.9104533195495605\n",
      "Epoch  115 MSE:  6.612207889556885\n",
      "Epoch  120 MSE:  6.323196887969971\n",
      "Epoch  125 MSE:  6.076883792877197\n",
      "Epoch  130 MSE:  5.808966159820557\n",
      "Epoch  135 MSE:  6.033420562744141\n",
      "Epoch  140 MSE:  5.82633638381958\n",
      "Epoch  145 MSE:  5.167993068695068\n",
      "Epoch  150 MSE:  4.982827186584473\n",
      "Epoch  155 MSE:  4.744247913360596\n",
      "Epoch  160 MSE:  4.426337242126465\n",
      "Epoch  165 MSE:  4.265533924102783\n",
      "Epoch  170 MSE:  4.0683207511901855\n",
      "Epoch  175 MSE:  3.9108166694641113\n",
      "Epoch  180 MSE:  3.777031660079956\n",
      "Epoch  185 MSE:  3.6618599891662598\n",
      "Epoch  190 MSE:  3.5305538177490234\n",
      "Epoch  195 MSE:  3.6561527252197266\n",
      "Epoch  200 MSE:  3.5031282901763916\n",
      "Epoch  205 MSE:  3.421518325805664\n",
      "Epoch  210 MSE:  3.2896506786346436\n",
      "Epoch  215 MSE:  3.1984055042266846\n",
      "Epoch  220 MSE:  3.085026741027832\n",
      "Epoch  225 MSE:  2.9824466705322266\n",
      "Epoch  230 MSE:  2.97460675239563\n",
      "Epoch  235 MSE:  3.035952091217041\n",
      "Epoch  240 MSE:  2.9843075275421143\n",
      "Epoch  245 MSE:  2.8830008506774902\n",
      "Epoch  250 MSE:  2.7901933193206787\n",
      "Epoch  255 MSE:  2.708209753036499\n",
      "Epoch  260 MSE:  2.648921251296997\n",
      "Epoch  265 MSE:  2.6863975524902344\n",
      "Epoch  270 MSE:  2.5727193355560303\n",
      "Epoch  275 MSE:  2.720862865447998\n",
      "Epoch  280 MSE:  2.6594066619873047\n",
      "Epoch  285 MSE:  2.6129202842712402\n",
      "Epoch  290 MSE:  2.541689395904541\n",
      "Epoch  295 MSE:  2.4991390705108643\n",
      "Epoch  300 MSE:  2.4423043727874756\n",
      "Epoch  305 MSE:  2.398376941680908\n",
      "Epoch  310 MSE:  2.4483916759490967\n",
      "Epoch  315 MSE:  2.3666603565216064\n",
      "Epoch  320 MSE:  2.4350240230560303\n",
      "Epoch  325 MSE:  2.382402181625366\n",
      "Epoch  330 MSE:  2.3066961765289307\n",
      "Epoch  335 MSE:  2.353675603866577\n",
      "Epoch  340 MSE:  2.342255115509033\n",
      "Epoch  345 MSE:  2.2792117595672607\n",
      "Epoch  350 MSE:  2.237532138824463\n",
      "Epoch  355 MSE:  2.222388982772827\n",
      "Epoch  360 MSE:  2.4343581199645996\n",
      "Epoch  365 MSE:  2.3213400840759277\n",
      "Epoch  370 MSE:  2.335930109024048\n",
      "Epoch  375 MSE:  2.2215075492858887\n",
      "Epoch  380 MSE:  2.1962850093841553\n",
      "Epoch  385 MSE:  2.1848113536834717\n",
      "Epoch  390 MSE:  2.1484813690185547\n",
      "Epoch  395 MSE:  2.1446077823638916\n",
      "Epoch  400 MSE:  2.117236852645874\n",
      "Epoch  405 MSE:  2.117607355117798\n",
      "Epoch  410 MSE:  2.3389084339141846\n",
      "Epoch  415 MSE:  2.1931893825531006\n",
      "Epoch  420 MSE:  2.165651321411133\n",
      "Epoch  425 MSE:  2.1243722438812256\n",
      "Epoch  430 MSE:  2.113487482070923\n",
      "Epoch  435 MSE:  2.0737955570220947\n",
      "Epoch  440 MSE:  2.0568578243255615\n",
      "Epoch  445 MSE:  2.0494251251220703\n",
      "Epoch  450 MSE:  2.0375752449035645\n",
      "Epoch  455 MSE:  2.1213905811309814\n",
      "Epoch  460 MSE:  2.0153210163116455\n",
      "Epoch  465 MSE:  2.037125825881958\n",
      "Epoch  470 MSE:  2.01289439201355\n",
      "Epoch  475 MSE:  2.001553535461426\n",
      "Epoch  480 MSE:  2.0465011596679688\n",
      "Epoch  485 MSE:  2.036360502243042\n",
      "Epoch  490 MSE:  1.9997035264968872\n",
      "Epoch  495 MSE:  1.9689370393753052\n",
      "Epoch  500 MSE:  1.9638981819152832\n",
      "Epoch  505 MSE:  1.9813486337661743\n",
      "Epoch  510 MSE:  2.0442452430725098\n",
      "Epoch  515 MSE:  1.9659714698791504\n",
      "Epoch  520 MSE:  1.935504674911499\n",
      "Epoch  525 MSE:  1.989017128944397\n",
      "Epoch  530 MSE:  1.9221055507659912\n",
      "Epoch  535 MSE:  1.9666416645050049\n",
      "Epoch  540 MSE:  1.9324785470962524\n",
      "Epoch  545 MSE:  1.9009393453598022\n",
      "Epoch  550 MSE:  1.921987533569336\n",
      "Epoch  555 MSE:  2.092700719833374\n",
      "Epoch  560 MSE:  2.0103940963745117\n",
      "Epoch  565 MSE:  1.9377572536468506\n",
      "Epoch  570 MSE:  1.9216232299804688\n",
      "Epoch  575 MSE:  1.9063791036605835\n",
      "Epoch  580 MSE:  1.8978602886199951\n",
      "Epoch  585 MSE:  1.8987092971801758\n",
      "Epoch  590 MSE:  1.868653655052185\n",
      "Epoch  595 MSE:  1.9141247272491455\n",
      "Epoch  600 MSE:  1.8482658863067627\n",
      "Epoch  605 MSE:  1.912591576576233\n",
      "Epoch  610 MSE:  1.8508774042129517\n",
      "Epoch  615 MSE:  1.8829911947250366\n",
      "Epoch  620 MSE:  1.831801414489746\n",
      "Epoch  625 MSE:  1.8710452318191528\n",
      "Epoch  630 MSE:  1.830570101737976\n",
      "Epoch  635 MSE:  1.829079270362854\n",
      "Epoch  640 MSE:  1.8469228744506836\n",
      "Epoch  645 MSE:  1.8228569030761719\n",
      "Epoch  650 MSE:  1.8043758869171143\n",
      "Epoch  655 MSE:  1.801361083984375\n",
      "Epoch  660 MSE:  2.1026594638824463\n",
      "Epoch  665 MSE:  1.9971648454666138\n",
      "Epoch  670 MSE:  1.8227614164352417\n",
      "Epoch  675 MSE:  1.94158136844635\n",
      "Epoch  680 MSE:  1.8497802019119263\n",
      "Epoch  685 MSE:  1.8795772790908813\n",
      "Epoch  690 MSE:  1.8506767749786377\n",
      "Epoch  695 MSE:  1.8122572898864746\n",
      "Epoch  700 MSE:  1.793369174003601\n",
      "Epoch  705 MSE:  1.7650337219238281\n",
      "Epoch  710 MSE:  1.7716741561889648\n",
      "Epoch  715 MSE:  1.7627555131912231\n",
      "Epoch  720 MSE:  1.757523775100708\n",
      "Epoch  725 MSE:  1.7505861520767212\n",
      "Epoch  730 MSE:  1.750984787940979\n",
      "Epoch  735 MSE:  1.7541992664337158\n",
      "Epoch  740 MSE:  1.8115264177322388\n",
      "Epoch  745 MSE:  1.7568814754486084\n",
      "Epoch  750 MSE:  1.754460334777832\n",
      "Epoch  755 MSE:  1.7489861249923706\n",
      "Epoch  760 MSE:  1.744062900543213\n",
      "Epoch  765 MSE:  1.7337141036987305\n",
      "Epoch  770 MSE:  1.7351157665252686\n",
      "Epoch  775 MSE:  1.7183629274368286\n",
      "Epoch  780 MSE:  1.7353287935256958\n",
      "Epoch  785 MSE:  1.8585187196731567\n",
      "Epoch  790 MSE:  1.7783212661743164\n",
      "Epoch  795 MSE:  1.7361141443252563\n",
      "Epoch  800 MSE:  1.7070903778076172\n",
      "Epoch  805 MSE:  1.7206249237060547\n",
      "Epoch  810 MSE:  1.704127550125122\n",
      "Epoch  815 MSE:  1.7180153131484985\n",
      "Epoch  820 MSE:  1.7180062532424927\n",
      "Epoch  825 MSE:  1.731044054031372\n",
      "Epoch  830 MSE:  1.7592015266418457\n",
      "Epoch  835 MSE:  1.704992413520813\n",
      "Epoch  840 MSE:  1.7215027809143066\n",
      "Epoch  845 MSE:  1.7345465421676636\n",
      "Epoch  850 MSE:  1.7004797458648682\n",
      "Epoch  855 MSE:  1.6935269832611084\n",
      "Epoch  860 MSE:  1.7739002704620361\n",
      "Epoch  865 MSE:  1.7168282270431519\n",
      "Epoch  870 MSE:  1.7437851428985596\n",
      "Epoch  875 MSE:  1.7039562463760376\n",
      "Epoch  880 MSE:  1.723675012588501\n",
      "Epoch  885 MSE:  1.7180724143981934\n",
      "Epoch  890 MSE:  1.6938493251800537\n",
      "Epoch  895 MSE:  1.691119909286499\n",
      "Epoch  900 MSE:  1.6729729175567627\n",
      "Epoch  905 MSE:  1.6704928874969482\n",
      "Epoch  910 MSE:  1.6633820533752441\n",
      "Epoch  915 MSE:  1.7062747478485107\n",
      "Epoch  920 MSE:  1.7155189514160156\n",
      "Epoch  925 MSE:  1.691733956336975\n",
      "Epoch  930 MSE:  1.6595869064331055\n",
      "Epoch  935 MSE:  1.6956931352615356\n",
      "Epoch  940 MSE:  1.6633212566375732\n",
      "Epoch  945 MSE:  1.6564286947250366\n",
      "Epoch  950 MSE:  1.6957004070281982\n",
      "Epoch  955 MSE:  1.6926283836364746\n",
      "Epoch  960 MSE:  1.6570812463760376\n",
      "Epoch  965 MSE:  1.710717797279358\n",
      "Epoch  970 MSE:  1.6496044397354126\n",
      "Epoch  975 MSE:  1.7098839282989502\n",
      "Epoch  980 MSE:  1.643884301185608\n",
      "Epoch  985 MSE:  1.6811485290527344\n",
      "Epoch  990 MSE:  1.6514368057250977\n",
      "Epoch  995 MSE:  1.6550116539001465\n",
      "Epoch  1000 MSE:  1.6329296827316284\n",
      "Epoch  1005 MSE:  1.7401986122131348\n",
      "Epoch  1010 MSE:  1.6299445629119873\n",
      "Epoch  1015 MSE:  1.6554372310638428\n",
      "Epoch  1020 MSE:  1.6231627464294434\n",
      "Epoch  1025 MSE:  1.6242425441741943\n",
      "Epoch  1030 MSE:  1.6243526935577393\n",
      "Epoch  1035 MSE:  2.083477020263672\n",
      "Epoch  1040 MSE:  1.9373785257339478\n",
      "Epoch  1045 MSE:  2.0285863876342773\n",
      "Epoch  1050 MSE:  1.8176863193511963\n",
      "Epoch  1055 MSE:  1.744388222694397\n",
      "Epoch  1060 MSE:  1.686969518661499\n",
      "Epoch  1065 MSE:  1.6701393127441406\n",
      "Epoch  1070 MSE:  1.6619032621383667\n",
      "Epoch  1075 MSE:  1.6371697187423706\n",
      "Epoch  1080 MSE:  1.6257518529891968\n",
      "Epoch  1085 MSE:  1.6208546161651611\n",
      "Epoch  1090 MSE:  1.6129435300827026\n",
      "Epoch  1095 MSE:  1.6152395009994507\n",
      "Epoch  1100 MSE:  1.646844506263733\n",
      "Epoch  1105 MSE:  1.6404974460601807\n",
      "Epoch  1110 MSE:  1.6515008211135864\n",
      "Epoch  1115 MSE:  1.6438822746276855\n",
      "Epoch  1120 MSE:  1.6088964939117432\n",
      "Epoch  1125 MSE:  1.5982431173324585\n",
      "Epoch  1130 MSE:  1.598919153213501\n",
      "Epoch  1135 MSE:  1.5910965204238892\n",
      "Epoch  1140 MSE:  1.5975465774536133\n",
      "Epoch  1145 MSE:  1.5915861129760742\n",
      "Epoch  1150 MSE:  1.5961962938308716\n",
      "Epoch  1155 MSE:  1.605677843093872\n",
      "Epoch  1160 MSE:  1.6422932147979736\n",
      "Epoch  1165 MSE:  1.5894453525543213\n",
      "Epoch  1170 MSE:  1.6400259733200073\n",
      "Epoch  1175 MSE:  1.580406904220581\n",
      "Epoch  1180 MSE:  1.6174448728561401\n",
      "Epoch  1185 MSE:  1.5817935466766357\n",
      "Epoch  1190 MSE:  1.6354038715362549\n",
      "Epoch  1195 MSE:  1.5795691013336182\n",
      "Epoch  1200 MSE:  1.6103595495224\n",
      "Epoch  1205 MSE:  1.5862302780151367\n",
      "Epoch  1210 MSE:  1.5762405395507812\n",
      "Epoch  1215 MSE:  1.6114065647125244\n",
      "Epoch  1220 MSE:  1.691348671913147\n",
      "Epoch  1225 MSE:  1.6210670471191406\n",
      "Epoch  1230 MSE:  1.5912631750106812\n",
      "Epoch  1235 MSE:  1.5857880115509033\n",
      "Epoch  1240 MSE:  1.5935136079788208\n",
      "Epoch  1245 MSE:  1.5806667804718018\n",
      "Epoch  1250 MSE:  1.5727362632751465\n",
      "Epoch  1255 MSE:  1.5774415731430054\n",
      "Epoch  1260 MSE:  1.5610359907150269\n",
      "Epoch  1265 MSE:  1.5853854417800903\n",
      "Epoch  1270 MSE:  1.7827558517456055\n",
      "Epoch  1275 MSE:  1.698111891746521\n",
      "Epoch  1280 MSE:  1.6110529899597168\n",
      "Epoch  1285 MSE:  1.5999780893325806\n",
      "Epoch  1290 MSE:  1.5741630792617798\n",
      "Epoch  1295 MSE:  1.5716642141342163\n",
      "Epoch  1300 MSE:  1.5660560131072998\n",
      "Epoch  1305 MSE:  1.5595699548721313\n",
      "Epoch  1310 MSE:  1.5645712614059448\n",
      "Epoch  1315 MSE:  1.548553705215454\n",
      "Epoch  1320 MSE:  1.566595196723938\n",
      "Epoch  1325 MSE:  1.610167384147644\n",
      "Epoch  1330 MSE:  1.5511271953582764\n",
      "Epoch  1335 MSE:  1.6098644733428955\n",
      "Epoch  1340 MSE:  1.569617509841919\n",
      "Epoch  1345 MSE:  1.5522404909133911\n",
      "Epoch  1350 MSE:  1.6249290704727173\n",
      "Epoch  1355 MSE:  1.5810983180999756\n",
      "Epoch  1360 MSE:  1.556121826171875\n",
      "Epoch  1365 MSE:  1.5710408687591553\n",
      "Epoch  1370 MSE:  1.5627005100250244\n",
      "Epoch  1375 MSE:  1.5853995084762573\n",
      "Epoch  1380 MSE:  1.5613003969192505\n",
      "Epoch  1385 MSE:  1.5323444604873657\n",
      "Epoch  1390 MSE:  1.5561541318893433\n",
      "Epoch  1395 MSE:  1.534055471420288\n",
      "Epoch  1400 MSE:  1.5386837720870972\n",
      "Epoch  1405 MSE:  1.6600557565689087\n",
      "Epoch  1410 MSE:  1.5649150609970093\n",
      "Epoch  1415 MSE:  1.5575029850006104\n",
      "Epoch  1420 MSE:  1.5414296388626099\n",
      "Epoch  1425 MSE:  1.5341655015945435\n",
      "Epoch  1430 MSE:  1.5375114679336548\n",
      "Epoch  1435 MSE:  1.5332794189453125\n",
      "Epoch  1440 MSE:  1.5466325283050537\n",
      "Epoch  1445 MSE:  1.5454635620117188\n",
      "Epoch  1450 MSE:  1.5279698371887207\n",
      "Epoch  1455 MSE:  1.5872886180877686\n",
      "Epoch  1460 MSE:  1.5218620300292969\n",
      "Epoch  1465 MSE:  1.5741348266601562\n",
      "Epoch  1470 MSE:  1.5146338939666748\n",
      "Epoch  1475 MSE:  1.5486119985580444\n",
      "Epoch  1480 MSE:  1.5518769025802612\n",
      "Epoch  1485 MSE:  1.5177149772644043\n",
      "Epoch  1490 MSE:  1.557279109954834\n",
      "Epoch  1495 MSE:  1.54914391040802\n",
      "Wall time: 9h 21min 14s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print ('*******************  run LSTM  *********************')\n",
    "    print ('The model is designed to make many to one prediction.')\n",
    "    print ('A series of multi-chanel variables will be input to the model.')\n",
    "    print ('The model learns by verifying the output at each timestep.')\n",
    "    # check the sequence length\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "\n",
    "    # initialize our model\n",
    "    model = deepclim.deepArray.ConvLSTM(input_channels, hidden_channels, kernel_size).cuda()\n",
    "    loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "    # stochastic gradient descent\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # Adam optimizer\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(model)\n",
    "    print(loss_fn)\n",
    "    print(optimiser)\n",
    "    print('##############################################################')\n",
    "    print('##################  start training loop  #####################')\n",
    "    print('##############################################################')\n",
    "    hist = np.zeros(num_epochs)\n",
    "    # loop of epoch\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # loop of timestep\n",
    "        for timestep in range(sequence_len - cross_valid_year*12*4 - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #################################################################################\n",
    "            ########          create input tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                ohc_exp_norm[timestep,:,:],\n",
    "                                t2m_exp_norm[timestep,:,:],\n",
    "                                slp_exp_norm[timestep,:,:],\n",
    "                                z500_exp_norm[timestep,:,:],\n",
    "                                z850_exp_norm[timestep,:,:],\n",
    "                                u10m_exp_norm[timestep,:,:],\n",
    "                                v10m_exp_norm[timestep,:,:],\n",
    "                                sflux_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).cuda()\n",
    "            #################################################################################\n",
    "            ########       create training tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            y_train_stack = sic_exp_norm[timestep+1,:,:] #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).cuda()\n",
    "            #################################################################################   \n",
    "            # Forward pass\n",
    "            y_pred, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_train = y_var        \n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            if timestep == 0:\n",
    "                loss = loss_fn(y_pred, y_train)\n",
    "            else:\n",
    "                loss += loss_fn(y_pred, y_train)\n",
    "            #print (timestep)\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 5 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            #print(y_pred)\n",
    "            # gradient check\n",
    "            # Gradcheck requires double precision numbers to run\n",
    "            #res = torch.autograd.gradcheck(loss_fn, (y_pred.double(), y_train.double()), eps=1e-6, raise_exception=True)\n",
    "            #print(res)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    torch.save(model, os.path.join(output_path,'convlstm.pkl')) # save lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print (\"*******************  Loss with time  **********************\")\n",
    "    fig00 = plt.figure()\n",
    "    try:\n",
    "        plt.plot(hist, label=\"Training loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig00.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred_error.png'),dpi=200)\n",
    "    except:\n",
    "        print('Model is reloaded instead of trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  evaluation matrix  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  evaluation matrix  *********************')\n",
    "    # The prediction will be evaluated through RMSE against climatology\n",
    "    \n",
    "    # error score for temporal-spatial fields, without keeping spatial pattern\n",
    "    def RMSE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the RMSE. x is input series and y is reference series.\n",
    "        It calculates RMSE over the domain, not over time. The spatial structure\n",
    "        will not be kept.\n",
    "        Parameter\n",
    "        ----------------------\n",
    "        x: input time series with the shape [time, lat, lon]\n",
    "        \"\"\"\n",
    "        x_series = x.reshape(x.shape[0],-1)\n",
    "        y_series = y.reshape(y.shape[0],-1)\n",
    "        rmse = np.sqrt(np.mean((x_series - y_series)**2,1))\n",
    "        rmse_std = np.sqrt(np.std((x_series - y_series)**2,1))\n",
    "    \n",
    "        return rmse, rmse_std\n",
    "    \n",
    "    # error score for temporal-spatial fields, keeping spatial pattern\n",
    "    def MAE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the MAE. x is input series and y is reference series.\n",
    "        It calculate MAE over time and keeps the spatial structure.\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(x-y),0)\n",
    "        \n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "###################  start prediction loop ###################\n",
      "##############################################################\n",
      "*******************************  one step ahead forecast  *********************************\n",
      "************  the last 4 years of total time series are treated as test data  ************\n",
      "Wall time: 35min 33s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    #################################################################################\n",
    "    ########  operational lead time dependent prediction with testing data   ########\n",
    "    #################################################################################\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(test_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 16 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    ohc_exp_norm[i-1,:,:],\n",
    "                                    t2m_exp_norm[i-1,:,:],\n",
    "                                    slp_exp_norm[i-1,:,:],\n",
    "                                    z500_exp_norm[i-1,:,:],\n",
    "                                    z850_exp_norm[i-1,:,:],\n",
    "                                    u10m_exp_norm[i-1,:,:],\n",
    "                                    v10m_exp_norm[i-1,:,:],\n",
    "                                    sflux_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[i-1,:,:],\n",
    "                                        t2m_exp_norm[i-1,:,:],\n",
    "                                        slp_exp_norm[i-1,:,:],\n",
    "                                        z500_exp_norm[i-1,:,:],\n",
    "                                        z850_exp_norm[i-1,:,:],\n",
    "                                        u10m_exp_norm[i-1,:,:],\n",
    "                                        v10m_exp_norm[i-1,:,:],\n",
    "                                        sflux_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[-1,:,:],\n",
    "                                        t2m_exp_norm[-1,:,:],\n",
    "                                        slp_exp_norm[-1,:,:],\n",
    "                                        z500_exp_norm[-1,:,:],\n",
    "                                        z850_exp_norm[-1,:,:],\n",
    "                                        u10m_exp_norm[-1,:,:],\n",
    "                                        v10m_exp_norm[-1,:,:],\n",
    "                                        sflux_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week2month(series, m):\n",
    "    \"\"\"\n",
    "    Select certain month from yearly data at weekly resolution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        Three-dimensional numeric arrays with time as the first dimenison [time, lat, lon]\n",
    "    m: int\n",
    "        Month (from 1 to 12).\n",
    "    \"\"\"\n",
    "    time_year, lat, lon = series.shape\n",
    "    time_month = time_year // 12\n",
    "    series_month = np.zeros((time_month, lat, lon), dtype=float)\n",
    "    series_month[::4,:,:] = series[(m-1)*4::48,:,:]\n",
    "    series_month[1::4,:,:] = series[(m-1)*4+1::48,:,:]\n",
    "    series_month[2::4,:,:] = series[(m-1)*4+2::48,:,:]\n",
    "    series_month[3::4,:,:] = series[(m-1)*4+3::48,:,:]\n",
    "    \n",
    "    return series_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   land-sea correction for sic prediction    #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "43.86874366886344 +- 88.71316844877391\n",
      "Mean RMSE with testing data - Climatology\n",
      "137.91125915081744 +- 212.30182952186146\n",
      "Mean RMSE with testing data - Persistence\n",
      "50.170794466755474 +- 104.00027668956385\n",
      "*******************     Lead time 1     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "62.885378008843475 +- 123.15174193891708\n",
      "Mean RMSE with testing data - Persistence\n",
      "75.9458837476023 +- 153.19988772374379\n",
      "*******************     Lead time 2     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "74.88768786732052 +- 143.42215772859598\n",
      "Mean RMSE with testing data - Persistence\n",
      "93.91398900099325 +- 186.0275468269465\n",
      "*******************     Lead time 3     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "83.70138683518532 +- 157.13401502143208\n",
      "Mean RMSE with testing data - Persistence\n",
      "110.1283583502817 +- 213.094235734779\n",
      "*******************     Lead time 4     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "90.74376650208177 +- 167.38542313799968\n",
      "Mean RMSE with testing data - Persistence\n",
      "124.67600441157826 +- 234.89027045606795\n",
      "*******************     Lead time 5     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "96.3023291619574 +- 175.33986269660872\n",
      "Mean RMSE with testing data - Persistence\n",
      "138.19688180429785 +- 253.81677913415027\n",
      "*******************     Lead time 6     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "100.25133054478357 +- 181.00564657343722\n",
      "Mean RMSE with testing data - Persistence\n",
      "149.77746834844268 +- 270.6962810970926\n",
      "*******************     Lead time 7     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "103.45136181153609 +- 185.80337397232566\n",
      "Mean RMSE with testing data - Persistence\n",
      "159.60545845774672 +- 284.95967429502997\n",
      "*******************     Lead time 8     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "105.98219394510235 +- 189.4283434563724\n",
      "Mean RMSE with testing data - Persistence\n",
      "169.0739008031674 +- 298.8606259213634\n",
      "*******************     Lead time 9     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "107.77000681018379 +- 191.20238779990515\n",
      "Mean RMSE with testing data - Persistence\n",
      "178.18772775645525 +- 312.3710063626479\n",
      "*******************     Lead time 10     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "109.27665440028217 +- 193.23757440572433\n",
      "Mean RMSE with testing data - Persistence\n",
      "186.4387677456621 +- 324.1197528679833\n",
      "*******************     Lead time 11     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "110.08673351017165 +- 194.01088461604908\n",
      "Mean RMSE with testing data - Persistence\n",
      "193.67677658258384 +- 334.85034457777834\n",
      "*******************     Lead time 12     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "110.78835888237894 +- 194.6824513152351\n",
      "Mean RMSE with testing data - Persistence\n",
      "200.32998646154147 +- 343.7878046378051\n",
      "*******************     Lead time 13     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "111.3928069461525 +- 195.34347465729698\n",
      "Mean RMSE with testing data - Persistence\n",
      "206.51543766766818 +- 351.4327196584504\n",
      "*******************     Lead time 14     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "111.93924349220879 +- 196.06446055577263\n",
      "Mean RMSE with testing data - Persistence\n",
      "212.35298561146928 +- 358.3893891527145\n",
      "*******************     Lead time 15     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "112.61436892359005 +- 196.83915044082573\n",
      "Mean RMSE with testing data - Persistence\n",
      "218.3251307542433 +- 365.56173747786585\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "RMSE - ConvLSTM       73.72692796577996 + - 142.4746987450842\n",
      "RMSE - Climatology    183.7509720006571 + - 280.49158911076336\n",
      "RMSE - Persistence    91.5997479178463 + - 180.9916370668999\n",
      "*******************    2     *******************\n",
      "RMSE - ConvLSTM       79.14728498817364 + - 156.67751947299126\n",
      "RMSE - Climatology    183.30330023934812 + - 273.347176573773\n",
      "RMSE - Persistence    82.42039524259681 + - 160.9626777381962\n",
      "*******************    3     *******************\n",
      "RMSE - ConvLSTM       73.5956399468034 + - 139.44440360159578\n",
      "RMSE - Climatology    199.89157834353045 + - 304.6694702911925\n",
      "RMSE - Persistence    83.91081918286545 + - 157.80740953153088\n",
      "*******************    4     *******************\n",
      "RMSE - ConvLSTM       68.30465673493484 + - 133.92397814508976\n",
      "RMSE - Climatology    207.80614115696767 + - 313.8306190145979\n",
      "RMSE - Persistence    76.4423650624924 + - 151.35030443127886\n",
      "*******************    5     *******************\n",
      "RMSE - ConvLSTM       54.12078060752441 + - 103.88016988475407\n",
      "RMSE - Climatology    182.62746359795213 + - 261.6904610096677\n",
      "RMSE - Persistence    69.48336388255558 + - 138.76485236989456\n",
      "*******************    6     *******************\n",
      "RMSE - ConvLSTM       32.73604980730889 + - 76.59393174980794\n",
      "RMSE - Climatology    149.31103629366999 + - 224.27726748630144\n",
      "RMSE - Persistence    46.7494984784352 + - 108.87355954907032\n",
      "*******************    7     *******************\n",
      "RMSE - ConvLSTM       14.008495364627692 + - 36.24543923654127\n",
      "RMSE - Climatology    73.29991659481595 + - 129.15866781119783\n",
      "RMSE - Persistence    23.531599159186072 + - 60.06396625220598\n",
      "*******************    8     *******************\n",
      "RMSE - ConvLSTM       10.781327841562465 + - 28.25584739983551\n",
      "RMSE - Climatology    46.699229356606295 + - 95.94623801740428\n",
      "RMSE - Persistence    10.98501001991176 + - 29.099367694308427\n",
      "*******************    9     *******************\n",
      "RMSE - ConvLSTM       8.24984088761451 + - 21.044692422102983\n",
      "RMSE - Climatology    42.771801967839494 + - 84.80483251772198\n",
      "RMSE - Persistence    5.635869541447041 + - 14.67266230343604\n",
      "*******************    10     *******************\n",
      "RMSE - ConvLSTM       20.810639913031345 + - 41.528473617324444\n",
      "RMSE - Climatology    72.14880256647942 + - 122.6752667121632\n",
      "RMSE - Persistence    14.224962737838037 + - 31.34599103819741\n",
      "*******************    11     *******************\n",
      "RMSE - ConvLSTM       35.155832307103196 + - 74.75945927954947\n",
      "RMSE - Climatology    147.2286115015819 + - 210.92864385138375\n",
      "RMSE - Persistence    30.528658826634373 + - 76.59773465361295\n",
      "*******************    12     *******************\n",
      "RMSE - ConvLSTM       55.78744766189692 + - 109.72940783061028\n",
      "RMSE - Climatology    166.09625619036072 + - 245.80172186617085\n",
      "RMSE - Persistence    66.53724354925671 + - 137.47315764613458\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with RMSE                 ########\n",
    "    ########              RMSE over time, and sum over domain                ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    RMSE_climatology, RMSE_climatology_std  = RMSE(climatology_seq * sic_max,sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_climatology = np.mean(RMSE_climatology)\n",
    "    RMSE_climatology_std = np.mean(RMSE_climatology_std)\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    RMSE_persist_0, RMSE_persist_0_std = RMSE(sic_exp_norm[-test_year*12*4-1:-1,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_persist_1, RMSE_persist_1_std = RMSE(sic_exp_norm[-test_year*12*4-1:-2,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_persist_2, RMSE_persist_2_std = RMSE(sic_exp_norm[-test_year*12*4-1:-3,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_persist_3, RMSE_persist_3_std = RMSE(sic_exp_norm[-test_year*12*4-1:-4,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_persist_4, RMSE_persist_4_std = RMSE(sic_exp_norm[-test_year*12*4-1:-5,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_persist_5, RMSE_persist_5_std = RMSE(sic_exp_norm[-test_year*12*4-1:-6,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_persist_6, RMSE_persist_6_std = RMSE(sic_exp_norm[-test_year*12*4-1:-7,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_persist_7, RMSE_persist_7_std = RMSE(sic_exp_norm[-test_year*12*4-1:-8,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_persist_8, RMSE_persist_8_std = RMSE(sic_exp_norm[-test_year*12*4-1:-9,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_persist_9, RMSE_persist_9_std = RMSE(sic_exp_norm[-test_year*12*4-1:-10,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_persist_10, RMSE_persist_10_std = RMSE(sic_exp_norm[-test_year*12*4-1:-11,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_persist_11, RMSE_persist_11_std = RMSE(sic_exp_norm[-test_year*12*4-1:-12,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_persist_12, RMSE_persist_12_std = RMSE(sic_exp_norm[-test_year*12*4-1:-13,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_persist_13, RMSE_persist_13_std = RMSE(sic_exp_norm[-test_year*12*4-1:-14,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_persist_14, RMSE_persist_14_std = RMSE(sic_exp_norm[-test_year*12*4-1:-15,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_persist_15, RMSE_persist_15_std = RMSE(sic_exp_norm[-test_year*12*4-1:-16,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print('##############################################################')\n",
    "    print('########   land-sea correction for sic prediction    #########')\n",
    "    print('##############################################################')\n",
    "    # correction for float point at 0\n",
    "    lead_pred_sic[lead_pred_sic<0] = 0\n",
    "    # extend the dimension of sea ice mask\n",
    "    sea_ice_mask_test = np.repeat(sea_ice_mask_barents[np.newaxis,:,:],test_year*48,0)\n",
    "    # correct the land cells in the prediction\n",
    "    for i in range(step_lead):\n",
    "        lead_pred_sic[:,i,:,:] = lead_pred_sic[:,i,:,:] * sea_ice_mask_test\n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    RMSE_ConvLSTM_0, RMSE_ConvLSTM_0_std = RMSE(lead_pred_sic[:,0,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_1, RMSE_ConvLSTM_1_std = RMSE(lead_pred_sic[:-1,1,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_2, RMSE_ConvLSTM_2_std = RMSE(lead_pred_sic[:-2,2,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_3, RMSE_ConvLSTM_3_std = RMSE(lead_pred_sic[:-3,3,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_4, RMSE_ConvLSTM_4_std = RMSE(lead_pred_sic[:-4,4,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_5, RMSE_ConvLSTM_5_std = RMSE(lead_pred_sic[:-5,5,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_6, RMSE_ConvLSTM_6_std = RMSE(lead_pred_sic[:-6,6,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_7, RMSE_ConvLSTM_7_std = RMSE(lead_pred_sic[:-7,7,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_8, RMSE_ConvLSTM_8_std = RMSE(lead_pred_sic[:-8,8,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_9, RMSE_ConvLSTM_9_std = RMSE(lead_pred_sic[:-9,9,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_10, RMSE_ConvLSTM_10_std = RMSE(lead_pred_sic[:-10,10,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_11, RMSE_ConvLSTM_11_std = RMSE(lead_pred_sic[:-11,11,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_12, RMSE_ConvLSTM_12_std = RMSE(lead_pred_sic[:-12,12,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_13, RMSE_ConvLSTM_13_std = RMSE(lead_pred_sic[:-13,13,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_14, RMSE_ConvLSTM_14_std = RMSE(lead_pred_sic[:-14,14,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_15, RMSE_ConvLSTM_15_std = RMSE(lead_pred_sic[:-15,15,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_0),\"+-\",np.mean(RMSE_ConvLSTM_0_std))\n",
    "    print(\"Mean RMSE with testing data - Climatology\")\n",
    "    print(RMSE_climatology,\"+-\",RMSE_climatology_std)\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_0),\"+-\",np.mean(RMSE_persist_0_std))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_1),\"+-\",np.mean(RMSE_ConvLSTM_1_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_1),\"+-\",np.mean(RMSE_persist_1_std))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_2),\"+-\",np.mean(RMSE_ConvLSTM_2_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_2),\"+-\",np.mean(RMSE_persist_2_std))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_3),\"+-\",np.mean(RMSE_ConvLSTM_3_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_3),\"+-\",np.mean(RMSE_persist_3_std))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_4),\"+-\",np.mean(RMSE_ConvLSTM_4_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_4),\"+-\",np.mean(RMSE_persist_4_std))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_5),\"+-\",np.mean(RMSE_ConvLSTM_5_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_5),\"+-\",np.mean(RMSE_persist_5_std))\n",
    "    print(\"*******************     Lead time 6     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_6),\"+-\",np.mean(RMSE_ConvLSTM_6_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_6),\"+-\",np.mean(RMSE_persist_6_std))\n",
    "    print(\"*******************     Lead time 7     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_7),\"+-\",np.mean(RMSE_ConvLSTM_7_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_7),\"+-\",np.mean(RMSE_persist_7_std))\n",
    "    print(\"*******************     Lead time 8     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_8),\"+-\",np.mean(RMSE_ConvLSTM_8_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_8),\"+-\",np.mean(RMSE_persist_8_std))\n",
    "    print(\"*******************     Lead time 9     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_9),\"+-\",np.mean(RMSE_ConvLSTM_9_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_9),\"+-\",np.mean(RMSE_persist_9_std))\n",
    "    print(\"*******************     Lead time 10     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_10),\"+-\",np.mean(RMSE_ConvLSTM_10_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_10),\"+-\",np.mean(RMSE_persist_10_std))\n",
    "    print(\"*******************     Lead time 11     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_11),\"+-\",np.mean(RMSE_ConvLSTM_11_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_11),\"+-\",np.mean(RMSE_persist_11_std))\n",
    "    print(\"*******************     Lead time 12     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_12),\"+-\",np.mean(RMSE_ConvLSTM_12_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_12),\"+-\",np.mean(RMSE_persist_12_std))\n",
    "    print(\"*******************     Lead time 13     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_13),\"+-\",np.mean(RMSE_ConvLSTM_13_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_13),\"+-\",np.mean(RMSE_persist_13_std))\n",
    "    print(\"*******************     Lead time 14     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_14),\"+-\",np.mean(RMSE_ConvLSTM_14_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_14),\"+-\",np.mean(RMSE_persist_14_std))\n",
    "    print(\"*******************     Lead time 15     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_15),\"+-\",np.mean(RMSE_ConvLSTM_15_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_15),\"+-\",np.mean(RMSE_persist_15_std))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_rmse_pred_16weeks.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_0),np.mean(RMSE_ConvLSTM_0_std)))\n",
    "    f.write(\"RMSE - Climatology    {} + - {}\\n\".format(RMSE_climatology, RMSE_climatology_std))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_0),np.mean(RMSE_persist_0_std)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_1),np.mean(RMSE_ConvLSTM_1_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_1),np.mean(RMSE_persist_1_std)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_2),np.mean(RMSE_ConvLSTM_2_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_2),np.mean(RMSE_persist_2_std)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_3),np.mean(RMSE_ConvLSTM_3_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_3),np.mean(RMSE_persist_3_std)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_4),np.mean(RMSE_ConvLSTM_4_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_4),np.mean(RMSE_persist_4_std)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_5),np.mean(RMSE_ConvLSTM_5_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_5),np.mean(RMSE_persist_5_std)))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_6),np.mean(RMSE_ConvLSTM_6_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_6),np.mean(RMSE_persist_6_std)))\n",
    "    f.write(\"*******************     Lead time 7     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_7),np.mean(RMSE_ConvLSTM_7_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_7),np.mean(RMSE_persist_7_std)))\n",
    "    f.write(\"*******************     Lead time 8     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_8),np.mean(RMSE_ConvLSTM_8_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_8),np.mean(RMSE_persist_8_std)))\n",
    "    f.write(\"*******************     Lead time 9     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_9),np.mean(RMSE_ConvLSTM_9_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_9),np.mean(RMSE_persist_9_std)))\n",
    "    f.write(\"*******************     Lead time 10     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_10),np.mean(RMSE_ConvLSTM_10_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_10),np.mean(RMSE_persist_10_std)))\n",
    "    f.write(\"*******************     Lead time 11     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_11),np.mean(RMSE_ConvLSTM_11_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_11),np.mean(RMSE_persist_11_std)))\n",
    "    f.write(\"*******************     Lead time 12     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_12),np.mean(RMSE_ConvLSTM_12_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_12),np.mean(RMSE_persist_12_std)))\n",
    "    f.write(\"*******************     Lead time 13     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_13),np.mean(RMSE_ConvLSTM_13_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_13),np.mean(RMSE_persist_13_std)))\n",
    "    f.write(\"*******************     Lead time 14     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_14),np.mean(RMSE_ConvLSTM_14_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_14),np.mean(RMSE_persist_14_std)))\n",
    "    f.write(\"*******************     Lead time 15     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_15),np.mean(RMSE_ConvLSTM_15_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_15),np.mean(RMSE_persist_15_std)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = week2month(lead_pred_sic[:,0,:,:], i)\n",
    "        persist_monthly_series = week2month(sic_exp_norm[-test_year*12*4-1:-1,:,:], i)\n",
    "        climatology_monthly_series = week2month(climatology_seq, i)\n",
    "        truth_monthly_series = week2month(sic_exp_norm[-test_year*12*4:,:,:], i)\n",
    "        \n",
    "        rmse_ConvLSTM_monthly, rmse_ConvLSTM_monthly_std = RMSE(ConvLSTM_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_persist_monthly, rmse_persist_monthly_std = RMSE(persist_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_climatology_monthly, rmse_climatology_monthly_std = RMSE(climatology_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        \n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"RMSE - ConvLSTM       {} + - {}\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        print(\"RMSE - Climatology    {} + - {}\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        print(\"RMSE - Persistence    {} + - {}\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        f.write(\"RMSE - Climatology    {} + - {}\\n\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        \n",
    "    #f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating accuracy/recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating accuracy/recall/precision  *********************')\n",
    "    def accuracy(pred, label):\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        boolean = (pred==label)\n",
    "        accu_seq = np.mean(np.mean(boolean.astype(float),2),1)\n",
    "        accu_spa = np.mean(boolean.astype(float),0)\n",
    "        \n",
    "        return accu_seq, accu_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########          transfer the sea ice fields into binary data           ########\n",
    "    #################################################################################\n",
    "    criterion_0 = 0.15 # ice concentration below the threshold is regarded as no ice\n",
    "    # remove the area weight\n",
    "    sic_exp_denorm = np.zeros(sic_exp_norm.shape, dtype=float)\n",
    "    lead_pred_sic_denorm = np.zeros(lead_pred_sic.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        lead_pred_sic_denorm[:,:,i,:] = lead_pred_sic[:,:,i,:] /dx[i+12] * dx[35]\n",
    "        sic_exp_denorm[:,i,:] = sic_exp_norm[:,i,:] / dx[i+12] * dx[35]\n",
    "    # turn sea ice fields into binary data\n",
    "    lead_pred_sic_bin = lead_pred_sic_denorm[:]\n",
    "    sic_exp_bin = sic_exp_denorm[:]\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin <= criterion_0] = 0\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin > criterion_0] = 1\n",
    "    sic_exp_bin[sic_exp_bin <= criterion_0] = 0\n",
    "    sic_exp_bin[sic_exp_bin > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    lead_pred_sic_bin = lead_pred_sic_bin.astype(int)\n",
    "    sic_exp_bin = sic_exp_bin.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9769345238095237\n",
      "Total accuracy with testing data - Climatology\n",
      "0.8557477678571428\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9749193948412698\n",
      "*******************     Lead time 1     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9601759224632263\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9597396222887059\n",
      "*******************     Lead time 2     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9513784461152882\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9483787593984963\n",
      "*******************     Lead time 3     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9448105631141346\n",
      "Total accuracy with testing data - Persistence\n",
      "0.936889802217183\n",
      "*******************     Lead time 4     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9399537740628165\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9253340298885512\n",
      "*******************     Lead time 5     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9363182773109244\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9133960402342755\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "accuracy - ConvLSTM       0.9552176339285714\n",
      "accuracy - Climatology    0.8189174107142857\n",
      "accuracy - Persistence    0.9498697916666667\n",
      "*******************    2     *******************\n",
      "accuracy - ConvLSTM       0.9608444940476191\n",
      "accuracy - Climatology    0.8280784970238095\n",
      "accuracy - Persistence    0.9636811755952381\n",
      "*******************    3     *******************\n",
      "accuracy - ConvLSTM       0.9657273065476191\n",
      "accuracy - Climatology    0.8337983630952381\n",
      "accuracy - Persistence    0.9605189732142858\n",
      "*******************    4     *******************\n",
      "accuracy - ConvLSTM       0.9718191964285714\n",
      "accuracy - Climatology    0.8166852678571428\n",
      "accuracy - Persistence    0.968563988095238\n",
      "*******************    5     *******************\n",
      "accuracy - ConvLSTM       0.9719587053571428\n",
      "accuracy - Climatology    0.7997116815476191\n",
      "accuracy - Persistence    0.9637276785714285\n",
      "*******************    6     *******************\n",
      "accuracy - ConvLSTM       0.9825148809523809\n",
      "accuracy - Climatology    0.8109654017857143\n",
      "accuracy - Persistence    0.9734933035714286\n",
      "*******************    7     *******************\n",
      "accuracy - ConvLSTM       0.9916759672619048\n",
      "accuracy - Climatology    0.905087425595238\n",
      "accuracy - Persistence    0.9868396577380952\n",
      "*******************    8     *******************\n",
      "accuracy - ConvLSTM       0.9970238095238095\n",
      "accuracy - Climatology    0.9574032738095237\n",
      "accuracy - Persistence    0.9961402529761905\n",
      "*******************    9     *******************\n",
      "accuracy - ConvLSTM       0.9979073660714286\n",
      "accuracy - Climatology    0.9603329613095238\n",
      "accuracy - Persistence    0.998046875\n",
      "*******************    10     *******************\n",
      "accuracy - ConvLSTM       0.9924665178571429\n",
      "accuracy - Climatology    0.9025762648809523\n",
      "accuracy - Persistence    0.9930245535714286\n",
      "*******************    11     *******************\n",
      "accuracy - ConvLSTM       0.9716331845238095\n",
      "accuracy - Climatology    0.8140811011904763\n",
      "accuracy - Persistence    0.9844680059523809\n",
      "*******************    12     *******************\n",
      "accuracy - ConvLSTM       0.9644252232142858\n",
      "accuracy - Climatology    0.8213355654761905\n",
      "accuracy - Persistence    0.9606584821428572\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with accuracy               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    accu_climatology, _ = accuracy(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    accu_persist_0, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_persist_1, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_persist_2, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_persist_3, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_persist_4, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_persist_5, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    accu_ConvLSTM_0, _ = accuracy(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_ConvLSTM_1, _ = accuracy(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_ConvLSTM_2, _ = accuracy(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_ConvLSTM_3, _ = accuracy(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_ConvLSTM_4, _ = accuracy(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_ConvLSTM_5, _ = accuracy(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_0))\n",
    "    print(\"Total accuracy with testing data - Climatology\")\n",
    "    print(np.mean(accu_climatology))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_1))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_2))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_3))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_4))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_5))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_accuracy_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_0)))\n",
    "    f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accu_climatology)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_1)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_2)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_3)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_4)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_5)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"accuracy - ConvLSTM       {}\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Climatology    {}\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Persistence    {}\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating recall/precision  *********************')\n",
    "    # positive is sea ice = 1\n",
    "    \n",
    "    def recall(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total actual positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False negative (is 1 but predict 0)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 0] = 2\n",
    "        label_dummy_2[label == 1] = 2\n",
    "        # count False Positive events\n",
    "        falseNegative = (pred_dummy_2 == label_dummy_2)\n",
    "\n",
    "#         recall_seq = np.mean(np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                     (truePositive.astype(float) + falseNegative.astype(float))),2),1)\n",
    "        \n",
    "        recall_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                       np.sum(np.sum(falseNegative.astype(float),2),1))\n",
    "        \n",
    "#         recall_spa = np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                           (truePositive.astype(float) + falseNegative.astype(float))),0)\n",
    "        \n",
    "        recall_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                             np.sum(falseNegative.astype(float),0))\n",
    "        \n",
    "        #return recall_seq, recall_spa\n",
    "        return np.nan_to_num(recall_seq), np.nan_to_num(recall_spa)\n",
    "    \n",
    "    def precision(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total predicted positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False positive (is 0 but predict 1)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 1] = 2\n",
    "        label_dummy_2[label == 0] = 2\n",
    "        # count False Positive events\n",
    "        falsePositive = (pred_dummy_2 == label_dummy_2)\n",
    "        \n",
    "        prec_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                     np.sum(np.sum(falsePositive.astype(float),2),1))\n",
    "        \n",
    "        prec_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                           np.sum(falsePositive.astype(float),0))\n",
    "        \n",
    "        return np.nan_to_num(prec_seq), np.nan_to_num(prec_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7392617514440815\n",
      "Total recall with testing data - Climatology\n",
      "0.7682106239728338\n",
      "Total recall with testing data - Persistence\n",
      "0.6956084994582069\n",
      "*******************     Lead time 1     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7379417966565415\n",
      "Total recall with testing data - Persistence\n",
      "0.6436574679557777\n",
      "*******************     Lead time 2     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7292935370923855\n",
      "Total recall with testing data - Persistence\n",
      "0.610596193945815\n",
      "*******************     Lead time 3     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7174284530053671\n",
      "Total recall with testing data - Persistence\n",
      "0.5745854442827217\n",
      "*******************     Lead time 4     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7011387875572724\n",
      "Total recall with testing data - Persistence\n",
      "0.5492875256810292\n",
      "*******************     Lead time 5     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6975171903393673\n",
      "Total recall with testing data - Persistence\n",
      "0.5259148468998884\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "recall - ConvLSTM       0.9092685496062015\n",
      "recall - Climatology    0.9985675754922358\n",
      "recall - Persistence    0.8314841347620789\n",
      "*******************    2     *******************\n",
      "recall - ConvLSTM       0.9674006886333054\n",
      "recall - Climatology    0.9988817097415507\n",
      "recall - Persistence    0.9345966147874042\n",
      "*******************    3     *******************\n",
      "recall - ConvLSTM       0.9670503489544954\n",
      "recall - Climatology    0.9997340425531915\n",
      "recall - Persistence    0.9106219389571815\n",
      "*******************    4     *******************\n",
      "recall - ConvLSTM       0.964980600217292\n",
      "recall - Climatology    0.998480953251909\n",
      "recall - Persistence    0.967307576852602\n",
      "*******************    5     *******************\n",
      "recall - ConvLSTM       0.9777823992531354\n",
      "recall - Climatology    0.9983039583340307\n",
      "recall - Persistence    0.980571197002486\n",
      "*******************    6     *******************\n",
      "recall - ConvLSTM       0.922736590498304\n",
      "recall - Climatology    0.99876677278424\n",
      "recall - Persistence    0.97613758406689\n",
      "*******************    7     *******************\n",
      "recall - ConvLSTM       0.6463793805893566\n",
      "recall - Climatology    0.6825157351595051\n",
      "recall - Persistence    0.731919166058951\n",
      "*******************    8     *******************\n",
      "recall - ConvLSTM       0.24602747227713057\n",
      "recall - Climatology    0.17666547941988756\n",
      "recall - Persistence    0.2848559532321151\n",
      "*******************    9     *******************\n",
      "recall - ConvLSTM       0.24091735537190082\n",
      "recall - Climatology    0.08532851239669421\n",
      "recall - Persistence    0.24395041322314048\n",
      "*******************    10     *******************\n",
      "recall - ConvLSTM       0.3128398914299367\n",
      "recall - Climatology    0.31790314876598696\n",
      "recall - Persistence    0.2090523667871815\n",
      "*******************    11     *******************\n",
      "recall - ConvLSTM       0.7814042954097846\n",
      "recall - Climatology    0.96659537082369\n",
      "recall - Persistence    0.5508195922988457\n",
      "*******************    12     *******************\n",
      "recall - ConvLSTM       0.9343534450881344\n",
      "recall - Climatology    0.9967842289510855\n",
      "recall - Persistence    0.7259854554696066\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with recall               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    recall_climatology, _ = recall(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    recall_persist_0, _ = recall(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_persist_1, _ = recall(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_persist_2, _ = recall(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_persist_3, _ = recall(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_persist_4, _ = recall(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_persist_5, _ = recall(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    recall_ConvLSTM_0, _ = recall(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_ConvLSTM_1, _ = recall(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_ConvLSTM_2, _ = recall(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_ConvLSTM_3, _ = recall(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_ConvLSTM_4, _ = recall(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_ConvLSTM_5, _ = recall(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_0))\n",
    "    print(\"Total recall with testing data - Climatology\")\n",
    "    print(np.mean(recall_climatology))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_1))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_2))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_3))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_4))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_5))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_recall_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total recall with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_0)))\n",
    "    f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall_climatology)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_1)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_2)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_3)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_4)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_5)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"recall - ConvLSTM       {}\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Climatology    {}\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Persistence    {}\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6714406335530798\n",
      "Total precision with testing data - Climatology\n",
      "0.4079385856750705\n",
      "Total precision with testing data - Persistence\n",
      "0.690738761572815\n",
      "*******************     Lead time 1     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6006687132066136\n",
      "Total precision with testing data - Persistence\n",
      "0.6356268444470853\n",
      "*******************     Lead time 2     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5745552161028654\n",
      "Total precision with testing data - Persistence\n",
      "0.6105407500493609\n",
      "*******************     Lead time 3     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5530044956414085\n",
      "Total precision with testing data - Persistence\n",
      "0.5833019876273877\n",
      "*******************     Lead time 4     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5368238977509635\n",
      "Total precision with testing data - Persistence\n",
      "0.553914255338053\n",
      "*******************     Lead time 5     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5252335428727974\n",
      "Total precision with testing data - Persistence\n",
      "0.5249623371537447\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "precision - ConvLSTM       0.8835184239613516\n",
      "precision - Climatology    0.5559542418518143\n",
      "precision - Persistence    0.9267649426825348\n",
      "*******************    2     *******************\n",
      "precision - ConvLSTM       0.8929249084611475\n",
      "precision - Climatology    0.6083904361457534\n",
      "precision - Persistence    0.9307656748230683\n",
      "*******************    3     *******************\n",
      "precision - ConvLSTM       0.9141075339712517\n",
      "precision - Climatology    0.625218252792524\n",
      "precision - Persistence    0.9461523115764381\n",
      "*******************    4     *******************\n",
      "precision - ConvLSTM       0.9384408450944679\n",
      "precision - Climatology    0.6066702234474803\n",
      "precision - Persistence    0.9246785425436064\n",
      "*******************    5     *******************\n",
      "precision - ConvLSTM       0.8783647694458196\n",
      "precision - Climatology    0.4990962228132416\n",
      "precision - Persistence    0.8529426820551145\n",
      "*******************    6     *******************\n",
      "precision - ConvLSTM       0.8620791128263219\n",
      "precision - Climatology    0.33860928286279546\n",
      "precision - Persistence    0.7542750415249005\n",
      "*******************    7     *******************\n",
      "precision - ConvLSTM       0.5522259686187484\n",
      "precision - Climatology    0.25354943889363424\n",
      "precision - Persistence    0.53082042568696\n",
      "*******************    8     *******************\n",
      "precision - ConvLSTM       0.2572585366200596\n",
      "precision - Climatology    0.25\n",
      "precision - Persistence    0.2554656667257474\n",
      "*******************    9     *******************\n",
      "precision - ConvLSTM       0.23643029925913314\n",
      "precision - Climatology    0.23414179104477612\n",
      "precision - Persistence    0.23465289256198346\n",
      "*******************    10     *******************\n",
      "precision - ConvLSTM       0.3043403695965592\n",
      "precision - Climatology    0.2184269005947958\n",
      "precision - Persistence    0.24059244560980797\n",
      "*******************    11     *******************\n",
      "precision - ConvLSTM       0.5379456875927404\n",
      "precision - Climatology    0.24949192107359747\n",
      "precision - Persistence    0.7659697420634921\n",
      "*******************    12     *******************\n",
      "precision - ConvLSTM       0.7996511471893575\n",
      "precision - Climatology    0.45571431658043265\n",
      "precision - Persistence    0.9257847710201259\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with precision              ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    prec_climatology, _ = precision(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    prec_persist_0, _ = precision(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_persist_1, _ = precision(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_persist_2, _ = precision(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_persist_3, _ = precision(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_persist_4, _ = precision(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_persist_5, _ = precision(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    prec_ConvLSTM_0, _ = precision(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_ConvLSTM_1, _ = precision(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_ConvLSTM_2, _ = precision(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_ConvLSTM_3, _ = precision(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_ConvLSTM_4, _ = precision(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_ConvLSTM_5, _ = precision(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_0))\n",
    "    print(\"Total precision with testing data - Climatology\")\n",
    "    print(np.mean(prec_climatology))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_1))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_2))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_3))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_4))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_5))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_precision_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total precision with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_0)))\n",
    "    f.write(\"precision - Climatology    {}\\n\".format(np.mean(prec_climatology)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_1)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_2)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_3)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_4)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_5)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"precision - ConvLSTM       {}\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Climatology    {}\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Persistence    {}\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Climatology    {}\\n\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Persistence    {}\\n\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
