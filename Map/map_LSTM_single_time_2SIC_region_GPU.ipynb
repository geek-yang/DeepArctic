{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Predict the Spatial Sea Ice Concentration with ConvLSTM at weekly time scale** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2019.05.21 ** <br>\n",
    "** Last Update  : 2019.05.21 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, deepclim, matplotlib <br>\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. We also include many climate index (to represent the forcing from atmosphere). The convolutional Long Short Time Memory neural network is used to deal with this spatial-temporal sequence problem. We use Pytorch as the deep learning framework. <br>\n",
    "\n",
    "Return Values   : Time series and figures <br>\n",
    "\n",
    "Here is the information of climate index in details:<br>\n",
    "**NAO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/nao.shtml <br>\n",
    "**ENSO - NINO 3.4 SST** @ 1950 Jan - 2018 Jan (817 records) <br>\n",
    "https://www.esrl.noaa.gov/psd/gcos_wgsp/Timeseries/Nino34/ <br>\n",
    "**AO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/ao.shtml <br>\n",
    "**AMO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "AMO unsmoothed, detrended from the Kaplan SST V2. The result is standarised.<br>\n",
    "https://www.esrl.noaa.gov/psd/data/timeseries/AMO/<br>\n",
    "\n",
    "(All the NOAA index shown above are given by NCEP/NCAR Reanalysis (CDAS))<br>\n",
    "\n",
    "**PDO** @ 1900 Jan - 2018 Feb (1418 records)<br>\n",
    "This PDO index comes from University of Washington, it contains SST data from the following 3 datasets:<br>\n",
    "- UKMO Historical SST data set for 1900-81;\n",
    "- Reynold's Optimally Interpolated SST (V1) for January 1982-Dec 2001)\n",
    "- OI SST Version 2 (V2) beginning January 2002 -<br>\n",
    "\n",
    "http://research.jisao.washington.edu/pdo/PDO.latest<br>\n",
    "\n",
    "The regionalization adopted here follows that of the MASIE (Multisensor Analyzed Sea Ice Extent) product available from the National Snow and Ice Data Center:<br>\n",
    "https://nsidc.org/data/masie/browse_regions<br>\n",
    "It is given by paper J.Walsh et. al., 2019. Benchmark seasonal prediction skill estimates based on regional indices.<br>\n",
    "\n",
    "The method comes from the study by Shi et. al. (2015) Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/Scripts/DeepClim\")\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import deepclim\n",
    "import deepclim.preprocess\n",
    "import deepclim.deepSeries\n",
    "import deepclim.deepArray_GPU\n",
    "#import deepclim.function\n",
    "\n",
    "# for visualization\n",
    "import deepclim.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import iris # also helps with regriding\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2264670,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data ** <br>\n",
    "Time span of each product included: <br>\n",
    "** Reanalysis ** <br>\n",
    "- **ERA-Interim** 1979 - 2016 (ECMWF)\n",
    "- **ORAS4**       1958 - 2014 (ECMWF)\n",
    "\n",
    "** Index ** <br>\n",
    "- **NINO3.4**     1950 - 2017 (NOAA)\n",
    "- **AO**          1950 - 2017 (NOAA)\n",
    "- **NAO**         1950 - 2017 (NOAA)\n",
    "- **AMO**         1950 - 2017 (NOAA)\n",
    "- **PDO**         1950 - 2017 (University of Washington)\n",
    "\n",
    "!! These index are given by NCEP/NCAR Reanalysis (CDAS) <br>\n",
    "\n",
    "\n",
    "Alternative (not in use yet) <br>\n",
    "** Reanalysis ** <br>\n",
    "- **MERRA2**      1980 - 2016 (NASA)\n",
    "- **JRA55**       1979 - 2015 (JMA)\n",
    "- **GLORYS2V3**   1993 - 2014 (Mercartor Ocean)\n",
    "- **SODA3**       1980 - 2015\n",
    "- **PIOMASS**     1980 - 2015\n",
    "\n",
    "** Observations ** <br>\n",
    "- **NSIDC**       1958 - 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################   path  ######################################\n",
    "# please specify data path\n",
    "datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ERA-Interim'\n",
    "datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/ORAS4'\n",
    "datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/Maps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    dataset_ERAI_fields_sic = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'sic_weekly_erai_1979_2017.nc'))\n",
    "    dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "                                 'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    # OHC interpolated on ERA-Interim grid\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                'ohc_monthly_oras2erai_1978_2017.nc'))\n",
    "    dataset_index = Dataset(os.path.join(datapath_clim_index,\n",
    "                            'index_climate_monthly_regress_1950_2017.nc'))\n",
    "    #dataset_ERAI_fields_flux = Dataset(os.path.join(datapath_ERAI_fields,\n",
    "    #                                  'surface_erai_monthly_regress_1979_2017_radiation.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    # ********************************** data gallery *************************************** #\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2013\n",
    "    # validation: 2014 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim)\n",
    "    SIC_ERAI = dataset_ERAI_fields_sic.variables['sic'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    #SIC_ERAI_mask = np.ma.getmaskarray(SIC_ERAI[0,:,:])\n",
    "    year_ERAI = dataset_ERAI_fields_sic.variables['year'][:-1]\n",
    "    week_ERAI = dataset_ERAI_fields_sic.variables['week'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_sic.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_sic.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # OHC (ORAS4)\n",
    "    # from 1978 - 2017 (for interpolation) / from 90 N upto 40 N\n",
    "    OHC_300_ORAS4 = dataset_ORAS4_OHC.variables['OHC'][:-1,:,:67,:]/1000 # unit Peta Joule\n",
    "    latitude_ORAS4 = dataset_ORAS4_OHC.variables['latitude'][:]\n",
    "    longitude_ORAS4 = dataset_ORAS4_OHC.variables['longitude'][:]\n",
    "    mask_OHC = np.ma.getmask(OHC_300_ORAS4[0,0,:,:])\n",
    "    # AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    AO = dataset_index.variables['AO'][348:-1] # from 1979 - 2017\n",
    "    NAO = dataset_index.variables['NAO'][348:-1]\n",
    "    NINO = dataset_index.variables['NINO'][348:-1]\n",
    "    AMO = dataset_index.variables['AMO'][348:-1]\n",
    "    PDO = dataset_index.variables['PDO'][348:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # *************************   regionalization sea mask   ***************************** #\n",
    "    print ('*********************** create mask *************************')\n",
    "    # W:-156 E:-124 N:80 S:67\n",
    "    mask_Beaufort = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:-156 N:80 S:66\n",
    "    mask_Chukchi = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:146 E:180 N:80 S:67\n",
    "    mask_EastSiberian = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:100 E:146 N:80 S:67\n",
    "    mask_Laptev = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:60 E:100 N:80 S:67\n",
    "    mask_Kara = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:18 E:60 N:80 S:64\n",
    "    mask_Barents = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-44 E:18 N:80 S:55\n",
    "    mask_Greenland = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:180 N:90 S:80\n",
    "    mask_CenArctic = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    print ('*********************** calc mask *************************')\n",
    "    mask_Beaufort[13:31,32:76] = 1\n",
    "\n",
    "    mask_Chukchi[13:32,0:32] = 1\n",
    "    mask_Chukchi[13:32,-1] = 1\n",
    "\n",
    "    mask_EastSiberian[13:31,434:479] = 1\n",
    "\n",
    "    mask_Laptev[13:31,374:434] = 1\n",
    "\n",
    "    mask_Kara[13:31,320:374] = 1\n",
    "\n",
    "    mask_Barents[13:36,264:320] = 1\n",
    "\n",
    "    mask_Greenland[13:47,179:264] = 1\n",
    "    mask_Greenland[26:47,240:264] = 0\n",
    "\n",
    "    mask_CenArctic[:13,:] = 1\n",
    "    print ('*********************** packing *************************')\n",
    "    mask_dict = {'Beaufort': mask_Beaufort[:,:],\n",
    "                 'Chukchi': mask_Chukchi[:,:],\n",
    "                 'EastSiberian': mask_EastSiberian[:,:],\n",
    "                 'Laptev': mask_Laptev[:,:],\n",
    "                 'Kara': mask_Kara[:,:],\n",
    "                 'Barents': mask_Barents[:,:],\n",
    "                 'Greenland': mask_Greenland[:,:],\n",
    "                 'CenArctic': mask_CenArctic[:,:]}\n",
    "    seas_namelist = ['Beaufort','Chukchi','EastSiberian','Laptev',\n",
    "                     'Kara', 'Barents', 'Greenland','CenArctic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # *************************  temporal interpolation matrix  ***************************** #\n",
    "    # interpolate from monthly to weekly\n",
    "    # original monthly data will be taken as the last week of the month\n",
    "    OHC_300_ORAS4_weekly_series = np.zeros(SIC_ERAI.reshape(len(year_ERAI)*48,len(latitude_ERAI),len(longitude_ERAI)).shape,\n",
    "                                           dtype=float)\n",
    "    OHC_300_ORAS4_series= deepclim.preprocess.operator.unfold(OHC_300_ORAS4)\n",
    "    # calculate the difference between two months\n",
    "    OHC_300_ORAS4_deviation_series = (OHC_300_ORAS4_series[1:,:,:] - OHC_300_ORAS4_series[:-1,:,:]) / 4\n",
    "    for i in np.arange(4):\n",
    "        OHC_300_ORAS4_weekly_series[3-i::4,:,:] = OHC_300_ORAS4_series[12:,:,:] - i * OHC_300_ORAS4_deviation_series[11:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # *************************  temporal interpolation single series  ***************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / 480\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,:,i,:] = SIC_ERAI[:,:,i,:]* dx[i] * dy / 1E+6 # unit km2\n",
    "    SIC_ERAI_area[SIC_ERAI_area<0] = 0 # switch the mask from -1 to 0\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    SIC_ERAI_area_series = deepclim.preprocess.operator.unfold(SIC_ERAI_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select the area between greenland and ice land for instance 60-70 N / 44-18 W\n",
    "    sic_exp = SIC_ERAI_area_series[:,18:26,212:228]\n",
    "    ohc_exp = OHC_300_ORAS4_weekly_series[:,18:26,212:228]\n",
    "    print(sic_exp.shape)\n",
    "    print(ohc_exp.shape)\n",
    "    print(latitude_ERAI[18:26])\n",
    "    print(longitude_ERAI[212:228])\n",
    "    print(latitude_ORAS4[18:26])\n",
    "    print(longitude_ORAS4[212:228])\n",
    "    #print(latitude_ERAI[26:40])\n",
    "    #print(longitude_ERAI[180:216])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('=========================   normalize data   ===========================')\n",
    "    sic_exp_norm = deepclim.preprocess.operator.normalize(sic_exp)\n",
    "    ohc_exp_norm = deepclim.preprocess.operator.normalize(ohc_exp)\n",
    "    print ('====================    A series of time (index)    ====================')\n",
    "    year = np.arange(1979,2017,1)\n",
    "    year_cycle = np.repeat(year,48)\n",
    "    month_cycle = np.repeat(np.arange(1,13,1),4)\n",
    "    month_cycle = np.tile(month_cycle,len(year))\n",
    "    month_cycle.astype(float)\n",
    "    month_2D = np.repeat(month_cycle[:,np.newaxis],8,1)\n",
    "    month_exp = np.repeat(month_2D[:,:,np.newaxis],16,2)\n",
    "    print ('===================  artificial data for evaluation ====================')\n",
    "    # calculate climatology of SIC\n",
    "#     seansonal_cycle_SIC = np.zeros(48,dtype=float)\n",
    "#     for i in np.arange(48):\n",
    "#         seansonal_cycle_SIC[i] = np.mean(SIC_ERAI_sum_norm[i::48],axis=0)\n",
    "    # weight for loss\n",
    "#     weight_month = np.array([0,1,1,\n",
    "#                              1,0,0,\n",
    "#                              1,1,1,\n",
    "#                              0,0,0])\n",
    "    #weight_loss = np.repeat(weight_month,4)\n",
    "    #weight_loss = np.tile(weight_loss,len(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(sic_exp.shape)\n",
    "    ax = plt.contourf(sic_exp[43,:,:])\n",
    "    print(sic_exp[443,:,:])\n",
    "    print(month_exp[0,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_input = np.stack((sic_exp_norm[1,:,:],\n",
    "                        ohc_exp_norm[1,:,:],\n",
    "                        month_exp[1,:,:]))\n",
    "    print(x_input[1,:,:])\n",
    "    print(x_input[:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************          prepare input tensor for LSTM           *********************')\n",
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    # specifications of neural network\n",
    "    input_channels = 3\n",
    "    hidden_channels = [3, 3, 2] # number of channels & hidden layers, the channels of last layer is the channels of output, too\n",
    "    kernel_size = 3\n",
    "    # here we input a sequence and predict the next step only\n",
    "    #step = 1 # how many steps to predict ahead\n",
    "    #effective_step = [0] # step to output\n",
    "    batch_size = 1\n",
    "    num_layers = 1\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 50\n",
    "    print (torch.__version__)\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    # standard LSTM input dimension\n",
    "    # [sequence_length(time step), batch_size, input_size]\n",
    "    # example : x = torch.autograd.Variable(torch.randn(6, 3, 5))\n",
    "    # prepare input array for LSTM input -> dimension\n",
    "    # take 3 year as test data\n",
    "    test_year = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  preview of input tensor  *********************')\n",
    "    #plt.plot(SIC_ERAI_sum)\n",
    "    #print(SIC_ERAI_sum_norm[:-test_year*12])\n",
    "    #print(x_input.shape)\n",
    "    #print(x_input[:,:,:])\n",
    "    sequence_len, _, _ = sic_exp_norm.shape\n",
    "    print(sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print ('*******************  module for calculating accuracy  *********************')\n",
    "#     def accuracy(out, labels):\n",
    "#         outputs = np.argmax(out, axis=1)\n",
    "#     return np.sum(outputs==labels)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%time\n",
    "    print ('*******************  run LSTM  *********************')\n",
    "    # check the sequence length\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "\n",
    "    # initialize our model\n",
    "    model = deepclim.deepArray_GPU.ConvLSTM(input_channels, hidden_channels, kernel_size).cuda()\n",
    "    loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "    # stochastic gradient descent\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # Adam optimizer\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(model)\n",
    "    print(loss_fn)\n",
    "    print(optimiser)\n",
    "    print('##############################################################')\n",
    "    print('##################  start training loop  #####################')\n",
    "    print('##############################################################')\n",
    "    hist = np.zeros(num_epochs)\n",
    "\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        \n",
    "        for timestep in range(sequence_len - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #print ('**********   create input tensor with multi-input dimension   ************')\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                ohc_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).cuda()\n",
    "            #print ('*******************   create training tensor with multi-input dimension  *********************')\n",
    "            y_train_stack = np.stack((sic_exp_norm[timestep+1,:,:],\n",
    "                                      ohc_exp_norm[timestep+1,:,:],)) #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).cuda()\n",
    "            #print ('*******************   create tensors and variables  *********************')   \n",
    "            # Forward pass\n",
    "            y_pred, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_train = y_var        \n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            if timestep == 0:\n",
    "                loss = loss_fn(y_pred, y_train)\n",
    "            else:\n",
    "                loss = loss_fn(y_pred, y_train) + loss\n",
    "            #print (timestep)\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 2 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            #print(y_pred)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    torch.save(model, os.path.join(output_path,'convlstm.pkl')) # save lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%time\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(test_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    pred_base_ohc = ohc_exp_norm[:-test_year*12*4,:,:]\n",
    "    # prediction validation with training data\n",
    "    pred_fake_sic = sic_exp_norm[0,:,:].reshape(-1,height,width)\n",
    "    pred_fake_ohc = ohc_exp_norm[0,:,:].reshape(-1,height,width)\n",
    "    \n",
    "    for i in np.arange(1,sequence_len,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        #model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        #############################################################################\n",
    "        ###############           before time of prediction           ###############\n",
    "        #############################################################################\n",
    "        if i <= (sequence_len-test_year*12*4):\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                ohc_exp_norm[i-1,:,:],\n",
    "                                month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "            x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                 requires_grad=False).cuda()\n",
    "            # make prediction\n",
    "            last_pred, _ = model(x_var_pred, i-1)\n",
    "            # save the predicted data for validation (GPU data should be transferred to CPU)\n",
    "            pred_fake_sic = np.vstack((pred_fake_sic[:,:,:],y_pred[0,0,:,:].cpu().data.numpy().reshape(-1,height,width)))\n",
    "            pred_fake_ohc = np.vstack((pred_fake_ohc[:,:,:],y_pred[0,1,:,:].cpu().data.numpy().reshape(-1,height,width)))\n",
    "            # record the real prediction after the time of prediction\n",
    "            if i == (sequence_len-test_year*12*4):\n",
    "                # GPU data should be transferred to CPU\n",
    "                data_predict_sic = y_pred[0,0,:,:].cpu().data.numpy().reshape(-1,height,width)\n",
    "                data_predict_ohc = y_pred[0,1,:,:].cpu().data.numpy().reshape(-1,height,width)\n",
    "        #############################################################################\n",
    "        ###############            after time of prediction           ###############\n",
    "        #############################################################################\n",
    "        else:\n",
    "            # prepare predictor\n",
    "            # use the predicted data to make new prediction\n",
    "            x_input = np.stack((data_predict_sic[i-(sequence_len-test_year*12*4+1),:,:],\n",
    "                                data_predict_ohc[i-(sequence_len-test_year*12*4+1),:,:],\n",
    "                                month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "            x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                 requires_grad=False).cuda()        \n",
    "            # make prediction\n",
    "            last_pred, _ = model(x_var_pred, i)\n",
    "            # record the prediction\n",
    "            data_predict_sic = np.vstack((data_predict_sic[:,:,:],y_pred[0,0,:,:].cpu().data.numpy().reshape(-1,height,width)))\n",
    "            data_predict_ohc = np.vstack((data_predict_ohc[:,:,:],y_pred[0,0,:,:].cpu().data.numpy().reshape(-1,height,width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(3):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  evaluation matrix  *********************')\n",
    "    # The prediction will be evaluated through RMSE against climatology\n",
    "    def RMSE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the RMSE. x is input series and y is reference series.\n",
    "        \"\"\"\n",
    "        rmse = np.sqrt((x-y)**2)\n",
    "    \n",
    "        return rmse\n",
    "\n",
    "    # calculate the RMSE error of predictied years\n",
    "    error_pred = np.sum(RMSE(data_predict[-test_year*12*4:,0],SIC_ERAI_sum_norm[-test_year*12*4:]))\n",
    "    print (\"RMSE = {}\".format(error_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # visualization\n",
    "    index_plot = np.arange(test_year*12*4)\n",
    "    year_index = np.arange(2013,2017,1)\n",
    "    # repeat climatology as reference\n",
    "    #climatology = np.tile(seansonal_cycle_SIC,len(year_ERAI))\n",
    "    # create index\n",
    "    print (\"*******************  verification  **********************\")\n",
    "    fig00 = plt.figure()\n",
    "    plt.plot(np.arange(sequence_len-test_year*12*4), np.sum(np.sum(pred_fake_sic[:-1,:,:],2),1), 'r',label=\"Prediction_fake\")\n",
    "    plt.plot(np.arange(sequence_len-test_year*12*4), np.sum(np.sum(sic_exp_norm[:-test_year*12*4,:,:],2),1), 'b',label=\"Data\")\n",
    "    plt.xlabel('time')\n",
    "    plt.legend(loc=1)\n",
    "    plt.show()\n",
    "    print (\"*******************  Predicted Ice Extent  **********************\")\n",
    "    # include text box in the figure\n",
    "    #text_content = '$RMSE=%.3f$ ' % (error_pred)\n",
    "    fig0 = plt.figure()\n",
    "    plt.plot(index_plot, np.sum(np.sum(data_predict_sic,2),1), 'r',label=\"Prediction\")\n",
    "    plt.scatter(index_plot, np.sum(np.sum(data_predict_sic,2),1), color='r')\n",
    "    plt.plot(index_plot, np.sum(np.sum(sic_exp_norm[-test_year*12*4:,:,:],2),1), 'b',label=\"Data\")\n",
    "    plt.scatter(index_plot,np.sum(np.sum(sic_exp_norm[-test_year*12*4:,:,:],2),1),color='b')\n",
    "\n",
    "    #plt.plot(index_plot, climatology, 'c--',label=\"climatology\")\n",
    "    plt.xlabel('time')\n",
    "    plt.xticks(np.arange(0,test_year*12*4,6*4),(['201401', '201407',  '201501', '201507', '201601', '201607']))\n",
    "    plt.legend(loc=1)\n",
    "    #props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "    #ax = plt.gca()\n",
    "    #ax.text(0.03,0.2,text_content,transform=ax.transAxes,fontsize=10,verticalalignment='top',bbox=props)\n",
    "    plt.show()\n",
    "    fig0.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred.png'),dpi=300)\n",
    "\n",
    "    print (\"*******************  Loss with time  **********************\")\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(hist, label=\"Training loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    fig1.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred_error.png'),dpi=300)\n",
    "    \n",
    "    print (\"*******************  Prediction Ice Distribution  **********************\")\n",
    "    weak = 1\n",
    "    fig2 = plt.figure()\n",
    "    plt.vlines(index_plot[-test_year*12*4],0,1,colors='g',linestyle='dashed',linewidth=2)\n",
    "    plt.plot(index_plot, data_predict[:,0], 'r',label=\"Prediction\") #[:,0] is SIC\n",
    "    plt.plot(index_plot, SIC_ERAI_sum_norm, 'b',label=\"Data\")\n",
    "    #plt.plot(index_plot, climatology, 'c--',label=\"climatology\")\n",
    "    #plt.plot(y_train[:,0,0].detach().numpy(), label=\"Data\")\n",
    "    #plt.plot(y_train[:,0,0].detach().numpy(), label=\"Training\")\n",
    "    plt.xlabel('time')\n",
    "    plt.xticks(np.arange(12*4+1,len(index_plot)+1,60*4),year_index)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    fig2.savefig(os.path.join(output_path,'SIC_{}_ERAI_LSTM_pred_full.png'.format(region_name)),dpi=300)\n",
    "    \n",
    "    print (\"*******************  Other variables (Prediction with testing sets only) **********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print ('*******************  run linear regression  *********************')\n",
    "#     model = LinearReg()\n",
    "#     # construct loss function and an optimizer.\n",
    "#     criterion = torch.nn.MSELoss(size_average=False)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(),lr=0.01) # learning rate (lr)\n",
    "#     #####################\n",
    "#     # Train Loop\n",
    "#     #####################\n",
    "#     for epoch in range(num_epoch):\n",
    "#         # predict y by fit x through linear regression\n",
    "#         y_predict = model(SIV_var)\n",
    "#         # compute and print loss\n",
    "#         loss = criterion(y_predict,y_train)\n",
    "        \n",
    "#         # backpropagation and gradient descend (fix steps)\n",
    "#         # Zero out gradient, else they will accumulate between epochs\n",
    "#         optimizer.zero_grad()\n",
    "    \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update parameters\n",
    "#         optimizer.step()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
