{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Predict the extent of Sea Ice Concentration with LSTM ** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2019.04.17 ** <br>\n",
    "** Last Update  : 2019.04.17 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, deepclim, matplotlib\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. As a pre-test, we start with time series of SIC extent anomalies. To make it easier, we use many climate index (to represent the forcing from atmosphere). In addtion,  <br>\n",
    "\n",
    "The Long Short Time Memory neural network is used here. We use Pytorch as the deep learning framework. <br>\n",
    "\n",
    "Return Values   : Time series and figures <br>\n",
    "\n",
    "Here is the information of climate index in details:<br>\n",
    "**NAO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/nao.shtml <br>\n",
    "**ENSO - NINO 3.4 SST** @ 1950 Jan - 2018 Jan (817 records) <br>\n",
    "https://www.esrl.noaa.gov/psd/gcos_wgsp/Timeseries/Nino34/ <br>\n",
    "**AO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/ao.shtml <br>\n",
    "**AMO** @ 1950 Jan - 2018 Feb (818 records) <br>\n",
    "AMO unsmoothed, detrended from the Kaplan SST V2. The result is standarised.<br>\n",
    "https://www.esrl.noaa.gov/psd/data/timeseries/AMO/<br>\n",
    "\n",
    "(All the NOAA index shown above are given by NCEP/NCAR Reanalysis (CDAS))<br>\n",
    "\n",
    "**PDO** @ 1900 Jan - 2018 Feb (1418 records)<br>\n",
    "This PDO index comes from University of Washington, it contains SST data from the following 3 datasets:<br>\n",
    "- UKMO Historical SST data set for 1900-81;\n",
    "- Reynold's Optimally Interpolated SST (V1) for January 1982-Dec 2001)\n",
    "- OI SST Version 2 (V2) beginning January 2002 -<br>\n",
    "\n",
    "http://research.jisao.washington.edu/pdo/PDO.latest<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/Scripts/DeepClim\")\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import deepclim.preprocess\n",
    "\n",
    "# for visualization\n",
    "import deepclim.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import iris # also helps with regriding\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2264670,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data ** <br>\n",
    "Time span of each product included: <br>\n",
    "** Reanalysis ** <br>\n",
    "- **ERA-Interim** 1979 - 2016 (ECMWF)\n",
    "- **ORAS4**       1958 - 2014 (ECMWF)\n",
    "\n",
    "** Index ** <br>\n",
    "- **NINO3.4**     1950 - 2017 (NOAA)\n",
    "- **AO**          1950 - 2017 (NOAA)\n",
    "- **NAO**         1950 - 2017 (NOAA)\n",
    "- **AMO**         1950 - 2017 (NOAA)\n",
    "- **PDO**         1950 - 2017 (University of Washington)\n",
    "\n",
    "!! These index are given by NCEP/NCAR Reanalysis (CDAS) <br>\n",
    "\n",
    "\n",
    "Alternative (not in use yet) <br>\n",
    "** Reanalysis ** <br>\n",
    "- **MERRA2**      1980 - 2016 (NASA)\n",
    "- **JRA55**       1979 - 2015 (JMA)\n",
    "- **GLORYS2V3**   1993 - 2014 (Mercartor Ocean)\n",
    "- **SODA3**       1980 - 2015\n",
    "- **PIOMASS**     1980 - 2015\n",
    "\n",
    "** Observations ** <br>\n",
    "- **NSIDC**       1958 - 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################   path  ######################################\n",
    "# please specify data path\n",
    "datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/ERAI/regression'\n",
    "datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/warehouse_met'\n",
    "datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/ORAS4'\n",
    "datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/Series'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    dataset_ERAI_fields_SIC_SST_SLP = Dataset(os.path.join(datapath_ERAI,\n",
    "                                              'surface_ERAI_monthly_regress_1979_2016.nc'))\n",
    "\n",
    "    dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "                                       'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                             'oras_model_monthly_1958_2017_ohc.nc'))\n",
    "    dataset_index = Dataset(os.path.join(datapath_clim_index,\n",
    "                                         'index_climate_monthly_regress_1950_2017.nc'))\n",
    "    #dataset_ERAI_fields_flux = Dataset(os.path.join(datapath_ERAI_fields,\n",
    "    #                                  'surface_erai_monthly_regress_1979_2017_radiation.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    # ********************************** data gallery *************************************** #\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2006\n",
    "    # validation: 2007 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim)\n",
    "    SIC_ERAI = dataset_ERAI_fields_SIC_SST_SLP.variables['ci'][:,:95,:] # 3D fields [time, lat, lon]\n",
    "    #SIC_ERAI_mask = np.ma.getmaskarray(SIC_ERAI[0,:,:])\n",
    "    time_ERAI = dataset_ERAI_fields_SIC_SST_SLP.variables['time'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_SIC_SST_SLP.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_SIC_SST_SLP.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # SST (ERA-Interim)\n",
    "    SST_ERAI = dataset_ERAI_fields_SIC_SST_SLP.variables['sst'][:,:95,:] # 3D fields [time, lat, lon]\n",
    "    # dimensions the same as above\n",
    "    # ST (ERA-Interim)\n",
    "    # OHC (ORAS4)\n",
    "    OHC_100_int_ORAS4 = dataset_ORAS4_OHC.variables['OHC_100_int'][21:-1,:,180:]/1000 # unit Peta Joule\n",
    "    OHC_300_int_ORAS4 = dataset_ORAS4_OHC.variables['OHC_300_int'][21:-1,:,180:]/1000\n",
    "    OHC_700_int_ORAS4 = dataset_ORAS4_OHC.variables['OHC_700_int'][21:-1,:,180:]/1000\n",
    "    #OHC_2000_int_ORAS4 = dataset_ORAS4_OHC.variables['OHC_2000_int'][21:-1,:,180:]/1000\n",
    "    \n",
    "    latitude_OHC_ORAS4 = dataset_ORAS4_OHC.variables['latitude_aux'][180:]\n",
    "    #gphiv_OHC_ORAS4 = dataset_ORAS4_OHC.variables['gphiv'][180:,:]\n",
    "    #glamv_OHC_ORAS4 = dataset_ORAS4_OHC.variables['glamv'][180:,:]\n",
    "    # AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    AO = dataset_index.variables['AO'][348:-12] # from 1979 - 2016\n",
    "    NAO = dataset_index.variables['NAO'][348:-12]\n",
    "    NINO = dataset_index.variables['NINO'][348:-12]\n",
    "    AMO = dataset_index.variables['AMO'][348:-12]\n",
    "    PDO = dataset_index.variables['PDO'][348:-12]\n",
    "    \n",
    "    # radiation flux\n",
    "    #rad_short_toa_ERAI = dataset_ERAI_fields_flux.variables['tsr'][:] # unit Watt/m2\n",
    "    #rad_long_toa_ERAI = dataset_ERAI_fields_flux.variables['ttr'][:] # unit Watt/m2\n",
    "    #rad_short_surf_ERAI = dataset_ERAI_fields_flux.variables['ssr'][:] # unit Watt/m2\n",
    "    #rad_long_surf_ERAI = dataset_ERAI_fields_flux.variables['str'][:] # unit Watt/m2\n",
    "    # turbulent flux\n",
    "    #SFlux_sensible_ERAI = dataset_ERAI_fields_flux.variables['sshf'][:] # unit Watt/m2\n",
    "    #SFlux_latent_ERAI = dataset_ERAI_fields_flux.variables['slhf'][:] # unit Watt/m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print (latitude_ERAI[:41])\n",
    "    print (latitude_OHC_ORAS4[53:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / len(latitude_ERAI)\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:95])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,i,:] = SIC_ERAI[:,i,:]* dx[i] * dy # unit m2\n",
    "    \n",
    "    print ('******************  calculate mean/sum from spatial fields  *******************')\n",
    "    # calculate the mean SST in the polar cap\n",
    "    SST_ERAI_mean = np.mean(np.mean(SST_ERAI[:,:41,:],2),1)\n",
    "    # calculate the total OHC in the polar cap\n",
    "    OHC_100_ORAS4_sum = np.sum(OHC_100_int_ORAS4[:,:,53:],2)\n",
    "    OHC_300_ORAS4_sum = np.sum(OHC_300_int_ORAS4[:,:,53:],2)\n",
    "    OHC_700_ORAS4_sum = np.sum(OHC_700_int_ORAS4[:,:,53:],2)\n",
    "    # calculate the total SIC in the polar cap\n",
    "    SIC_ERAI_sum = np.sum(np.sum(SIC_ERAI_area[:,:41,:],2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    OHC_700_ORAS4_sum_series= deepclim.preprocess.operator.unfold(OHC_700_ORAS4_sum)\n",
    "    print ('================  normalize data  =================')\n",
    "    SIC_ERAI_sum_norm = deepclim.preprocess.operator.normalize(SIC_ERAI_sum)\n",
    "    SIV_PIOMASS_norm = deepclim.preprocess.operator.normalize(SIV_PIOMASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  preview of input time series  *********************')\n",
    "    #plt.plot(SIC_ERAI_sum)\n",
    "    plt.plot(SIV_PIOMASS_norm)\n",
    "    #plt.plot(SIC_ERAI_sum_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    print (torch.__version__)\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    # create variables\n",
    "    SIC_var = torch.autograd.Variable(torch.Tensor(np.split(SIC_ERAI_sum_norm,len(SIC_ERAI_sum_norm))))\n",
    "    SIV_var = torch.autograd.Variable(torch.Tensor(np.split(SIV_PIOMASS_norm,len(SIV_PIOMASS_norm))))\n",
    "    #SIC_tensor = torch.from_Numpy(SIC_ERAI_sum) # another way fro creating tensor\n",
    "    #SIV_tensor = torch.Tensor(SIV_PIOMASS)\n",
    "    # specifications of neural network\n",
    "    input_size = 1\n",
    "    hidden_size = 1\n",
    "    output_size = 1\n",
    "    num_layers = 1\n",
    "    learning_rate = 0.01\n",
    "    num_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(SIC_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                 num_layers=2):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM module in Pytorch and specify the basic model structure.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__() #inheritance https://realpython.com/python-super/\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression module in Pytorch and specify the basic model structure.       \n",
    "        \"\"\"\n",
    "        super(LinearReg, self).__init__() #inheritance https://realpython.com/python-super/\n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        \"\"\"\n",
    "        Forward training data to the model.\n",
    "        \"\"\"\n",
    "        y_output = self.linear(x_input)\n",
    "        \n",
    "        return y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  run LSTM  *********************')\n",
    "    # initialize our model\n",
    "    model = LSTM(lstm_input_size, h1, batch_size=num_train, output_dim=output_dim, num_layers=num_layers)\n",
    "    loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #####################\n",
    "    # Train Loop\n",
    "    #####################\n",
    "    hist = np.zeros(num_epochs)\n",
    "\n",
    "    for t in range(num_epochs):\n",
    "        # Initialise hidden state\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        model.hidden = model.init_hidden()\n",
    "    \n",
    "        # Forward pass\n",
    "        y_pred = model(SIV_var)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  run linear regression  *********************')\n",
    "    model = LinearReg()\n",
    "    # construct loss function and an optimizer.\n",
    "    criterion = torch.nn.MSELoss(size_average=False)\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=0.01) # learning rate (lr)\n",
    "    #####################\n",
    "    # Train Loop\n",
    "    #####################\n",
    "    for epoch in range(num_epoch):\n",
    "        # predict y by fit x through linear regression\n",
    "        y_predict = model(SIV_var)\n",
    "        # compute and print loss\n",
    "        loss = criterion(y_predict,SIC_var)\n",
    "        \n",
    "        # backpropagation and gradient descend (fix steps)\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    k = model.forward(SIV_var)\n",
    "    #plt.plot()\n",
    "    print (k.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # visualization\n",
    "    plt.plot(y_pred.detach().numpy(), label=\"Preds\")\n",
    "    plt.plot(y_train.detach().numpy(), label=\"Data\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(hist, label=\"Training loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
